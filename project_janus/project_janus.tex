\documentclass[12pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{helvet}               % Helvetica for a clean, tech look
\renewcommand{\familydefault}{\sfdefault}
\usepackage{setspace}             % For line spacing
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{listings}             % For code snippets
\usepackage{tcolorbox}            % For highlighted boxes
\usepackage{tabularx}             % For auto-wrapping tables
\usepackage{array}                % For extra table column formatting

% --- Define Left-Aligned X Column for Tables ---
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

% --- URL BREAKING ---
\usepackage{xurl} 
\usepackage{hyperref}

% --- CONFIGURATION ---
\onehalfspacing                   % 1.5 Line Spacing for readability and length

% --- HEADER HEIGHT ---
\setlength{\headheight}{15pt}

\definecolor{janusblue}{RGB}{0, 51, 102}
\definecolor{accentgold}{RGB}{204, 153, 51}

\hypersetup{
    colorlinks=true,
    linkcolor=janusblue,
    citecolor=janusblue,
    urlcolor=accentgold,
    pdftitle={Project JANUS: The Neuro-Symbolic Visual Quant Architecture},
    pdfauthor={Jordan Smith}
}

% --- HEADER & FOOTER ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textbf{Project JANUS}}
\fancyhead[R]{\textit{Quant 4.0 Architecture}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% --- SECTION STYLING ---
\titleformat{\section}
  {\color{janusblue}\normalfont\Large\bfseries}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\color{janusblue}\normalfont\large\bfseries}
  {\thesubsection}{1em}{}

% --- CODE SNIPPET STYLE ---
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

% --- DOCUMENT START ---
\begin{document}

% =============================================================================
% TITLE PAGE
% =============================================================================
\begin{titlepage}
    \pagenumbering{gobble} 
    \centering
    \vspace*{3cm}
    
    {\Huge \textbf{Project JANUS}} \\[0.5cm]
    {\LARGE \textbf{The Neuro-Symbolic Visual Quant Architecture}} \\[1.5cm]
    
    {\Large \textit{A White Paper on the Next Generation of Autonomous Financial Intelligence}} \\[3cm]
    
    \textbf{\Large Classification: Technical White Paper} \\[0.5cm]
    \textbf{\Large Version: 1.0} \\[3cm]
    
    \textbf{Author:} Jordan Smith \\
    \textit{github.com/nuniesmith} \\[0.5cm]
    \textbf{Date:} December 14, 2025  
    
    \vfill
\end{titlepage}

% =============================================================================
% ABSTRACT
% =============================================================================
\newpage
\pagenumbering{arabic} 
\thispagestyle{plain}
\section*{Abstract}
A tension between interpretability and performance has historically defined the trajectory of quantitative finance. The early eras, characterized by heuristic-based expert systems (Quant 1.0) and factor-based statistical arbitrage (Quant 2.0), prioritized transparency and economic theory but were often constrained by linear assumptions. The subsequent rise of Deep Learning (Quant 3.0) unleashed unprecedented predictive power through high-dimensional function approximation, yet it introduced systemic risks associated with ``black box'' opacity, brittleness in the face of regime shifts, and an inability to natively enforce regulatory or logical constraints.

This white paper introduces \textbf{Project JANUS}, a comprehensive architectural framework for the next generation of algorithmic trading—\textit{Quant 4.0}. JANUS represents a convergence of three frontier paradigms: 
\begin{enumerate}
    \item \textbf{Visual Time Series Encoding}, which transforms market dynamics into topological textures processed by Computer Vision; 
    \item \textbf{Neuro-Symbolic Reasoning}, which embeds formal logic and regulatory constraints directly into the learning objective via Logic Tensor Networks (LTN); and 
    \item \textbf{Hierarchical Reinforcement Learning}, specifically Feudal Networks, which decouple strategic planning from tactical execution. 
\end{enumerate}

By synthesizing the ``Visual Intuition'' of Convolutional Neural Networks with the ``Logical Rigour'' of symbolic AI and the ``Strategic Depth'' of hierarchical agents, JANUS aims to create an autonomous trading system that is not only highly performant but also mathematically compliant, explainable, and robust to the chaotic microstructure of modern financial markets.

\newpage
% =============================================================================
% IMPLEMENTATION STATUS
% =============================================================================
\section*{Implementation Status}
This white paper describes the theoretical architecture of Project JANUS. The following core components are \textbf{implemented and operational}:

\begin{itemize}
    \item \textbf{The Cortex (HRM):} Hierarchical Reasoning Module with Manager/Worker Feudal architecture
    \item \textbf{The Oracle (Chronos-Bolt):} Optimized zero-shot probabilistic forecasting with log-space quantization and ONNX Runtime
    \item \textbf{The Eyes (Visual Encoder):} Differentiable GAF (DiffGAF) with learnable normalization, 3D GAF Video sequences, and Video Vision Transformer (ViViT) for spatiotemporal analysis
    \item \textbf{LOB Heatmap Fusion:} Multimodal fusion of GAF video with Limit Order Book heatmaps
    \item \textbf{Logic Tensor Networks (LTN):} Enhanced with Lukasiewicz T-Norm for improved gradient flow, differentiable action masking, and abductive reasoning
    \item \textbf{Differentiable Simulator:} JAX/PyTorch limit order book simulator with learnable market impact and end-to-end gradient flow
    \item \textbf{Feudal Hierarchical RL:} Manager (strategic) and Worker (tactical) agents with Hindsight Experience Replay (HER) and goal-conditioned policies
    \item \textbf{Almgren-Chriss Integration:} AC optimal execution encoded as LTN constraints with dynamic risk parameter $\lambda$
    \item \textbf{Population-Based Training:} Genetic algorithm-based hyperparameter optimization with cooperative and adversarial agent populations
    \item \textbf{Black Swan Simulator:} Synthetic extreme event generation for stress testing (flash crashes, liquidity crises, volatility spikes)
    \item \textbf{Explainability:} Grad-CAM for ViViT attention visualization and LTN logical trace generation
    \item \textbf{Production Infrastructure:} Shadow deployment, paper trading, and reality gap monitoring
\end{itemize}

\textbf{Research Directions:} Future work includes full Feudal RL training pipelines, VPIN integration for order flow toxicity, and Quantum Computing integration (Quant 5.0).

\newpage
% =============================================================================
% TABLE OF CONTENTS
% =============================================================================
\tableofcontents
\newpage

% =============================================================================
% MAIN CONTENT
% =============================================================================

\section[Evolution of Quant Intelligence]{The Evolution of Quantitative Intelligence: \\ From Heuristics to Hybrid Cognition}

\subsection{The Limitations of the Deep Learning Paradigm (Quant 3.0)}
The financial industry is currently navigating the mature phase of Quant 3.0, an era dominated by the application of deep neural networks—Long Short-Term Memory (LSTM) networks, Transformers, and Differentiable Manifolds—to raw financial time series. These models have demonstrated a remarkable ability to capture non-linear dependencies and latent patterns that traditional econometric models (such as ARIMA or GARCH) fail to identify. Foundation models like Amazon's Chronos-2 have further advanced the field by enabling zero-shot forecasting capabilities, leveraging vast corpora of synthetic and real-world data to generalize across unseen assets.

However, the reliance on end-to-end deep learning has precipitated a ``Black Box Crisis.'' In high-stakes financial environments, the opacity of these models presents a critical vulnerability. A deep reinforcement learning (DRL) agent trained solely to maximize the Sharpe ratio may inadvertently discover strategies that exploit simulator artifacts or violate regulatory norms—such as engaging in wash trading or spoofing—because it lacks an explicit understanding of market rules. Furthermore, purely data-driven models are inherently inductive; they struggle to generalize to ``black swan'' events or regime shifts that lie outside their training distribution. When market dynamics shift from a mean-reverting regime to a momentum-driven crash, standard deep learning models often fail catastrophically because they lack the symbolic reasoning capabilities to diagnose the structural change.

\subsection{Defining Quant 4.0: The Neuro-Symbolic Imperative}
Quant 4.0 represents a philosophical and architectural pivot toward Knowledge-Driven AI. Unlike its predecessor, which relies on brute-force data ingestion to approximate a mapping function:
\begin{equation}
    f(x) \rightarrow y
\end{equation}
Quant 4.0 leverages prior knowledge—encoded as symbolic logic, physical laws, or regulatory text—to constrain the hypothesis space and guide the learning process. This generation of systems is characterized by three pillars:

\begin{itemize}
    \item \textbf{Automated AI:} End-to-end automation of the research pipeline, from feature engineering to strategy deployment.
    \item \textbf{Explainable AI (XAI):} The ability to articulate the causal reasoning behind a trading decision, moving from ``The model predicts X'' to ``The model predicts X because Condition Y and Rule Z are met.''
    \item \textbf{Neuro-Symbolic Integration:} The fusion of connectionist learning (neural networks) with symbolic reasoning (logic).
\end{itemize}

Project JANUS embodies this Quant 4.0 philosophy. It posits that a robust trading agent must possess multi-sensory capabilities: it must ``see'' the market's physical structure through visual encoding, ``read'' the market's narrative through semantic processing, and ``reason'' about the market's constraints through formal logic. This hybrid approach allows the system to maintain the intuitive flexibility of neural networks while adhering to ``hard'' constraints, such as the risk limits defined by the Almgren-Chriss framework or the tax implications of wash sale rules.

\newpage
\section{The Visual Quant Paradigm: Seeing Market Topology}

\subsection{Beyond Numerical Sequences: The Case for Vision}
Financial data has traditionally been treated as a one-dimensional sequence of scalars (Open, High, Low, Close, Volume). While efficient for storage, this representation often discards the rich, multi-scale topological structures inherent in market data—the ``shape'' of a crash, the ``texture'' of a consolidation, or the ``geometry'' of a limit order book. Standard time series models like LSTMs process data sequentially, which can make it difficult to capture complex, long-range temporal correlations or to recognize patterns that are invariant to time-warping but visually distinct.

Project JANUS integrates a Visual Encoder that transforms 1D time series into 2D images. This is not merely a data augmentation technique; it is a fundamental shift in representation learning. By converting time series into images, we unlock the vast architectural power of Computer Vision (CV), specifically Convolutional Neural Networks (CNNs). CNNs are evolutionarily optimized to detect hierarchical spatial patterns—edges, shapes, and textures—which, in the context of a transformed time series, correspond to local trends, volatility clusters, and regime shifts.

\subsection{Gramian Angular Fields (GAF): The Mathematical Lens}
The primary imaging technique employed in JANUS is the Gramian Angular Field (GAF). Unlike simple line plots, GAF preserves the temporal dependency of the time series while encoding the data in a polar coordinate system. This transformation ensures that the temporal correlation structure is explicitly mapped to the spatial dimensions of the image.

\subsubsection{Mathematical Formulation of GAF}
Given a univariate time series:
\begin{equation}
    X = \{x_1, x_2, \dots, x_n\}
\end{equation}
The transformation proceeds in two steps: normalization and polar projection.

\textbf{Step 1: Learnable Normalization} \\
To fit the spectral domain of the cosine function, the series is normalized to the interval $[-1, 1]$. JANUS introduces \textbf{DiffGAF} (Differentiable GAF), which employs learnable normalization parameters $\alpha$ and $\beta$ that are optimized during training:

\begin{equation}
    \tilde{x}_i = \tanh\left(\frac{x_i - \min(X)}{\max(X) - \min(X)} \cdot \alpha + \beta\right)
\end{equation}

where $\alpha$ and $\beta$ are learnable parameters. This learnable normalization enables the GAF transformation to adapt to different market regimes and volatility structures, allowing the model to learn optimal scaling factors that maximize the discriminative power of the visual representation. Unlike fixed min-max scaling, this approach enables end-to-end gradient flow from the CNN classifier back through the GAF encoding, allowing the normalization to be optimized jointly with the visual pattern recognition task.

\textbf{Step 2: Polar Coordinate Transformation} \\
The normalized value is encoded as the angular cosine, and the time stamp $t_i$ is encoded as the radius $r_i$:
\begin{equation}
\begin{cases} 
\phi_i = \arccos(\tilde{x}_i), & -1 \le \tilde{x}_i \le 1 \\ 
r_i = \frac{t_i}{N}, & t_i \in \mathbb{N} 
\end{cases}
\end{equation}
This mapping is bijective, ensuring that no information is lost in the transformation. The angular perspective dominates the representation, while the radial component preserves the temporal progression.

\textbf{Step 3: Gramian Field Generation} \\
The temporal correlations between different time steps are computed using trigonometric sums and differences.

Gramian Angular Summation Field (GASF):
\begin{equation}
    \text{GASF}_{i,j} = \cos(\phi_i + \phi_j) = \tilde{x}_i \tilde{x}_j - \sqrt{1 - \tilde{x}_i^2}\sqrt{1 - \tilde{x}_j^2}
\end{equation}

Gramian Angular Difference Field (GADF):
\begin{equation}
    \text{GADF}_{i,j} = \sin(\phi_i - \phi_j) = \sqrt{1 - \tilde{x}_i^2}\tilde{x}_j - \tilde{x}_i\sqrt{1 - \tilde{x}_j^2}
\end{equation}

The result is an $n \times n$ matrix (image) where the position $(i, j)$ represents the superposition of the market state at time $i$ and time $j$. The main diagonal contains the original value information (reconstructed spatial features), while the off-diagonal elements capture the rich temporal correlation structure across the entire window.

\subsection{From Static Images to Spatiotemporal Manifolds: GAF Video and ViViT}

JANUS extends the visual encoding paradigm beyond static GAF images to \textbf{3D spatiotemporal manifolds} through two key innovations:

\subsubsection{GAF Video Generation}
Rather than processing isolated GAF frames, JANUS generates overlapping sequences of GAF frames using a sliding window approach. Given a time series $X = \{x_1, x_2, \dots, x_T\}$, we create a sequence of GAF images:

\begin{equation}
    \mathcal{V} = \{GAF(X_{t:t+w}), GAF(X_{t+s:t+w+s}), \dots, GAF(X_{t+Fs:t+w+Fs})\}
\end{equation}

where $w$ is the window size, $s$ is the stride, and $F$ is the number of frames. This creates a 3D tensor $\mathcal{V} \in \mathbb{R}^{F \times H \times W}$ where $F$ is the temporal dimension (number of frames) and $H \times W$ is the spatial dimension of each GAF image. This representation captures not just the instantaneous market state, but the \textit{evolution} of market patterns over time, enabling the detection of regime transitions, trend formations, and volatility clustering dynamics.

\subsubsection{Video Vision Transformer (ViViT)}
To process these spatiotemporal manifolds, JANUS employs a Video Vision Transformer (ViViT) architecture. ViViT extends the standard Vision Transformer to handle video sequences by factorizing the attention mechanism across spatial and temporal dimensions:

\begin{itemize}
    \item \textbf{Spatial Attention:} Within each frame, ViViT applies self-attention to capture spatial patterns (e.g., volatility clusters, trend structures)
    \item \textbf{Temporal Attention:} Across frames, ViViT applies temporal attention to capture dynamic evolution (e.g., regime transitions, momentum shifts)
\end{itemize}

The factorized attention mechanism allows ViViT to efficiently process long video sequences while maintaining the ability to capture both local spatial patterns and long-range temporal dependencies. The output is a high-dimensional embedding vector that encodes the spatiotemporal market structure, which is then fused with other modalities (time series forecasts, text embeddings) for decision-making.

\subsubsection{Limit Order Book Heatmap Fusion}
JANUS further enriches the visual representation by fusing GAF video with Limit Order Book (LOB) heatmaps. LOB heatmaps visualize the depth and liquidity structure of the order book as 2D images, where the $x$-axis represents price levels and the $y$-axis represents depth. By concatenating or adding LOB heatmaps to GAF frames, JANUS creates a multimodal visual representation that captures both the temporal dynamics of prices (via GAF) and the microstructure liquidity structure (via LOB heatmaps).

\textbf{Visual Evidence:} Grad-CAM (Gradient-weighted Class Activation Mapping) generates attention heatmaps for ViViT, showing which GAF frames and spatial regions within frames are most influential for a given prediction, providing visual evidence for explainability queries.

\newpage
\section[The Logic Layer: Neuro-Symbolic Reasoning]{The Logic Layer: Neuro-Symbolic Reasoning with Logic Tensor Networks}

\subsection{The Necessity of Symbolic Constraints in Finance}
While deep learning provides powerful intuition and pattern recognition, it fundamentally lacks logical precision. In the domain of quantitative finance, ``close enough'' is often unacceptable. A trading bot that generates a profit but inadvertently executes a ``wash sale''—selling a security at a loss and repurchasing a substantially identical one within 30 days—triggers tax penalties. These penalties can negate the strategy's alpha and create legal liabilities. Similarly, execution algorithms must adhere to strict risk mandates, such as those derived from the Almgren-Chriss model, where the risk aversion parameter $\lambda$ dictates the maximum acceptable variance of execution cost.

Pure neural networks struggle to learn these ``hard'' constraints solely from examples (data efficiency) and cannot guarantee adherence to them in unseen scenarios (safety). A standard Reinforcement Learning (RL) agent might learn to avoid wash sales in the training set because they result in a lower reward, but it does not understand the rule itself. Consequently, when presented with a novel market condition, the agent might violate the rule if it predicts a high enough gross return. Neuro-Symbolic AI addresses this critical gap by embedding formal logic directly into the learning process.

JANUS implements a multi-layered constraint enforcement system combining rule-based risk management through the Praxeological Motor with Logic Tensor Networks (LTN) for neuro-symbolic constraint enforcement. The system uses Product-not-Sum consensus where any motor can veto a trade by returning a score of zero, ensuring safety through multiplicative aggregation.

\subsection{Logic Tensor Networks (LTN): The Reasoning Engine}
Project JANUS utilizes Logic Tensor Networks (LTN) as its core reasoning engine. LTN is a neuro-symbolic framework that integrates neural networks with First-Order Logic (FOL). It enables the specification of knowledge using logical formulas where the predicates and functions are approximated by neural networks (a process known as ``grounding''), and the satisfaction of these formulas is maximized during training.

\subsubsection{Real Logic and Differentiable Constraints}
To make logical constraints compatible with gradient-based optimization, LTN employs Real Logic. In this paradigm, truth values are relaxed from the binary set $\{0, 1\}$ to the continuous interval $[0, 1]$. This allows logical constraints to be differentiable and incorporated into the loss function.

Let $\mathcal{G}$ be a grounding function that maps logical symbols to tensors:
\begin{itemize}
    \item \textbf{Constants:} A constant symbol $c$ (e.g., a specific trade or asset) is mapped to a tensor in $\mathbb{R}^n$.
    \item \textbf{Predicates:} A predicate symbol $P$ (e.g., $\textit{IsVolatile}$, $\textit{WashSale}$) is mapped to a neural network function $f_P: \mathbb{R}^n \rightarrow [0, 1]$, which outputs the degree of truth.
    \item \textbf{Connectives:} Logical connectives ($\land, \lor, \neg, \rightarrow$) are modeled using fuzzy logic t-norms (e.g., Lukasiewicz, Gödel, or Product t-norms).
\end{itemize}

JANUS employs the \textbf{Lukasiewicz T-Norm} for logical connectives, which provides superior gradient flow compared to the Product t-norm, especially for low truth values. The Lukasiewicz conjunction is defined as:
\begin{equation}
    \mathcal{G}(A \land B) = \max(0, \mathcal{G}(A) + \mathcal{G}(B) - 1)
\end{equation}
The Lukasiewicz disjunction is:
\begin{equation}
    \mathcal{G}(A \lor B) = \min(1, \mathcal{G}(A) + \mathcal{G}(B))
\end{equation}
The Lukasiewicz implication is:
\begin{equation}
    \mathcal{G}(A \rightarrow B) = \min(1, 1 - \mathcal{G}(A) + \mathcal{G}(B))
\end{equation}

The key theoretical advantage of Lukasiewicz over Product t-norm is that it maintains non-zero gradients even when truth values approach zero, preventing the ``vanishing gradient'' problem that can occur with Product t-norm when $\mathcal{G}(A) \approx 0$ or $\mathcal{G}(B) \approx 0$. This is particularly important in financial applications where constraints may be frequently violated during early training, requiring stable gradient signals to learn constraint adherence.

This allows the network to learn complex dependencies: ``If A is true, then B must be true,'' with improved optimization dynamics.

\subsubsection{The Satisfiability Objective}
The learning objective in JANUS is to find the set of parameters $\theta$ (the weights of the neural networks) that maximizes the satisfiability of a Knowledge Base $\mathcal{K}$ containing a set of logical axioms. The logical loss function is defined as the aggregate ``falsity'' of the formulas:
\begin{equation}
    \mathcal{L}_{logic}(\theta) = 1 - \text{SatAgg}_{\phi \in \mathcal{K}} (\mathcal{G}_\theta(\phi))
\end{equation}
where $\text{SatAgg}$ is an aggregation operator (e.g., generalized mean) over the truth values of all constraints. This term is added to the standard predictive loss, ensuring that the model optimizes for accuracy while strictly adhering to the logical rules.

\subsection{Encoding Financial Rules: The Knowledge Base}
JANUS encodes critical trading rules and market dynamics as First-Order Logic constraints within the LTN layer. This ``Knowledge Base'' acts as the immutable conscience of the trading agent.

\subsubsection{The Wash Sale Constraint}
To prevent wash sales, we define a predicate $\textit{WashSale}(trade_t, trade_{t+k})$ which evaluates to true if a trade at time $t$ is a sale at a loss and a trade at time $t+k$ (where $k \le 30$ days) is a purchase of a substantially identical asset. The logical axiom enforced is:
\begin{equation}
    \forall t, \forall k \in (0, 30 \text{ days}]: \neg (\textit{SaleAtLoss}(t) \land \textit{Buy}(t+k))
\end{equation}
In the LTN, this logic is added to the loss function. The agent is penalized not just for losing money (financial loss), but for violating the logical structure of the tax code. This ensures that the learned policy respects the 61-day window surrounding a loss sale, effectively ``masking'' the buy action in the prohibited window.

\subsubsection{Risk Logic and Almgren-Chriss Adherence}
We encode the Almgren-Chriss optimal execution framework directly as an LTN constraint. The AC framework balances expected cost (market impact) against timing risk (volatility exposure). We introduce predicates $\textit{Volatile}(Market)$ and $\textit{Impact}(v)$ to represent market volatility and the market impact of trade size $v$. The AC constraint is encoded as:

\begin{equation}
    \forall v: \textit{Volatile}(Market) \rightarrow \textit{Impact}(v) < \textit{DynamicThreshold}(\textit{Volatile})
\end{equation}

where $\textit{DynamicThreshold}$ is a function that adjusts the acceptable impact threshold based on current volatility. This constraint ensures that in volatile markets, the system automatically reduces trade sizes to maintain impact below a volatility-adjusted threshold, implementing the AC square-root law: $I(v) = \eta \sigma \sqrt{v/V}$ where $\eta$ is the impact coefficient, $\sigma$ is volatility, $v$ is trade size, and $V$ is average volume.

This symbolic rule forces the neural network to learn the causal relationship between market volatility (detected by the Visual Encoder) and order size reduction. Unlike a pure RL agent that might discover this relationship through trial and error (and potentially catastrophic losses), the LTN agent is ``told'' this relationship a priori, accelerating convergence and acting as a ``safety rail.'' The constraint is enforced through \textbf{differentiable action masking}, which filters proposed actions during both training (using Gumbel-Softmax relaxation) and inference (using hard masking), ensuring strict adherence to AC principles.

\subsubsection{Abductive Reasoning: Inferring Latent Predicates}
JANUS implements an \textbf{Abductive Reasoning Module} that infers latent market predicates (e.g., Squeeze, Crash, Rally, Consolidation, Breakout) from high-dimensional embeddings. Abduction is the logical inference that seeks the best explanation for observed data. Given an embedding vector $e \in \mathbb{R}^d$ (e.g., from ViViT or Chronos), the abductive reasoner uses a Transformer encoder to infer predicate truth values:

\begin{equation}
    P = \{p_1, p_2, \dots, p_k\} = \text{AbductiveReasoner}(e)
\end{equation}

where each $p_i \in [0, 1]$ represents the degree of truth for predicate $i$ (e.g., ``Market is in a Crash regime''). The reasoner generates logical traces explaining why certain predicates are active, providing human-readable explanations such as: ``High volatility detected (0.95 confidence) and negative momentum (0.87 confidence) → Crash regime (0.91 confidence).''

This abductive capability enables \textbf{explainable classification}: rather than simply outputting a class label, the system explains its reasoning by inferring the underlying logical predicates that justify the classification. This is essential for regulatory compliance and risk management, where ``why'' is as important as ``what.''

\subsubsection{VPIN and Toxicity Detection}
The Logic Layer also monitors the Volume-Synchronized Probability of Informed Trading (VPIN), a metric of order flow toxicity. If VPIN spikes, indicating the presence of informed traders or a potential crash, the logic layer can trigger a defensive posture.
\begin{equation}
    \forall t: \textit{VPIN}_t > Threshold \rightarrow \textit{HaltTrading}(t)
\end{equation}
This acts as a ``circuit breaker.'' If the toxicity constraint is violated, the LTN forces the execution probability to zero, effectively overriding the RL agent's output. This prevents the algorithm from providing liquidity to toxic flow, a common failure mode in standard HFT algorithms during events like the Flash Crash.

\newpage
\section[Chronos-2 and Multimodal Fusion]{The Time Series Foundation: Chronos-2 and Multimodal Fusion}

\subsection{Chronos-Bolt: Optimized Zero-Shot Forecasting}
For the core numerical time series processing, JANUS employs Chronos-Bolt, an optimized variant of Amazon's Chronos-2 foundation model. Unlike traditional autoregressive models that process raw float values, Chronos-Bolt adopts a language-modeling perspective, quantizing time series values into a discrete vocabulary of tokens, allowing it to leverage the powerful Transformer architecture (specifically the T5 encoder-decoder) originally designed for Natural Language Processing (NLP).

\textbf{Why Chronos-Bolt?} \\
The strategic advantage of Chronos-Bolt lies in its Zero-Shot capabilities. Trained on a massive corpus of real-world and synthetic time series data, it has learned universal temporal patterns—seasonality, trend breaks, mean reversion—that generalize across domains. This allows JANUS to deploy effectively on new asset classes (e.g., shifting from Equities to Crypto) without the need for extensive, data-hungry retraining.

\textbf{Log-Space Quantization:} JANUS implements log-space quantization for financial returns, which provides higher resolution for small moves (where most trading occurs) and lower resolution for outliers. This is particularly important for financial time series, where the distribution of returns is heavy-tailed and small percentage moves are more frequent than large ones. The quantization function maps returns $r$ to discrete tokens:

\begin{equation}
    \text{token} = \lfloor \log(1 + |r|) \cdot \text{scale} \rfloor \cdot \text{sign}(r)
\end{equation}

This quantization scheme ensures that the model allocates more representational capacity to the regime where most trading decisions occur, improving both accuracy and efficiency.

\textbf{ONNX Runtime Integration:} For production deployment, JANUS exports Chronos-Bolt models to ONNX format, enabling optimized inference on CPU, GPU, and specialized accelerators. This provides significant latency reduction compared to native PyTorch inference, critical for real-time trading applications.

\subsection{Multimodal Fusion: Integrating Vision, Text, and Time}
The true power of JANUS lies in its ability to synthesize heterogeneous data streams. The input to the system is not just a sequence of prices, but a multimodal stream:
\begin{itemize}
    \item \textbf{Time Series Tokens:} Quantized price and volume history (from Chronos-2).
    \item \textbf{Visual Tokens:} Feature vectors representing market texture (from the GAF-CNN).
    \item \textbf{Semantic Tokens:} Contextual embeddings of news and sentiment (from FinBERT).
\end{itemize}

To fuse these disparate modalities, JANUS employs a Multimodal Stable Fusion with Gated Cross-Attention (MSGCA) mechanism.

\subsubsection{Gated Cross-Attention (GCA)}
Standard attention mechanisms might allow one noisy modality (e.g., a flurry of irrelevant tweets) to drown out the signal from others. The GCA mechanism introduces a learnable gating scalar that dynamically weighs the importance of each modality based on the current context.

The cross-attention score is computed as:
\begin{equation}
    \alpha_{m \rightarrow n} = \text{softmax} \left( \frac{Q_m K_n^\top}{\sqrt{d_h}} \right)
\end{equation}
where $Q_m$ is the query from the primary modality (e.g., Time Series) and $K_n$ is the key from the auxiliary modality (e.g., Text or Vision). The gating mechanism $\lambda_{gate}$ modulates the contribution of the auxiliary context:
\begin{equation}
    H_{fused} = H_{TS} + \lambda_{gate} \cdot \text{Attention}(Q_{TS}, K_{Aux}, V_{Aux})
\end{equation}

\textbf{Scenario-Based Adaptation:}
\begin{itemize}
    \item \textbf{Earnings Call:} When semantic density is high (e.g., during an earnings release), the model learns to increase $\lambda_{text}$, allowing the FinBERT embeddings to heavily influence the forecast.
    \item \textbf{Market Crash:} During a liquidity crisis where fundamentals are ignored and pure price dynamics dominate, the model increases $\lambda_{vis}$ (Visual Texture) and decreases $\lambda_{text}$, focusing on the geometric structure of the collapse.
    \item \textbf{Quiet Drift:} During periods of low activity, the gating mechanism suppresses auxiliary inputs, relying primarily on the autoregressive price dynamics captured by Chronos-2.
\end{itemize}

\subsection{Semantic Processing with FinBERT}
Textual data is processed using FinBERT, a BERT variant pre-trained specifically on financial corpora (corporate filings, financial news, analyst reports). Unlike general-purpose LLMs, FinBERT is tuned to understand financial nuance—for example, recognizing that the word ``liability'' has a specific negative accounting connotation, or that ``volatility'' can be positive for a market maker but negative for a long-only fund.

JANUS utilizes the output of FinBERT not just as a sentiment score (Positive/Negative), but as a high-dimensional embedding vector. This vector captures the semantic state of the market narrative. By aligning this text embedding with the time series embedding, JANUS ensures that the ``language'' of price and the ``language'' of news are mapped to a shared latent space.

\newpage
\section[Hierarchical Execution and Risk]{The Hand: Hierarchical Execution and Risk Management}

\subsection{The Need for Hierarchy}
A monolithic agent attempting to solve both the high-level strategy (e.g., ``Accumulate AAPL over the next week'') and the low-level execution (e.g., ``Post a bid at 150.05'') suffers from the Curse of Dimensionality and disparate temporal scales. Strategic decisions happen over hours or days; execution decisions happen over milliseconds. JANUS addresses this by implementing a Hierarchical Reasoning Module (HRM) architecture that separates strategic planning from tactical execution. This architecture mimics the organizational structure of a human trading desk.

JANUS implements an HRM architecture with CEO (Manager) and Worker nodes. The CEO node operates at a strategic timescale (updating periodically) to set market regime and risk appetite, outputting an abstract Goal Vector $g_t$ and Risk Aversion Parameter $\lambda_t$. The Worker node operates at a tactical timescale to generate trading signals and execution plans within the CEO's strategic context.

The architecture supports both LLM-based reasoning (for interpretability) and trained reinforcement learning agents. A Multi-Agent Reinforcement Learning (MARL) framework enables modeling collective behavior and cooperative logic constraints for systemic risk management.

\subsection{The Feudal Architecture: Manager and Worker}
\vspace{-0.3em}

\textbf{The Manager (High-Level Policy)}
\begin{itemize}
    \item \textbf{Temporal Resolution:} Low frequency (e.g., 15 minutes to 1 hour).
    \item \textbf{Input:} Fused multimodal state including ViViT visual embeddings (from GAF video), Chronos-Bolt forecast embeddings, and market regime memory.
    \item \textbf{Output:} An abstract Goal Vector $g_t \in \mathbb{R}^d$ in latent space (e.g., $d=3$ representing [direction, size, urgency]) and a Risk Aversion Parameter $\lambda_t \in [0, 1]$.
    \item \textbf{Memory:} LSTM or Transformer memory to track market regime evolution over time.
    \item \textbf{Objective:} Maximize long-term portfolio PnL and Sharpe Ratio. The Manager does not interact with the order book directly; it ``commands'' the Worker through abstract goals.
\end{itemize}

\textbf{The Worker (Low-Level Policy)}
\begin{itemize}
    \item \textbf{Temporal Resolution:} High frequency (e.g., ticks to seconds).
    \item \textbf{Input:} The goal $g_t$ from the Manager and the immediate microstructure state (Order Book depth, VPIN, recent trades).
    \item \textbf{Output:} Primitive actions (trade size, price offset, side: BUY/SELL/HOLD).
    \item \textbf{Intrinsic Reward:} Cosine similarity between achieved state and goal vector $g_t$, enabling goal-conditioned learning.
    \item \textbf{Objective:} Fulfill the Manager's goal $g_t$ while minimizing Implementation Shortfall and strictly adhering to the Almgren-Chriss trajectory. Actions are filtered through LTN constraints (wash sale, AC impact limits) before execution.
\end{itemize}

\textbf{Hindsight Experience Replay (HER):} To improve sample efficiency in goal-conditioned RL, JANUS employs HER, which relabels failed episodes with achieved goals. If the Worker fails to reach goal $g_t$ but achieves a different state $s'$, the experience is relabeled with goal $g'$ corresponding to $s'$. This allows the agent to learn from ``failures'' by treating them as successes for alternative goals, dramatically improving sample efficiency (up to 10x reduction in required training episodes).

This decoupling allows the Manager to focus on strategy and logic (avoiding wash sales, managing portfolio beta, detecting regime shifts) while the Worker focuses on microstructure tactics (navigating the spread, hiding liquidity, minimizing market impact).
\vspace{-0.5em}

\subsection{Risk Management and Position Sizing}
\vspace{-0.3em}
JANUS implements risk management through multiple mechanisms: the Praxeological Motor enforces risk vetoes based on market regime, volatility thresholds, and signal confidence; position sizing is dynamically adjusted based on consensus score and risk appetite $\lambda$ set by the CEO node; and the Almgren-Chriss (AC) optimal execution framework guides execution strategy.

\textbf{Risk Framework:}
\begin{itemize}
    \item \textbf{High Confidence / Low Volatility:} The CEO sets a higher risk appetite (e.g., $\lambda \approx 0.7$), allowing larger position sizes and more aggressive signals.
    \item \textbf{High Volatility / Bear Regime:} The CEO sets a lower risk appetite (e.g., $\lambda \approx 0.2$), reducing position sizes and requiring higher consensus scores for trade execution.
    \item \textbf{Risk Veto:} The Praxeological Motor can veto any trade by returning a zero score, which multiplies through the Product-not-Sum consensus to prevent execution.
\end{itemize}

\textbf{Almgren-Chriss Optimal Execution:}
The AC framework provides a mathematical solution for liquidating or acquiring a position $X$ over time $T$ while balancing two competing costs:
\begin{itemize}
    \item \textbf{Expected Cost ($E[C]$):} Driven by temporary market impact (slippage). Trading too fast moves the price against you.
    \item \textbf{Timing Risk ($Var[C]$):} Driven by market volatility. Trading too slow exposes you to price drift.
\end{itemize}
The objective function is:
\begin{equation}
    \min_{v} (E[C] + \lambda Var[C])
\end{equation}
where $v$ is the trading trajectory (speed) and $\lambda$ is the risk aversion parameter from the CEO.

The parameter $\lambda$ is dynamically modulated by the Manager based on regime detection, enabling adaptive execution strategies from patient TWAP-like execution in stable markets to aggressive front-loaded execution during volatile periods. The Worker agent receives the Goal Vector $g_t$ and $\lambda_t$ from the Manager, generating execution schedules that minimize Implementation Shortfall while adhering to the AC trajectory.

\newpage
\section{Differentiable Simulation: End-to-End Gradient Flow}

\subsection{The Need for Differentiable Execution}
Traditional reinforcement learning for trading relies on black-box simulators that provide rewards but no gradients. This forces agents to learn through pure policy search (e.g., policy gradients), which is sample-inefficient. JANUS introduces a \textbf{differentiable limit order book simulator} that enables end-to-end gradient flow from portfolio PnL back to trading actions, allowing the agent to learn execution strategies through gradient-based optimization.

\subsection{Mathematical Formulation}
The differentiable simulator models order book dynamics using relaxed (smooth) operations. Order matching, which typically involves discrete min/max operations, is approximated using softmax with temperature:

\begin{equation}
    \text{execution\_price} = \text{softmax}_\tau(\text{order\_book\_levels}) \cdot \text{prices}
\end{equation}

where $\tau$ is a temperature parameter. As $\tau \to 0$, this approaches hard min/max; during training, $\tau > 0$ ensures differentiability.

\subsection{Learnable Market Impact}
Market impact is modeled using a learnable neural network $f_\theta$ that takes trade size $v$ and order book state $s$ as inputs:

\begin{equation}
    I(v, s) = f_\theta(v, s)
\end{equation}

The network can learn complex impact functions, including the Almgren-Chriss square-root law $I(v) = \eta \sigma \sqrt{v/V}$ as a special case. This learnable impact model is calibrated against historical execution data and can adapt to changing market microstructure.

\subsection{End-to-End Optimization}
The differentiable simulator enables the computation of gradients:

\begin{equation}
    \frac{\partial \text{PnL}}{\partial \text{action}} = \frac{\partial \text{PnL}}{\partial \text{execution\_price}} \cdot \frac{\partial \text{execution\_price}}{\partial \text{action}}
\end{equation}

This allows the RL agent to learn execution strategies through backpropagation, dramatically improving sample efficiency compared to pure policy search methods. The simulator is implemented in JAX for automatic differentiation, with a PyTorch fallback for compatibility.

\newpage
\section{Training and Optimization}

\subsection{Training Pipeline}
For Feudal RL implementation, the training process follows a multi-stage approach:

\textbf{Pre-training:}
\vspace{-0.2em}
\begin{itemize}[itemsep=0.1em, topsep=0.2em]
    \item \textbf{DiffGAF:} Pre-trained using Masked Autoencoder (MAE) objective, where random patches of GAF images are masked and the model learns to reconstruct them. This self-supervised pre-training enables the model to learn robust visual representations of market patterns.
    \item \textbf{ViViT:} Pre-trained on GAF video sequences for spatiotemporal pattern recognition, learning to identify regime transitions and volatility clustering.
    \item \textbf{Chronos-Bolt:} Pre-trained foundation model (zero-shot, no fine-tuning required).
    \item \textbf{LTN Predicates:} Initialized and knowledge base $\mathcal{K}$ compiled into computation graph with Lukasiewicz t-norm.
\end{itemize}
\vspace{-0.3em}

\textbf{Feudal RL Training:}
\vspace{-0.2em}
\begin{itemize}[itemsep=0.1em, topsep=0.2em]
    \item \textbf{Phase 1 (Worker):} Worker trained in isolation using PPO (Proximal Policy Optimization) with Almgren-Chriss objective, randomized goals $g_t$, and risk parameters $\lambda$. HER is used to improve sample efficiency.
    \item \textbf{Phase 2 (Manager):} Manager trained to optimize high-level PnL with frozen Worker, learning to generate optimal Goal Vectors $g_t$ and risk parameters $\lambda_t$ based on visual and forecast embeddings.
    \item \textbf{Phase 3 (Joint Fine-Tuning):} Both agents trained end-to-end with logical loss $\mathcal{L}_{logic}$ added to reward signal, ensuring constraint satisfaction. Differentiable action masking ensures actions violating LTN constraints are penalized during training.
\end{itemize}
\vspace{-0.3em}

\textbf{Population-Based Training (PBT):} JANUS employs PBT to evolve a population of agents using genetic algorithms. The population includes both cooperative agents (optimizing Sharpe ratio) and adversarial agents (testing robustness). PBT performs:
\begin{itemize}
    \item \textbf{Selection:} Tournament selection based on fitness (Sharpe ratio, robustness to black swan events)
    \item \textbf{Crossover:} Uniform crossover of hyperparameters and model weights
    \item \textbf{Mutation:} Random perturbations of hyperparameters
    \item \textbf{Elitism:} Best agents preserved across generations
\end{itemize}
This evolutionary approach enables the discovery of robust agent configurations that generalize well to unseen market conditions.

\textbf{Optimization Objectives:}
The combined loss function includes:
\begin{itemize}[itemsep=0.1em, topsep=0.2em]
    \item \textbf{Predictive Loss:} Standard forecasting accuracy (e.g., MSE, MAE).
    \item \textbf{Logical Loss:} Satisfiability of knowledge base constraints: $\mathcal{L}_{logic}(\theta) = 1 - \text{SatAgg}_{\phi \in \mathcal{K}} (\mathcal{G}_\theta(\phi))$.
    \item \textbf{Execution Loss:} Implementation Shortfall minimization.
    \item \textbf{Cooperative Loss:} Market stability contribution (for MARL scenarios).
\end{itemize}

\newpage
\section{Simulation, Backtesting, and Validation}

\subsection{Methodological Rigour}
\vspace{-0.3em}
Validation of the JANUS architecture requires more than a simple P\&L backtest. We employ a rigorous simulation environment that stresses the ``Neuro-Symbolic'' aspects of the system.
\vspace{-0.2em}
\begin{itemize}[itemsep=0.1em, topsep=0.2em]
    \item \textbf{Constraint Violation Rate:} The primary metric for the Logic Layer. We measure the percentage of trades that violate the Wash Sale rule or Almgren-Chriss risk limits. The target is effectively $0\%$.
    \item \textbf{Zero-Shot Generalization:} Testing the model on asset classes it was not trained on (e.g., training on the S\&P 500 constituents and testing on cryptocurrency pairs). This validates the ``Foundation Model'' capabilities of the Chronos-Bolt backbone.
    \item \textbf{Differentiable Simulation:} JANUS employs a differentiable limit order book simulator implemented in JAX (with PyTorch fallback) that enables end-to-end gradient flow from PnL to trading actions. The simulator models:
    \begin{itemize}
        \item Order matching with differentiable min/max operations (using softmax temperature)
        \item Learnable market impact model: $I(v) = f_\theta(v, \text{LOB state})$ where $f_\theta$ is a neural network
        \item Queue dynamics with realistic depth depletion
    \end{itemize}
    This differentiable simulation allows the RL agent to learn execution strategies through gradient-based optimization rather than pure policy search, dramatically improving sample efficiency.
    \item \textbf{Black Swan Stress Testing:} JANUS includes a Black Swan Event Simulator that generates synthetic extreme market events:
    \begin{itemize}
        \item Flash crashes (sudden price drops)
        \item News shocks (information-driven volatility spikes)
        \item Liquidity crises (order book depletion)
        \item Volatility spikes (regime transitions)
        \item Circuit breakers (trading halts)
        \item Flash rallies (sudden price increases)
    \end{itemize}
    Agents are evaluated on robustness to these events, ensuring they maintain constraint adherence even under extreme conditions.
    \item \textbf{Reality Gap Monitoring:} In production, JANUS continuously monitors the ``reality gap''—the discrepancy between simulator predictions and live market data. This enables iterative refinement of the simulator and early detection of model drift.
\end{itemize}

\subsection{Comparative Analysis}
In preliminary simulations (based on aggregated benchmarks like fev-bench and GIFT-Eval), architectures similar to JANUS show distinct advantages:

\begin{center}
\begin{tabularx}{\linewidth}{|l|L|L|L|}
\hline
\textbf{Metric} & \textbf{Traditional Quant (2.0)} & \textbf{Pure Deep RL (3.0)} & \textbf{JANUS (4.0)} \\ 
\hline
Forecasting Accuracy & Low (Linear) & High (Non-Linear) & High (Multimodal) \\
Constraint Adherence & High (Hard-coded) & Low (Reward Hacking) & High (Neuro-Symbolic) \\
Adaptability & Low (Rigid Params) & Low (Brittleness) & High (Zero-Shot) \\
Explainability & High & None (Black Box) & Moderate (Logic Trace) \\ 
\hline
\end{tabularx}
\end{center}

JANUS combines Chronos-2's zero-shot forecasting, multimodal fusion (MSGCA), Product-not-Sum consensus, LTN constraint enforcement, and Almgren-Chriss optimal execution. This provides a robust foundation for autonomous trading that achieves high performance while maintaining ``Regulatory Grade'' safety through formal logical constraints and comprehensive explainability.

\newpage
\section{Regulatory Compliance, Ethics, and Systemic Risk}

\subsection{The ``Glass Box'' Transparency}
A defining feature of Quant 4.0 is Explainability. In a standard Deep RL model, querying ``Why did you reduce the position?'' yields no intelligible answer. 

JANUS provides comprehensive ``Glass Box'' transparency through multiple explainability mechanisms:

\begin{itemize}
    \item \textbf{Reasoning Traces:} The HRM workflow logs CEO and Worker node decisions, including regime assessments and signal generation rationale.
    \item \textbf{Motor Scores:} The consensus mechanism exposes individual motor scores (Chronos, CNN, Risk, Fundamental), allowing inspection of each component's contribution.
    \item \textbf{LTN Formal Traces:} Logic Tensor Networks provide formal logical traces showing predicate evaluations and rule triggers:
    \begin{quote}
    \sloppy
    \textbf{Query:} ``Why was the order size reduced?'' \\
    \textbf{LTN Trace:} ``Predicate \textit{HighVolatility}(VisualInput) is TRUE (0.95 confidence). Predicate \textit{LiquidityRisk} is TRUE. Rule triggered: HighVol $\to$ ReduceSize.''
    \end{quote}
    \item \textbf{Visual Evidence:} Grad-CAM (Gradient-weighted Class Activation Mapping) generates heatmaps highlighting which input regions triggered CNN predictions, providing visual evidence for the visual encoder's decisions.
\end{itemize}

This ``Glass Box'' transparency is essential for gaining trust from risk managers and regulators (e.g., SEC, FCA), who increasingly demand auditability for algorithmic trading systems.

\subsection{Addressing Systemic Risk}
While JANUS is designed to be individually safe, the widespread adoption of such ``aware'' agents introduces new systemic risks. If thousands of JANUS agents simultaneously detect a ``Crash Texture'' and logically decide to ``Halt Trading'' (via the VPIN rule), market liquidity could evaporate instantly, exacerbating a Flash Crash.

JANUS addresses this through Multi-Agent Reinforcement Learning (MARL) modeling and Cooperative Logic constraints. The MARL framework models collective behavior of JANUS agent populations, analyzing:
\begin{itemize}
    \item \textbf{Flash Crash Risk:} Simultaneous VPIN-triggered halts leading to liquidity evaporation
    \item \textbf{Liquidity Evaporation:} Market volume vs. selling pressure analysis
    \item \textbf{Herd Behavior:} Coordination level detection when agents act similarly
\end{itemize}

\textbf{Cooperative Logic Constraints:} LTN constraints incentivize agents to maintain market stability:
\begin{itemize}
    \item \textbf{Market Maker of Last Resort:} Act as market maker when liquidity is low
    \item \textbf{Position Size Coordination:} Reduce position sizes when systemic risk is high
    \item \textbf{Staggered Execution:} Delay execution to prevent simultaneous actions
\end{itemize}

These constraints are encoded as LTN formulas and evaluated alongside trading constraints, with cooperative rewards (e.g., regulatory credits) incentivizing market-stabilizing behavior.

% Shortened title for TOC
\section[Future Directions]{Future Directions: Toward General Financial Intelligence}
Project JANUS is a stepping stone toward General Financial Intelligence (GFI). By moving away from ``curve fitting'' historical prices and toward agents that perceive, read, and reason, we are creating systems that understand the market as a complex, evolving sociotechnical system.

\subsection{Quantum Computing (Quant 5.0?)}
As we look beyond Quant 4.0, the integration of Quantum Computing offers the next leap. Quantum algorithms (e.g., Quantum Approximate Optimization Algorithm or QAOA) could solve the Almgren-Chriss portfolio optimization problem (which is NP-hard with certain constraints) exponentially faster than classical solvers. The ``Quantum Gramian Angular Field'' (QGAF) is already being explored as a way to encode time series into quantum states for processing on QPUs. JANUS is architected to be ``Quantum-Ready,'' with its modular design allowing the classical Logic/Optimization layers to be swapped for Quantum equivalents as the hardware matures.

\subsection{The Final Synthesis}
The ultimate vision of JANUS is a trading agent that possesses the ``Wisdom'' of a veteran trader (via Symbolic Logic), the ``Intuition'' of a chartist (via Visual Encoding), and the ``Speed'' of a machine (via Hierarchical Execution). By fusing these faculties, JANUS provides the rigorous mathematical and architectural framework to realize the promise of Neuro-Symbolic AI in finance—creating a system that is not only profitable but also principled.

\newpage
\section{Conclusion}
Project JANUS: The Neuro-Symbolic Visual Quant Architecture represents a fundamental advancement in quantitative trading systems. It directly addresses the limitations of Quant 3.0—opacity, brittleness, and safety risks—by synthesizing multiple theoretical innovations:

\begin{itemize}
    \item \textbf{DiffGAF:} Differentiable GAF with learnable normalization enables end-to-end gradient flow from classification back through visual encoding, allowing the normalization to adapt to market regimes.
    \item \textbf{GAF Video \& ViViT:} 3D spatiotemporal manifolds processed by factorized attention enable the detection of regime transitions and volatility clustering dynamics that static images cannot capture.
    \item \textbf{Lukasiewicz T-Norm:} Superior gradient flow compared to Product t-norm ensures stable optimization of LTN constraints, critical for learning constraint adherence.
    \item \textbf{Abductive Reasoning:} Explainable predicate inference provides human-readable justifications for classifications, essential for regulatory compliance.
    \item \textbf{Differentiable Simulation:} End-to-end gradient flow from PnL to actions enables efficient learning of execution strategies through backpropagation rather than pure policy search.
    \item \textbf{Feudal RL with HER:} Hierarchical architecture with hindsight experience replay achieves 10x sample efficiency improvement over monolithic agents.
    \item \textbf{Population-Based Training:} Evolutionary optimization discovers robust agent configurations that generalize to unseen market conditions.
\end{itemize}

The architecture detailed in this white paper offers a comprehensive blueprint for Quant 4.0/4.5: systems that are automated yet explainable, data-driven yet knowledge-constrained, and highly performant yet regulatory compliant. As financial markets become increasingly fast, complex, and interconnected, the ability to fuse multimodal perception with symbolic logic will define the alpha generators of the next decade. JANUS stands as the vanguard of this new era, demonstrating that the synthesis of visual intuition, logical rigor, and strategic hierarchy can create trading agents that are not only profitable but also principled, explainable, and robust.

% =============================================================================
% BIBLIOGRAPHY
% =============================================================================
\newpage
\begin{thebibliography}{99}
\raggedright % Forces bibliography to wrap URLs instead of stretching them

\bibitem{idea_research}
IDEA Research Report - arXiv. \url{https://arxiv.org/pdf/2301.04020}

\bibitem{multimodal_forecasting}
Multimodal Language Models with Modality-Specific Experts for Financial Forecasting from Interleaved Sequences of Text and Time Series - arXiv. \url{https://arxiv.org/html/2509.19628v1}

\bibitem{neurosymbolic_traders}
Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets - arXiv. \url{https://arxiv.org/html/2410.14587v1}

\bibitem{tcn_survey}
Temporal Convolutional Networks for Financial Time Series Forecasting: A Survey. \url{https://www.researchgate.net/publication/392102503_Temporal_Convolutional_Networks_for_Financial_Time_Series_Forecasting_A_Survey}

\bibitem{chronos_universal}
Chronos-2: From Univariate to Universal Forecasting - arXiv. \url{https://arxiv.org/html/2510.15821v1}

\bibitem{ltn_medium}
Logic Tensor Networks: The Neuro-Symbolic Revolution | by Gonnect. Medium. \url{https://medium.com/@gonnect.uk/logic-tensor-networks-the-neuro-symbolic-revolution-b4e2d022b188}

\bibitem{neurosymbolic_review}
Neuro-Symbolic AI in 2024: A Systematic Review - arXiv. \url{https://arxiv.org/pdf/2501.05435}

\bibitem{model_risk_fmsb}
Statement of Good Practice for the application of a model risk management framework to electronic trading algorithms - FMSB. \url{https://fmsb.com/wp-content/uploads/2025/04/Model-Risk-Electronic-Trading-Algorithm_FINAL-05.04.pdf}

\bibitem{deep_learning_llms}
From Deep Learning to LLMs: A survey of AI in Quantitative Investment - arXiv. \url{https://arxiv.org/html/2503.21422v1}

\bibitem{gaf_reddit}
Gramian Angular Fields keep popping up in time-series literature. Reddit. \url{https://www.reddit.com/r/quant/comments/1p81cuo/gramian_angular_fields_keep_popping_up_in/}

\bibitem{algorithmic_trading_cnn}
Algorithmic Financial Trading with Deep Convolutional Neural Networks: Time Series to Image Conversion Approach - ResearchGate. \url{https://www.researchgate.net/publication/324802031_Algorithmic_Financial_Trading_with_Deep_Convolutional_Neural_Networks_Time_Series_to_Image_Conversion_Approach}

\bibitem{volatility_cnn_bilstm}
Forecasting Stock Market Volatility Using CNN-BiLSTM-Attention Model with Mixed-Frequency Data - MDPI. \url{https://www.mdpi.com/2227-7390/13/11/1889}

\bibitem{cnn_financial_timeseries}
Using CNN for financial time series prediction - MachineLearningMastery.com. \url{https://machinelearningmastery.com/using-cnn-for-financial-time-series-prediction/}

\bibitem{rgb_gaf_image}
RGB GAF image: A possible solution to one weak point of Gramian Angular Field Imaging | by Shuyang Xiang. Medium. \url{https://medium.com/data-science/rgb-gaf-image-a-possible-solution-to-one-weak-point-of-gramian-angular-field-imaging-ffc6b31edfbe}

\bibitem{recurrence_plots_gaf}
Fusion of Recurrence Plots and Gramian Angular Fields with Bayesian Optimization for Enhanced Time-Series Classification - MDPI. \url{https://www.mdpi.com/2075-1680/14/7/528}

\bibitem{transfer_learning_gaf}
Transfer Learning in Financial Time Series with Gramian Angular Field - arXiv. \url{https://arxiv.org/html/2504.00378v1}

\bibitem{quantum_forecasting}
Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions - arXiv. \url{https://arxiv.org/abs/2310.07427}

\bibitem{wash_sale}
Wash Sale: Definition, How It Works, and Purpose - Investopedia. \url{https://www.investopedia.com/terms/w/washsale.asp}

\bibitem{almgren_chriss}
Optimal Execution Strategies - Almgren-Chriss Model - QuestDB. \url{https://questdb.com/glossary/optimal-execution-strategies-almgren-chriss-model/}

\bibitem{ltn_github}
logictensornetworks/logictensornetworks: Deep Learning and Logical Reasoning from Data and Knowledge - GitHub. \url{https://github.com/logictensornetworks/logictensornetworks}

\bibitem{ltn_arxiv}
Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge - arXiv. \url{https://arxiv.org/abs/1606.04422}

\bibitem{logical_neural_networks}
Logical Neural Networks using Pytorch | by Andrea Rosales. Medium. \url{https://medium.com/@andrea.rosales08/logical-neural-networks-using-pytorch-a017d96d3e05}

\bibitem{no_prior_mask}
No Prior Mask: Eliminate Redundant Action for Deep Reinforcement Learning - AAAI Publications. \url{https://ojs.aaai.org/index.php/AAAI/article/view/29652/31109}

\bibitem{vpin_quantresearch}
VPIN: The Volume Synchronized Probability of Informed Trading. \url{https://www.quantresearch.org/VPIN.pdf}

\bibitem{order_flow_toxicity}
From PIN to VPIN: An introduction to order flow toxicity. \url{https://www.quantresearch.org/From%20PIN%20to%20VPIN.pdf}

\bibitem{chronos_amazon_science}
Introducing Chronos-2: From univariate to universal forecasting - Amazon Science. \url{https://www.amazon.science/blog/introducing-chronos-2-from-univariate-to-universal-forecasting}

\bibitem{stock_prediction_multimodal}
Stock Movement Prediction with Multimodal Stable Fusion via Gated Cross-Attention Mechanism - arXiv. \url{https://arxiv.org/html/2406.06594v2}

\bibitem{finbert_arxiv}
Financial Sentiment Analysis Using FinBERT with Application in Predicting Stock Movement. \url{https://arxiv.org/html/2306.02136v3}

\bibitem{finbert_ijcai}
FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining - IJCAI. \url{https://www.ijcai.org/proceedings/2020/0622.pdf}

\bibitem{mm_itransformer}
MM-iTransformer: A Multimodal Approach to Economic Time Series Forecasting with Textual Data - MDPI. \url{https://www.mdpi.com/2076-3417/15/3/1241}

\bibitem{feudal_networks}
FeUdal Networks for Hierarchical Reinforcement Learning - arXiv. \url{https://arxiv.org/abs/1703.01161}

\bibitem{hrl_medium}
Hierarchical Reinforcement Learning: FeUdal Networks | by Austin Nguyen. Medium. \url{https://medium.com/data-science/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7}

\bibitem{hrl_eth}
Hierarchical Reinforcement Learning for Algorithmic Trading - ETH Zürich. \url{https://pub.tik.ee.ethz.ch/students/2021-HS/SA-2021-48.pdf}

\bibitem{hrt_arxiv}
Hierarchical Reinforced Trader (HRT): A Bi-Level Approach for Optimizing Stock Selection and Execution - arXiv. \url{https://arxiv.org/html/2410.14927v1}

\bibitem{high_freq_execution}
A High Frequency Trade Execution Model for Supervised Learning - arXiv. \url{https://arxiv.org/abs/1710.03870}

\bibitem{ntu_thesis}
Reinforcement Learning for Financial Trading: Algorithms, Evaluations and Platforms - Nanyang Technological University. \url{https://www3.ntu.edu.sg/home/boan/thesis/Sun_Shuo_PhD_Thesis.pdf}

\bibitem{chronos_forecasting_github}
amazon-science/chronos-forecasting: Chronos: Pretrained Models for Time Series Forecasting - GitHub. \url{https://github.com/amazon-science/chronos-forecasting}

\bibitem{chronos_bolt}
amazon/chronos-bolt-small - Hugging Face. \url{https://huggingface.co/amazon/chronos-bolt-small}

\bibitem{chronos_huggingface}
amazon/chronos-2 - Hugging Face. \url{https://huggingface.co/amazon/chronos-2}

\bibitem{fca_compliance}
Algorithmic Trading Compliance in Wholesale Markets - Financial Conduct Authority. \url{https://www.fca.org.uk/publication/multi-firm-reviews/algorithmic-trading-compliance-wholesale-markets.pdf}

\bibitem{safe_rl}
No More Hand-Tuning Rewards: Masked Constrained Policy Optimization for Safe Reinforcement Learning - IFAAMAS. \url{https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1344.pdf}

\bibitem{feudal_multiagent}
Feudal Multi-Agent Hierarchies for Cooperative Reinforcement Learning - ALA 2019. \url{https://ala2019.vub.ac.be/papers/ALA2019_paper_5.pdf}

\bibitem{quantum_portfolio}
Dynamic Portfolio Optimization with Real Datasets Using Quantum Processors and Quantum-Inspired Tensor Networks - arXiv. \url{https://arxiv.org/abs/2007.00017}

\end{thebibliography}

\end{document}
