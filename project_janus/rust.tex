\documentclass[12pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{pmboxdraw}
\usepackage{newunicodechar}
\usepackage[english]{babel}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{setspace}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

% --- Define Left-Aligned X Column for Tables ---
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

% --- URL BREAKING ---
\usepackage{xurl}
\usepackage{hyperref}

% --- CONFIGURATION ---
\onehalfspacing
\setlength{\headheight}{15pt}

\definecolor{janusblue}{RGB}{0, 51, 102}
\definecolor{accentgold}{RGB}{204, 153, 51}
\definecolor{codegray}{RGB}{245, 245, 245}
\definecolor{rustcolor}{RGB}{70, 130, 180}
\definecolor{pythoncolor}{RGB}{55, 118, 171}

% --- UNICODE CHARACTER DECLARATIONS ---
\newunicodechar{▼}{\ensuremath{\blacktriangledown}}
\newunicodechar{→}{\ensuremath{\rightarrow}}
\newunicodechar{←}{\ensuremath{\leftarrow}}
\newunicodechar{↔}{\ensuremath{\leftrightarrow}}
\newunicodechar{⇒}{\ensuremath{\Rightarrow}}
\newunicodechar{…}{\ldots}
\newunicodechar{≥}{\ensuremath{\geq}}
\newunicodechar{≤}{\ensuremath{\leq}}
\newunicodechar{≠}{\ensuremath{\neq}}
\newunicodechar{≈}{\ensuremath{\approx}}
\newunicodechar{∈}{\ensuremath{\in}}
\newunicodechar{∉}{\ensuremath{\notin}}
\newunicodechar{∧}{\ensuremath{\wedge}}
\newunicodechar{∨}{\ensuremath{\vee}}
\newunicodechar{¬}{\ensuremath{\neg}}
\newunicodechar{×}{\ensuremath{\times}}
\newunicodechar{÷}{\ensuremath{\div}}
\newunicodechar{∞}{\ensuremath{\infty}}
\newunicodechar{∑}{\ensuremath{\sum}}
\newunicodechar{∏}{\ensuremath{\prod}}
\newunicodechar{∫}{\ensuremath{\int}}
\newunicodechar{√}{\ensuremath{\sqrt}}
\newunicodechar{∂}{\ensuremath{\partial}}
\newunicodechar{∇}{\ensuremath{\nabla}}
\newunicodechar{α}{\ensuremath{\alpha}}
\newunicodechar{β}{\ensuremath{\beta}}
\newunicodechar{γ}{\ensuremath{\gamma}}
\newunicodechar{δ}{\ensuremath{\delta}}
\newunicodechar{ε}{\ensuremath{\epsilon}}
\newunicodechar{θ}{\ensuremath{\theta}}
\newunicodechar{λ}{\ensuremath{\lambda}}
\newunicodechar{μ}{\ensuremath{\mu}}
\newunicodechar{π}{\ensuremath{\pi}}
\newunicodechar{σ}{\ensuremath{\sigma}}
\newunicodechar{τ}{\ensuremath{\tau}}
\newunicodechar{φ}{\ensuremath{\phi}}
\newunicodechar{ω}{\ensuremath{\omega}}
\newunicodechar{Δ}{\ensuremath{\Delta}}
\newunicodechar{Σ}{\ensuremath{\Sigma}}
\newunicodechar{Π}{\ensuremath{\Pi}}
\newunicodechar{Ω}{\ensuremath{\Omega}}

\hypersetup{
    colorlinks=true,
    linkcolor=janusblue,
    citecolor=janusblue,
    urlcolor=accentgold,
    pdftitle={JANUS: Rust-First ML Implementation Strategy},
    pdfauthor={Jordan Smith}
}

% --- HEADER & FOOTER ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textbf{JANUS Rust Implementation}}
\fancyhead[R]{\textit{ML Migration Strategy}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% --- SECTION STYLING ---
\titleformat{\section}
  {\color{rustcolor}\normalfont\Large\bfseries}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\color{rustcolor}\normalfont\large\bfseries}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\color{rustcolor}\normalfont\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% --- CODE SNIPPET STYLE ---
\lstdefinelanguage{Rust}{
    morekeywords={fn, let, mut, pub, struct, enum, impl, trait, use, mod, crate, async, await, match, if, else, while, for, loop, return, break, continue, const, static, type, where, self, Self, super, unsafe, extern, dyn, Box, Vec, Option, Result, Some, None, Ok, Err},
    sensitive=true,
    morecomment=[l]{//},
    morecomment=[s]{/*}{*/},
    morestring=[b]",
}

\lstdefinelanguage{yaml}{
    keywords={true, false, null, yes, no},
    keywordstyle=\color{blue}\bfseries,
    sensitive=false,
    comment=[l]{\#},
    morestring=[b]',
    morestring=[b]",
}

\lstdefinestyle{rust}{
    language=Rust,
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{rustcolor},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    tabsize=4,
    extendedchars=true,
    literate={▼}{{\$\blacktriangledown\$}}1
             {─}{{-}}1
             {│}{{|}}1
             {├}{{+}}1
             {└}{{`}}1
             {→}{{->}}1
             {…}{{...}}1
             {≥}{{>=}}1
             {≤}{{<=}}1
             {∈}{{in}}1
             {∧}{{and}}1
             {∨}{{or}}1
             {¬}{{not}}1
             {≈}{{~}}1
             {×}{{*}}1
             {÷}{{/}}1
             {™}{{TM}}1
             {₀}{{\_0}}1
             {₁}{{\_1}}1
             {₂}{{\_2}}1
             {₃}{{\_3}}1
             {₄}{{\_4}}1
             {₅}{{\_5}}1
             {₆}{{\_6}}1
             {₇}{{\_7}}1
             {₈}{{\_8}}1
             {₉}{{\_9}}1
}

\lstdefinestyle{python}{
    language=Python,
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{pythoncolor},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    tabsize=4,
    extendedchars=true,
    literate={▼}{{\$\blacktriangledown\$}}1
}

\lstdefinestyle{yaml}{
    language=yaml,
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    tabsize=2,
    extendedchars=true,
}

\lstset{style=rust}

% --- MATH OPERATORS ---
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\sigmoid}{sigmoid}

% --- DOCUMENT START ---
\begin{document}

% =============================================================================
% TITLE PAGE
% =============================================================================
\begin{titlepage}
    \pagenumbering{gobble}
    \centering
    \vspace*{3cm}

    {\Huge \textbf{\textcolor{rustcolor}{JANUS}}} \\[0.3cm]
    {\LARGE \textbf{Rust-First ML Implementation}} \\[1.5cm]

    {\Large \textit{End-to-End Machine Learning with Rust}} \\[0.3cm]
    {\Large \textit{FastAPI Gateway \& Batch Processing Architecture}} \\[3cm]

    \textbf{\Large Classification: Architecture \& Implementation Strategy} \\[0.5cm]
    \textbf{\Large Version: 1.0} \\[3cm]

    \textbf{Author:} Jordan Smith \\
    \textit{github.com/nuniesmith} \\[0.5cm]
    \textbf{Date:} \today

    \vfill
    \begin{tcolorbox}[colback=codegray, colframe=rustcolor, width=0.85\textwidth]
    \centering
    \textbf{Migration Philosophy:}
    \begin{itemize}[leftmargin=*]
        \item \textbf{Core ML in Rust:} Maximum performance, safety, and type guarantees
        \item \textbf{Python Gateway:} FastAPI for orchestration and API surface
        \item \textbf{Hybrid Approach:} Use best tool for each layer
        \item \textbf{Modern Stack:} Latest technologies (Candle, ONNX, Burn, tch-rs)
    \end{itemize}
    \end{tcolorbox}
    \vfill
\end{titlepage}

% =============================================================================
% ABSTRACT
% =============================================================================
\newpage
\pagenumbering{arabic}
\thispagestyle{plain}
\section*{Abstract}

This document outlines a comprehensive strategy for implementing the JANUS trading system with a \textbf{Rust-first approach to machine learning}, while maintaining a Python FastAPI gateway for orchestration and external API exposure. The architecture leverages Rust's performance, memory safety, and type system for the entire ML pipeline—from data preprocessing to inference—while using Python strategically for high-level coordination, batch job scheduling, and developer-friendly interfaces.

\textbf{Key architectural decisions:}
\begin{itemize}
    \item \textbf{Hot path (Forward):} Pure Rust with ONNX Runtime or tch-rs for <100ms latency
    \item \textbf{Cold path (Backward):} Rust core with optional Python for experimentation
    \item \textbf{Gateway:} FastAPI service exposing REST/gRPC APIs
    \item \textbf{Batch processing:} Rust workers with async job queue
    \item \textbf{Training:} Hybrid Python (PyTorch) to Rust (ONNX/tch-rs) pipeline
\end{itemize}

This approach maximizes performance and safety while maintaining ecosystem compatibility and developer productivity.

\newpage
\tableofcontents
\newpage

% =============================================================================
% SECTION 1: ARCHITECTURAL OVERVIEW
% =============================================================================
\section{Architectural Overview}
\label{sec:architecture}

\subsection{The Rust-First Philosophy}

Traditional ML systems place Python at the center, with C++/Rust used only for performance-critical bottlenecks. JANUS inverts this model:

\begin{center}
\begin{tcolorbox}[colback=codegray, colframe=rustcolor, width=0.9\textwidth, title=\textbf{JANUS Layering}]
\begin{enumerate}
    \item \textbf{Core Layer (Rust):} All ML inference, data processing, logic evaluation
    \item \textbf{Service Layer (Rust):} gRPC services for Forward and Backward
    \item \textbf{Gateway Layer (Python FastAPI):} HTTP API, authentication, rate limiting
    \item \textbf{Training Layer (Hybrid):} PyTorch training → ONNX/tch export → Rust inference
\end{enumerate}
\end{tcolorbox}
\end{center}

\subsection{Component Diagram}

\begin{verbatim}
┌─────────────────────────────────────────────────────────────┐
│                   Python FastAPI Gateway                    │
│  • REST API endpoints                                       │
│  • Authentication & rate limiting                           │
│  • Job queue management (Celery/RQ)                        │
│  • Monitoring & logging aggregation                        │
└────────────────┬────────────────────────────┬───────────────┘
                 │ gRPC                       │ gRPC
                 ▼                            ▼
    ┌────────────────────────┐   ┌────────────────────────┐
    │   Forward Service      │   │   Backward Service     │
    │     (Rust + tch-rs)    │   │   (Rust + ndarray)     │
    ├────────────────────────┤   ├────────────────────────┤
    │ • GAF transformation   │   │ • Prioritized replay   │
    │ • ViViT inference      │   │ • Schema consolidation │
    │ • LTN evaluation       │   │ • UMAP projection      │
    │ • Decision engine      │   │ • Vector DB sync       │
    └────────────┬───────────┘   └───────────┬────────────┘
                 │                           │
                 └───────────┬───────────────┘
                             ▼
                   ┌──────────────────┐
                   │  Shared Crates   │
                   │   (Pure Rust)    │
                   ├──────────────────┤
                   │ • janus-core     │
                   │ • janus-vision   │
                   │ • janus-logic    │
                   │ • janus-memory   │
                   │ • janus-proto    │
                   └──────────────────┘
\end{verbatim}

\subsection{Design Principles}

\begin{enumerate}
    \item \textbf{Rust Everywhere Possible}
    \begin{itemize}
        \item All inference, data transformation, and business logic in Rust
        \item Zero-copy operations where possible
        \item Compile-time guarantees for correctness
    \end{itemize}

    \item \textbf{Python as Orchestration Layer}
    \begin{itemize}
        \item FastAPI for HTTP endpoints and developer UX
        \item Async job scheduling (Celery, Dramatiq, or Python RQ)
        \item High-level monitoring and alerting
    \end{itemize}

    \item \textbf{gRPC for Internal Communication}
    \begin{itemize}
        \item Type-safe, efficient service-to-service communication
        \item Shared protobuf definitions in \texttt{janus-proto}
        \item Streaming support for real-time data
    \end{itemize}

    \item \textbf{Gradual Migration Strategy}
    \begin{itemize}
        \item Start with inference in Rust (ONNX models)
        \item Migrate data preprocessing incrementally
        \item Keep training in PyTorch initially, export to Rust
        \item Eventually move training to Rust (Burn/Candle)
    \end{itemize}
\end{enumerate}

% =============================================================================
% SECTION 2: ML FRAMEWORK SELECTION
% =============================================================================
\section{Machine Learning Framework Strategy}
\label{sec:mlframeworks}

\subsection{Framework Comparison Matrix}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|L|L|L|}
\hline
\textbf{Framework} & \textbf{Pros} & \textbf{Cons} & \textbf{Use Case} \\
\hline
\textbf{tch-rs} &
Full PyTorch bindings, mature, GPU support &
Requires LibTorch, C++ dependency &
\textbf{Phase 1: Inference} \\
\hline
\textbf{ONNX Runtime} &
Lightweight, optimized, cross-platform &
Inference only, limited operators &
\textbf{Phase 1: Production} \\
\hline
\textbf{Candle (HF)} &
Pure Rust, no C++ deps, growing ecosystem &
Young, fewer pre-trained models &
\textbf{Phase 2: Long-term} \\
\hline
\textbf{Burn} &
Backend-agnostic, autodiff, training support &
Early stage, limited models &
\textbf{Phase 3: Full Rust} \\
\hline
\textbf{ndarray} &
NumPy-like API, stable, CPU optimized &
No GPU, no autodiff &
\textbf{Data processing} \\
\hline
\end{tabularx}
\caption{Rust ML Framework Comparison}
\end{table}

\subsection{Recommended Migration Path}

\subsubsection{Phase 1: Hybrid (Months 1-3)}
\begin{itemize}
    \item \textbf{Training:} PyTorch on GPU cluster
    \item \textbf{Export:} \texttt{torch.onnx.export()} or \texttt{torch.jit.save()}
    \item \textbf{Inference:} ONNX Runtime (\texttt{ort}) or tch-rs in Rust services
    \item \textbf{Why:} Fastest path to production, leverage existing PyTorch ecosystem
\end{itemize}

\subsubsection{Phase 2: Rust-Native Inference (Months 4-6)}
\begin{itemize}
    \item \textbf{Training:} Still PyTorch
    \item \textbf{Export:} Candle-compatible checkpoints
    \item \textbf{Inference:} Candle or custom Rust implementations
    \item \textbf{Why:} Eliminate LibTorch dependency, reduce binary size
\end{itemize}

\subsubsection{Phase 3: Full Rust ML (Months 7-12)}
\begin{itemize}
    \item \textbf{Training:} Burn or Candle with custom training loops
    \item \textbf{Inference:} Same framework as training
    \item \textbf{Why:} Complete type safety, no Python GIL, full control
\end{itemize}

\subsection{Framework Selection by Component}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|L|L|}
\hline
\textbf{Component} & \textbf{Framework (Phase 1)} & \textbf{Framework (Phase 3)} \\
\hline
GAF Transformation & \texttt{ndarray} & \texttt{ndarray} or \texttt{candle} \\
\hline
ViViT Inference & \texttt{ort} (ONNX) & \texttt{candle} \\
\hline
LTN Predicates & \texttt{tch-rs} & \texttt{candle} \\
\hline
Chronos-Bolt & \texttt{ort} (ONNX) & \texttt{candle} \\
\hline
FinBERT & \texttt{ort} (ONNX) & \texttt{candle-transformers} \\
\hline
Decision Engine & \texttt{tch-rs} & \texttt{burn} \\
\hline
PER Buffer & Custom Rust & Custom Rust \\
\hline
UMAP & \texttt{linfa-reduction} & \texttt{linfa-reduction} \\
\hline
\end{tabularx}
\caption{Component-Level Framework Selection}
\end{table}

% =============================================================================
% SECTION 3: FORWARD SERVICE (HOT PATH)
% =============================================================================
\section{Forward Service: Rust Implementation}
\label{sec:forward}

\subsection{Performance Requirements}

\begin{itemize}
    \item \textbf{End-to-end latency:} <100ms (target: 50ms)
    \item \textbf{Throughput:} >100 requests/second
    \item \textbf{Memory:} <2GB per instance
    \item \textbf{CPU:} Efficient use of multi-core (no GIL)
\end{itemize}

\subsection{Core Data Structures}

\begin{lstlisting}[style=rust]
// janus-core/src/types.rs
use ndarray::{Array1, Array2, Array3};

#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct MarketState {
    pub timestamp: i64,
    pub price_series: Array1<f32>,  // Last N prices
    pub volume_series: Array1<f32>, // Last N volumes
    pub lob_snapshot: LimitOrderBook,
    pub vpin: f32,
    pub volatility: f32,
}

#[derive(Clone, Debug)]
pub struct LimitOrderBook {
    pub bids: Vec<(f32, f32)>,  // (price, size)
    pub asks: Vec<(f32, f32)>,
}

#[derive(Clone, Debug)]
pub enum Action {
    Buy { size: f32 },
    Sell { size: f32 },
    Hold,
}

#[derive(Clone, Debug)]
pub struct Decision {
    pub action: Action,
    pub confidence: f32,
    pub ltn_satisfaction: f32,
    pub risk_score: f32,
    pub metadata: HashMap<String, f32>,
}
\end{lstlisting}

\subsection{GAF Transformation Module}

\begin{lstlisting}[style=rust]
// janus-vision/src/gaf.rs
use ndarray::{Array1, Array2};

pub struct GafTransformer {
    alpha: f32,  // Learnable normalization param
    beta: f32,   // Learnable normalization param
}

impl GafTransformer {
    pub fn transform(&self, series: &Array1<f32>) -> (Array2<f32>, Array2<f32>) {
        // Step 1: Normalize to [-1, 1]
        let min = series.iter().cloned().fold(f32::INFINITY, f32::min);
        let max = series.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let normalized = series.mapv(|x| {
            let norm = (x - min) / (max - min + 1e-8);
            (norm * self.alpha + self.beta).tanh()
        });

        // Step 2: Polar coordinates
        let phi = normalized.mapv(|x| x.acos());

        // Step 3: Gramian fields
        let n = series.len();
        let mut gasf = Array2::<f32>::zeros((n, n));
        let mut gadf = Array2::<f32>::zeros((n, n));

        for i in 0..n {
            for j in 0..n {
                gasf[[i, j]] = (phi[i] + phi[j]).cos();
                gadf[[i, j]] = (phi[i] - phi[j]).sin();
            }
        }

        (gasf, gadf)
    }

    pub fn video(&self, series: &Array1<f32>, window: usize, stride: usize, frames: usize)
        -> Array3<f32>
    {
        let mut video_frames = Vec::new();

        for f in 0..frames {
            let start = f * stride;
            let end = start + window;
            if end > series.len() {
                break;
            }
            let window_series = series.slice(s![start..end]).to_owned();
            let (gasf, gadf) = self.transform(&window_series);

            // Stack GASF and GADF as channels
            video_frames.push(gasf);
            video_frames.push(gadf);
        }

        // Stack into 3D tensor: [frames, 2, window, window]
        stack_frames(&video_frames, frames, window)
    }
}
\end{lstlisting}

\subsection{LTN Constraint Evaluation}

\begin{lstlisting}[style=rust]
// janus-logic/src/ltn.rs
use tch::{Tensor, nn};

pub struct LogicTensorNetwork {
    predicates: HashMap<String, PredicateNet>,
}

pub struct PredicateNet {
    model: nn::Sequential,
}

impl PredicateNet {
    pub fn evaluate(&self, embedding: &Tensor) -> f32 {
        let logit = self.model.forward(embedding);
        logit.sigmoid().double_value(&[]) as f32
    }
}

impl LogicTensorNetwork {
    pub fn check_wash_sale(&self, action: &Action, history: &[Action]) -> f32 {
        // Encode wash sale constraint
        // ∀t, ∀k ∈ [1,30]: ¬(SaleAtLoss(t) ∧ Buy(t+k))
        let mut min_satisfaction = 1.0;

        // Implementation of Łukasiewicz t-norm
        for (t, past_action) in history.iter().enumerate() {
            if let Action::Sell { .. } = past_action {
                if is_loss(past_action) {
                    for k in 1..=30 {
                        if t + k < history.len() {
                            if let Action::Buy { .. } = &history[t + k] {
                                // Łukasiewicz conjunction: max(0, a + b - 1)
                                let conjunction = (1.0 + 1.0 - 1.0).max(0.0);
                                // Negation: 1 - conjunction
                                let satisfaction = 1.0 - conjunction;
                                min_satisfaction = min_satisfaction.min(satisfaction);
                            }
                        }
                    }
                }
            }
        }

        min_satisfaction
    }

    pub fn check_almgren_chriss(&self, state: &MarketState, action: &Action) -> f32 {
        let volatility = state.volatility;
        let avg_volume = state.volume_series.mean().unwrap();

        match action {
            Action::Buy { size } | Action::Sell { size } => {
                let eta = 0.1;  // Market impact coefficient
                let threshold = eta * volatility * (size / avg_volume).sqrt();

                // If impact < threshold, satisfaction = 1.0
                // Smooth approximation with sigmoid
                let impact_ratio = size / (avg_volume * threshold);
                1.0 - impact_ratio.min(1.0)
            }
            Action::Hold => 1.0,
        }
    }

    pub fn check_vpin(&self, state: &MarketState) -> f32 {
        let vpin_threshold = 0.7;

        // If VPIN > threshold, should halt trading
        if state.vpin > vpin_threshold {
            0.0  // Constraint violated
        } else {
            1.0  // Constraint satisfied
        }
    }

    pub fn evaluate_all(&self, state: &MarketState, action: &Action, history: &[Action]) -> f32 {
        let wash_sale = self.check_wash_sale(action, history);
        let almgren = self.check_almgren_chriss(state, action);
        let vpin = self.check_vpin(state);

        // Generalized mean (p=2 for quadratic mean)
        let constraints = vec![wash_sale, almgren, vpin];
        let sum_squared: f32 = constraints.iter().map(|x| x.powi(2)).sum();
        (sum_squared / constraints.len() as f32).sqrt()
    }
}
\end{lstlisting}

\subsection{Model Inference with ONNX Runtime}

\begin{lstlisting}[style=rust]
// janus-vision/src/vivit.rs
use ort::{Environment, Session, SessionBuilder, Value};
use ndarray::Array4;

pub struct ViViTInference {
    session: Session,
    env: Arc<Environment>,
}

impl ViViTInference {
    pub fn new(model_path: &str) -> Result<Self, ort::OrtError> {
        let env = Arc::new(Environment::builder().build()?);
        let session = SessionBuilder::new(&env)?
            .with_optimization_level(ort::GraphOptimizationLevel::Level3)?
            .with_intra_threads(4)?
            .with_model_from_file(model_path)?;

        Ok(Self { session, env })
    }

    pub fn infer(&self, video: &Array4<f32>) -> Result<Array1<f32>, ort::OrtError> {
        // Convert ndarray to ONNX tensor
        let input_tensor = Value::from_array(self.session.allocator(), video)?;

        // Run inference
        let outputs = self.session.run(vec![input_tensor])?;

        // Extract output
        let output_tensor = outputs[0].try_extract()?;
        let embedding = output_tensor.view().to_owned();

        Ok(embedding)
    }
}
\end{lstlisting}

\subsection{Async Service with Tokio}

\begin{lstlisting}[style=rust]
// services/forward/src/main.rs
use tonic::{transport::Server, Request, Response, Status};
use janus_proto::forward_service_server::{ForwardService, ForwardServiceServer};
use janus_proto::{DecisionRequest, DecisionResponse};

pub struct ForwardServiceImpl {
    gaf_transformer: Arc<GafTransformer>,
    vivit_model: Arc<ViViTInference>,
    ltn: Arc<LogicTensorNetwork>,
    decision_engine: Arc<DecisionEngine>,
}

#[tonic::async_trait]
impl ForwardService for ForwardServiceImpl {
    async fn get_decision(
        &self,
        request: Request<DecisionRequest>,
    ) -> Result<Response<DecisionResponse>, Status> {
        let req = request.into_inner();

        // Parse market state
        let state = parse_market_state(&req)?;

        // 1. GAF transformation
        let video = self.gaf_transformer.video(
            &state.price_series,
            60,  // window
            10,  // stride
            16,  // frames
        );

        // 2. ViViT inference
        let visual_embedding = self.vivit_model.infer(&video)
            .map_err(|e| Status::internal(format!("ViViT inference failed: {}", e)))?;

        // 3. LTN evaluation
        let ltn_satisfaction = self.ltn.evaluate_all(&state, &Action::Hold, &[]);

        // 4. Decision engine
        let decision = self.decision_engine.decide(
            &state,
            &visual_embedding,
            ltn_satisfaction,
        )?;

        // 5. Build response
        let response = DecisionResponse {
            action: decision.action.to_string(),
            confidence: decision.confidence,
            ltn_satisfaction: decision.ltn_satisfaction,
            risk_score: decision.risk_score,
            metadata: decision.metadata,
        };

        Ok(Response::new(response))
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let addr = "0.0.0.0:50051".parse()?;

    let service = ForwardServiceImpl {
        gaf_transformer: Arc::new(GafTransformer::new(1.0, 0.0)),
        vivit_model: Arc::new(ViViTInference::new("models/vivit.onnx")?),
        ltn: Arc::new(LogicTensorNetwork::load("models/ltn")?),
        decision_engine: Arc::new(DecisionEngine::new()),
    };

    println!("Forward service listening on {}", addr);

    Server::builder()
        .add_service(ForwardServiceServer::new(service))
        .serve(addr)
        .await?;

    Ok(())
}
\end{lstlisting}

% =============================================================================
% SECTION 4: BACKWARD SERVICE (COLD PATH)
% =============================================================================
\section{Backward Service: Batch Processing}
\label{sec:backward}

\subsection{Performance Requirements}

\begin{itemize}
    \item \textbf{Latency:} Not critical (batch processing)
    \item \textbf{Throughput:} Process 10k-100k transitions per sleep cycle
    \item \textbf{Memory:} Can use up to 16GB for large batches
    \item \textbf{Parallelism:} Leverage all CPU cores with Rayon
\end{itemize}

\subsection{Prioritized Experience Replay}

\begin{lstlisting}[style=rust]
// janus-memory/src/replay.rs
use rayon::prelude::*;
use rand::distributions::WeightedIndex;
use rand::prelude::*;

pub struct PrioritizedReplayBuffer {
    buffer: Vec<Transition>,
    priorities: Vec<f32>,
    capacity: usize,
    alpha: f32,  // Prioritization exponent
    beta: f32,   // Importance sampling correction
}

impl PrioritizedReplayBuffer {
    pub fn sample(&self, batch_size: usize) -> (Vec<Transition>, Vec<f32>) {
        let probabilities: Vec<f32> = self.priorities
            .iter()
            .map(|p| p.powf(self.alpha))
            .collect();

        let total: f32 = probabilities.iter().sum();
        let normalized_probs: Vec<f32> = probabilities
            .iter()
            .map(|p| p / total)
            .collect();

        let dist = WeightedIndex::new(&normalized_probs).unwrap();
        let mut rng = thread_rng();

        let indices: Vec<usize> = (0..batch_size)
            .map(|_| dist.sample(&mut rng))
            .collect();

        // Compute importance sampling weights
        let n = self.buffer.len() as f32;
        let weights: Vec<f32> = indices
            .iter()
            .map(|&i| {
                let prob = normalized_probs[i];
                ((1.0 / n) * (1.0 / prob)).powf(self.beta)
            })
            .collect();

        // Normalize weights
        let max_weight = weights.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let normalized_weights: Vec<f32> = weights
            .iter()
            .map(|w| w / max_weight)
            .collect();

        let transitions: Vec<Transition> = indices
            .iter()
            .map(|&i| self.buffer[i].clone())
            .collect();

        (transitions, normalized_weights)
    }

    pub fn update_priorities(&mut self, indices: Vec<usize>, td_errors: Vec<f32>,
                             logic_violations: Vec<f32>, rewards: Vec<f32>) {
        for (idx, (&i, (&td, (&vio, &rew)))) in indices.iter()
            .zip(td_errors.iter()
                .zip(logic_violations.iter()
                    .zip(rewards.iter())))
            .enumerate()
        {
            // Priority = |TD-error| + λ_logic * violation + λ_reward * |reward|
            self.priorities[i] = td.abs() + 2.0 * vio + 0.5 * rew.abs();
        }
    }
}
\end{lstlisting}

\subsection{Schema Consolidation}

\begin{lstlisting}[style=rust]
// janus-memory/src/schema.rs
use ndarray::{Array1, Array2};

pub struct SchemaMemory {
    schemas: Vec<Schema>,
    recall_threshold: f32,
    logic_threshold: f32,
}

impl SchemaMemory {
    pub fn consolidate(&mut self, transitions: &[Transition], ltn: &LogicTensorNetwork) {
        for transition in transitions {
            // Compute gates
            let recall_gate = self.compute_recall_gate(&transition.state);
            let logic_gate = ltn.evaluate_all(&transition.state, &transition.action, &[]);

            // Only consolidate if both gates pass
            if recall_gate > self.recall_threshold && logic_gate > self.logic_threshold {
                // Find best matching schema
                let (best_idx, best_likelihood) = self.find_best_schema(&transition.state);

                if best_likelihood < 0.01 {
                    // Create new schema
                    self.create_schema(&transition.state);
                } else {
                    // Update existing schema
                    self.update_schema(best_idx, &transition.state, 0.01);
                }
            }
        }
    }

    fn update_schema(&mut self, idx: usize, state: &Array1<f32>, lr: f32) {
        let schema = &mut self.schemas[idx];

        // Update mean: μ = (1-η)μ + η*x
        let diff = state - &schema.mean;
        schema.mean = &schema.mean + lr * &diff;

        // Update covariance: Σ = (1-η)Σ + η*(x-μ)(x-μ)ᵀ
        let outer = outer_product(&diff, &diff);
        schema.covariance = (1.0 - lr) * &schema.covariance + lr * outer;

        schema.num_points += 1;
    }
}
\end{lstlisting}

\subsection{Parallel Sleep Cycle}

\begin{lstlisting}[style=rust]
// services/backward/src/sleep_cycle.rs
use rayon::prelude::*;

pub async fn run_sleep_cycle(
    replay_buffer: &mut PrioritizedReplayBuffer,
    schema_memory: &mut SchemaMemory,
    ltn: &LogicTensorNetwork,
    policy: &mut Policy,
) -> Result<SleepCycleMetrics, BackwardError> {
    let start = std::time::Instant::now();
    let mut metrics = SleepCycleMetrics::default();

    // Phase 1: Prioritized Replay (1000 iterations)
    for iteration in 0..1000 {
        let (batch, weights) = replay_buffer.sample(256);

        // Parallel TD-error computation
        let td_errors: Vec<f32> = batch.par_iter()
            .map(|transition| compute_td_error(transition, policy))
            .collect();

        // Parallel logic violation scoring
        let violations: Vec<f32> = batch.par_iter()
            .map(|transition| {
                1.0 - ltn.evaluate_all(&transition.state, &transition.action, &[])
            })
            .collect();

        // Update policy with importance-weighted gradients
        policy.update(&batch, &weights);

        // Update priorities
        let rewards: Vec<f32> = batch.iter().map(|t| t.reward).collect();
        let indices: Vec<usize> = (0..batch.len()).collect();
        replay_buffer.update_priorities(indices, td_errors, violations, rewards);

        if iteration % 100 == 0 {
            println!("Replay iteration {}/1000", iteration);
        }
    }

    // Phase 2: Schema Consolidation
    let all_transitions: Vec<Transition> = replay_buffer.buffer.clone();
    schema_memory.consolidate(&all_transitions, ltn);
    metrics.num_schemas = schema_memory.schemas.len();

    // Phase 3: UMAP Update
    let embeddings = extract_schema_embeddings(schema_memory);
    let umap_projection = fit_aligned_umap(&embeddings)?;
    metrics.num_clusters = detect_clusters(&umap_projection);

    // Phase 4: Vector DB Sync
    sync_to_qdrant(schema_memory).await?;

    metrics.duration = start.elapsed();
    Ok(metrics)

\end{lstlisting}

% =============================================================================
% SECTION 5: PYTHON FASTAPI GATEWAY
% =============================================================================
\section{Python FastAPI Gateway}
\label{sec:gateway}

\subsection{Architecture Purpose}

The Python gateway serves as:
\begin{itemize}
    \item \textbf{API Surface:} User-friendly REST endpoints
    \item \textbf{Authentication:} JWT tokens, API key validation
    \item \textbf{Rate Limiting:} Per-user request throttling
    \item \textbf{Job Scheduling:} Async batch processing with Celery/Dramatiq
    \item \textbf{Monitoring:} Metrics aggregation and alerting
\end{itemize}

\subsection{FastAPI Service Implementation}

\begin{lstlisting}[style=python]
# gateway/main.py
from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import grpc
from pydantic import BaseModel
import asyncio

# Generated from proto files
from janus_proto import forward_service_pb2, forward_service_pb2_grpc
from janus_proto import backward_service_pb2, backward_service_pb2_grpc

app = FastAPI(title="JANUS Trading System API")
security = HTTPBearer()

# gRPC channel pool
forward_channel = grpc.aio.insecure_channel('forward-service:50051')
backward_channel = grpc.aio.insecure_channel('backward-service:50052')

forward_stub = forward_service_pb2_grpc.ForwardServiceStub(forward_channel)
backward_stub = backward_service_pb2_grpc.BackwardServiceStub(backward_channel)

class MarketStateRequest(BaseModel):
    timestamp: int
    price_series: list[float]
    volume_series: list[float]
    lob_bids: list[tuple[float, float]]
    lob_asks: list[tuple[float, float]]

class DecisionResponse(BaseModel):
    action: str
    confidence: float
    ltn_satisfaction: float
    risk_score: float
    latency_ms: float

@app.post("/api/v1/decision", response_model=DecisionResponse)
async def get_trading_decision(
    request: MarketStateRequest,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """Get real-time trading decision from Forward service."""
    # Authenticate
    validate_token(credentials.credentials)

    # Convert to protobuf
    grpc_request = forward_service_pb2.DecisionRequest(
        timestamp=request.timestamp,
        price_series=request.price_series,
        volume_series=request.volume_series,
        # ... other fields
    )

    # Call Rust Forward service via gRPC
    start = asyncio.get_event_loop().time()
    try:
        grpc_response = await forward_stub.GetDecision(grpc_request)
    except grpc.RpcError as e:
        raise HTTPException(status_code=503, detail=f"Forward service unavailable: {e}")

    latency = (asyncio.get_event_loop().time() - start) * 1000

    return DecisionResponse(
        action=grpc_response.action,
        confidence=grpc_response.confidence,
        ltn_satisfaction=grpc_response.ltn_satisfaction,
        risk_score=grpc_response.risk_score,
        latency_ms=latency,
    )

@app.post("/api/v1/sleep-cycle")
async def trigger_sleep_cycle(background_tasks: BackgroundTasks):
    """Trigger memory consolidation (runs asynchronously)."""
    # Enqueue background job
    background_tasks.add_task(run_sleep_cycle_job)
    return {"status": "sleep_cycle_queued", "job_id": "unique-job-id"}

async def run_sleep_cycle_job():
    """Background task that calls Backward service."""
    grpc_request = backward_service_pb2.SleepCycleRequest()

    try:
        grpc_response = await backward_stub.RunSleepCycle(grpc_request)
        print(f"Sleep cycle completed: {grpc_response.num_schemas} schemas")
    except grpc.RpcError as e:
        print(f"Sleep cycle failed: {e}")

# Monitoring endpoints
@app.get("/api/v1/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "forward_service": await check_forward_health(),
        "backward_service": await check_backward_health(),
    }

@app.get("/api/v1/metrics")
async def get_metrics():
    """Prometheus metrics endpoint."""
    # Aggregate metrics from Rust services
    forward_metrics = await forward_stub.GetMetrics(empty_pb2.Empty())
    backward_metrics = await backward_stub.GetMetrics(empty_pb2.Empty())

    return {
        "forward": forward_metrics,
        "backward": backward_metrics,
    }
\end{lstlisting}

\subsection{Batch Job Processing with Celery}

\begin{lstlisting}[style=python]
# gateway/tasks.py
from celery import Celery
import grpc

celery_app = Celery('janus', broker='redis://localhost:6379/0')

@celery_app.task(bind=True, max_retries=3)
def run_sleep_cycle(self):
    """Celery task for long-running sleep cycle."""
    channel = grpc.insecure_channel('backward-service:50052')
    stub = backward_service_pb2_grpc.BackwardServiceStub(channel)

    try:
        request = backward_service_pb2.SleepCycleRequest()
        response = stub.RunSleepCycle(request)

        return {
            "status": "success",
            "num_schemas": response.num_schemas,
            "duration_seconds": response.duration_seconds,
        }
    except grpc.RpcError as e:
        # Retry on failure
        raise self.retry(exc=e, countdown=60)

@celery_app.task
def backtest_strategy(start_date: str, end_date: str):
    """Long-running backtesting job."""
    # Call Backward service for historical simulation
    # This can take hours, so runs in background
    pass
\end{lstlisting}

% =============================================================================
% SECTION 6: TRAINING PIPELINE
% =============================================================================
\section{Hybrid Training Pipeline}
\label{sec:training}

\subsection{PyTorch Training (Phase 1)}

\begin{lstlisting}[style=python]
# training/train_vivit.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

class ViViTTrainer:
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    def train_epoch(self, dataloader):
        self.model.train()
        total_loss = 0

        for batch in dataloader:
            videos, labels = batch
            videos = videos.to(self.device)
            labels = labels.to(self.device)

            # Forward pass
            outputs = self.model(videos)
            loss = nn.CrossEntropyLoss()(outputs, labels)

            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / len(dataloader)

    def export_to_onnx(self, path: str):
        """Export trained model to ONNX for Rust inference."""
        self.model.eval()
        dummy_input = torch.randn(1, 16, 2, 60, 60).to(self.device)

        torch.onnx.export(
            self.model,
            dummy_input,
            path,
            export_params=True,
            opset_version=14,
            do_constant_folding=True,
            input_names=['video'],
            output_names=['embedding'],
            dynamic_axes={
                'video': {0: 'batch_size'},
                'embedding': {0: 'batch_size'}
            }
        )
        print(f"Model exported to {path}")

# Usage
trainer = ViViTTrainer(model)
for epoch in range(100):
    loss = trainer.train_epoch(train_loader)
    print(f"Epoch {epoch}: Loss = {loss}")

# Export to ONNX
trainer.export_to_onnx("models/vivit.onnx")
\end{lstlisting}

\subsection{Model Export \& Validation}

\begin{lstlisting}[style=python]
# training/validate_export.py
import torch
import onnx
import onnxruntime as ort
import numpy as np

def validate_onnx_export(pytorch_model, onnx_path):
    """Validate that ONNX export matches PyTorch output."""

    # Load ONNX model
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)

    # Create ONNX Runtime session
    ort_session = ort.InferenceSession(onnx_path)

    # Test input
    dummy_input = torch.randn(1, 16, 2, 60, 60)

    # PyTorch inference
    pytorch_model.eval()
    with torch.no_grad():
        pytorch_output = pytorch_model(dummy_input).numpy()

    # ONNX inference
    ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.numpy()}
    onnx_output = ort_session.run(None, ort_inputs)[0]

    # Compare outputs
    diff = np.abs(pytorch_output - onnx_output).max()
    print(f"Max difference: {diff}")

    if diff < 1e-5:
        print("[OK] ONNX export validated successfully")
    else:
        print("[FAIL] ONNX export validation failed")

validate_onnx_export(model, "models/vivit.onnx")
\end{lstlisting}

% =============================================================================
% SECTION 7: DEPLOYMENT & OPERATIONS
% =============================================================================
\section{Deployment Architecture}
\label{sec:deployment}

\subsection{Docker Compose Setup}

\begin{lstlisting}[language=yaml]
# docker-compose.yml
version: '3.8'

services:
  gateway:
    build: ./gateway
    ports:
      - "8000:8000"
    environment:
      - FORWARD_SERVICE_URL=forward:50051
      - BACKWARD_SERVICE_URL=backward:50052
    depends_on:
      - forward
      - backward
      - redis

  forward:
    build: ./services/forward
    ports:
      - "50051:50051"
    volumes:
      - ./models:/models:ro
    environment:
      - RUST_LOG=info
      - MODEL_PATH=/models/vivit.onnx
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 2G

  backward:
    build: ./services/backward
    ports:
      - "50052:50052"
    volumes:
      - ./models:/models:ro
      - ./data:/data
    environment:
      - RUST_LOG=info
      - QDRANT_URL=http://qdrant:6333
    depends_on:
      - qdrant

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  celery_worker:
    build: ./gateway
    command: celery -A tasks worker --loglevel=info
    depends_on:
      - redis
      - backward

volumes:
  qdrant_data:
\end{lstlisting}

\subsection{Kubernetes Deployment}

\begin{lstlisting}[language=yaml]
# k8s/forward-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: janus-forward
spec:
  replicas: 3
  selector:
    matchLabels:
      app: janus-forward
  template:
    metadata:
      labels:
        app: janus-forward
    spec:
      containers:
      - name: forward
        image: janus/forward:latest
        ports:
        - containerPort: 50051
        resources:
          requests:
            memory: "1Gi"
            cpu: "2"
          limits:
            memory: "2Gi"
            cpu: "4"
        env:
        - name: RUST_LOG
          value: "info"
        - name: MODEL_PATH
          value: "/models/vivit.onnx"
        volumeMounts:
        - name: models
          mountPath: /models
          readOnly: true
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: model-storage
---
apiVersion: v1
kind: Service
metadata:
  name: janus-forward
spec:
  selector:
    app: janus-forward
  ports:
  - protocol: TCP
    port: 50051
    targetPort: 50051
  type: ClusterIP
\end{lstlisting}

% =============================================================================
% SECTION 8: IMPLEMENTATION ROADMAP
% =============================================================================
\section{Implementation Roadmap}
\label{sec:roadmap}

\subsection{Phase 1: Foundation (Weeks 1-4)}

\begin{enumerate}
    \item \textbf{Week 1: Project Setup}
    \begin{itemize}
        \item[$\square$] Initialize Rust workspace with cargo
        \item[$\square$] Set up CI/CD (GitHub Actions)
        \item[$\square$] Create protobuf definitions
        \item[$\square$] Generate gRPC stubs for Rust and Python
    \end{itemize}

    \item \textbf{Week 2: Core Crates}
    \begin{itemize}
        \item[$\square$] Implement \texttt{janus-core} with domain types
        \item[$\square$] Implement \texttt{janus-vision} GAF transformation
        \item[$\square$] Add comprehensive unit tests
    \end{itemize}

    \item \textbf{Week 3: Model Integration}
    \begin{itemize}
        \item[$\square$] Train ViViT model in PyTorch
        \item[$\square$] Export to ONNX and validate
        \item[$\square$] Integrate ONNX Runtime in Rust
        \item[$\square$] Benchmark inference latency
    \end{itemize}

    \item \textbf{Week 4: Basic Services}
    \begin{itemize}
        \item[$\square$] Implement Forward service skeleton
        \item[$\square$] Implement Backward service skeleton
        \item[$\square$] Set up FastAPI gateway
        \item[$\square$] Test end-to-end flow
    \end{itemize}
\end{enumerate}

\subsection{Phase 2: Core Features (Weeks 5-8)}

\begin{enumerate}
    \item \textbf{Week 5: LTN Implementation}
    \begin{itemize}
        \item[$\square$] Implement Łukasiewicz t-norms
        \item[$\square$] Encode wash sale constraint
        \item[$\square$] Encode Almgren-Chriss constraint
        \item[$\square$] Encode VPIN constraint
    \end{itemize}

    \item \textbf{Week 6: Decision Engine}
    \begin{itemize}
        \item[$\square$] Implement dual pathway logic
        \item[$\square$] Add risk gating
        \item[$\square$] Add logic gating
        \item[$\square$] Implement cerebellar forward model
    \end{itemize}

    \item \textbf{Week 7: Memory System}
    \begin{itemize}
        \item[$\square$] Implement episodic buffer
        \item[$\square$] Implement prioritized replay
        \item[$\square$] Add schema consolidation
    \end{itemize}

    \item \textbf{Week 8: Vector Database}
    \begin{itemize}
        \item[$\square$] Set up Qdrant
        \item[$\square$] Implement schema storage
        \item[$\square$] Implement similarity search
        \item[$\square$] Add periodic pruning
    \end{itemize}
\end{enumerate}

\subsection{Phase 3: Production Readiness (Weeks 9-12)}

\begin{enumerate}
    \item \textbf{Week 9: Performance Optimization}
    \begin{itemize}
        \item[$\square$] Profile and optimize hot paths
        \item[$\square$] Add SIMD optimizations
        \item[$\square$] Implement model quantization
        \item[$\square$] Benchmark end-to-end latency
    \end{itemize}

    \item \textbf{Week 10: Safety \& Testing}
    \begin{itemize}
        \item[$\square$] Remove all panic!() calls
        \item[$\square$] Add comprehensive error handling
        \item[$\square$] Write integration tests
        \item[$\square$] Add property-based tests
    \end{itemize}

    \item \textbf{Week 11: Monitoring \& Observability}
    \begin{itemize}
        \item[$\square$] Add Prometheus metrics
        \item[$\square$] Implement distributed tracing
        \item[$\square$] Set up alerting
        \item[$\square$] Create dashboards
    \end{itemize}

    \item \textbf{Week 12: Deployment}
    \begin{itemize}
        \item[$\square$] Create Docker images
        \item[$\square$] Write Kubernetes manifests
        \item[$\square$] Set up staging environment
        \item[$\square$] Run load tests
    \end{itemize}
\end{enumerate}

% =============================================================================
% SECTION 9: MIGRATION CHECKLIST
% =============================================================================
\section{Complete Implementation Checklist}
{sec:checklist}

\subsection{Infrastructure Setup}

\begin{itemize}
    \item[$\square$] Create Rust workspace (\texttt{Cargo.toml})
    \item[$\square$] Set up monorepo structure (services + crates)
    \item[$\square$] Configure CI/CD pipeline
    \item[$\square$] Set up Docker build system
    \item[$\square$] Configure linting (clippy, rustfmt)
\end{itemize}

\subsection{Shared Crates}

\begin{itemize}
    \item[$\square$] \textbf{janus-core}: Domain types, errors, utilities
    \item[$\square$] \textbf{janus-vision}: GAF, ViViT, image processing
    \item[$\square$] \textbf{janus-logic}: LTN, constraint evaluation
    \item[$\square$] \textbf{janus-memory}: Replay buffer, schemas
    \item[$\square$] \textbf{janus-execution}: Order execution, market simulation
    \item[$\square$] \textbf{janus-proto}: Protobuf definitions and gRPC stubs
\end{itemize}

\subsection{Forward Service (Rust)}

\begin{itemize}
    \item[$\square$] gRPC server setup with Tonic
    \item[$\square$] GAF transformation pipeline
    \item[$\square$] ONNX model loading and inference
    \item[$\square$] LTN constraint evaluation
    \item[$\square$] Multimodal fusion
    \item[$\square$] Decision engine with dual pathways
    \item[$\square$] Metrics export (Prometheus)
    \item[$\square$] Health check endpoint
    \item[$\square$] Graceful shutdown
\end{itemize}

\subsection{Backward Service (Rust)}

\begin{itemize}
    \item[$\square$] gRPC server setup
    \item[$\square$] Episodic buffer implementation
    \item[$\square$] Prioritized replay sampling
    \item[$\square$] TD-error computation
    \item[$\square$] Schema consolidation
    \item[$\square$] UMAP integration
    \item[$\square$] Qdrant client and sync
    \item[$\square$] Sleep cycle orchestration
    \item[$\square$] Parallel batch processing
\end{itemize}

\subsection{Gateway Service (Python)}

\begin{itemize}
    \item[$\square$] FastAPI application setup
    \item[$\square$] gRPC client stubs
    \item[$\square$] REST endpoints (/decision, /sleep-cycle, etc.)
    \item[$\square$] JWT authentication
    \item[$\square$] Rate limiting middleware
    \item[$\square$] Celery worker setup
    \item[$\square$] Background job management
    \item[$\square$] Health check aggregation
    \item[$\square$] Metrics aggregation
\end{itemize}

\subsection{Training Pipeline (Python)}

\begin{itemize}
    \item[$\square$] PyTorch model definitions
    \item[$\square$] Data loaders and preprocessing
    \item[$\square$] Training loops
    \item[$\square$] ONNX export scripts
    \item[$\square$] Export validation
    \item[$\square$] Model versioning
\end{itemize}

\subsection{Testing}

\begin{itemize}
    \item[$\square$] Unit tests for all crates
    \item[$\square$] Integration tests for services
    \item[$\square$] End-to-end tests
    \item[$\square$] Load tests
    \item[$\square$] Chaos testing
\end{itemize}

\subsection{Deployment}

\begin{itemize}
    \item[$\square$] Docker images for all services
    \item[$\square$] Docker Compose for local dev
    \item[$\square$] Kubernetes manifests
    \item[$\square$] Helm charts
    \item[$\square$] Staging environment
    \item[$\square$] Production environment
\end{itemize}

% =============================================================================
% CONCLUSION
% =============================================================================
\section{Conclusion}
\label{sec:conclusion}

This Rust-first implementation strategy for JANUS provides:

\begin{enumerate}
    \item \textbf{Maximum Performance}: Rust's zero-cost abstractions and no GIL enable <100ms latency for Forward service
    \item \textbf{Type Safety}: Compile-time guarantees prevent entire classes of bugs
    \item \textbf{Memory Safety}: No null pointer dereferences, no data races
    \item \textbf{Developer Experience}: Python gateway maintains ease of use for API consumers
    \item \textbf{Ecosystem Compatibility}: Hybrid approach leverages best tools from both ecosystems
\end{enumerate}

\textbf{Next Steps:}
\begin{itemize}
    \item Begin with Phase 1 foundation (weeks 1-4)
    \item Train initial models in PyTorch and export to ONNX
    \item Implement Forward service with ONNX Runtime
    \item Gradually migrate components to pure Rust (Candle/Burn)
    \item Monitor performance and iterate
\end{itemize}

The architecture is designed for \textbf{incremental migration}, allowing you to start with ONNX inference in Rust while keeping training in PyTorch, then gradually move to a fully Rust-native stack as the ecosystem matures.

% =============================================================================
% BIBLIOGRAPHY
% =============================================================================
\newpage
\begin{thebibliography}{99}
\raggedright

\bibitem{janus_forward} Jordan Smith, "JANUS Forward: Wake State Logic Trading Algorithm," 2025.

\bibitem{janus_backward} Jordan Smith, "JANUS Backward: Sleep State Memory Management," 2025.

\bibitem{tch_rs} Laurent Mazare, "tch-rs: Rust bindings for PyTorch," GitHub, 2024.

\bibitem{onnx_runtime} Microsoft, "ONNX Runtime: Cross-platform ML inference," 2024.

\bibitem{candle} Hugging Face, "Candle: Minimalist ML framework in Rust," 2024.

\bibitem{burn} Burn Contributors, "Burn: Deep learning framework in Rust," 2024.

\bibitem{tonic} Tokio Contributors, "Tonic: A native gRPC client \& server implementation," 2024.

\bibitem{fastapi} Sebastián Ramírez, "FastAPI: Modern, fast web framework for Python," 2024.

\bibitem{rayon} Rayon Contributors, "Rayon: Data parallelism library for Rust," 2024.

\bibitem{ndarray} ndarray Contributors, "ndarray: N-dimensional arrays for Rust," 2024.

\bibitem{qdrant} Qdrant Team, "Qdrant: Vector database for ML applications," 2024.

\end{thebibliography}

\end{document}
