\documentclass[12pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{pmboxdraw}
\usepackage{newunicodechar}
\usepackage[english]{babel}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{setspace}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

% --- Define Left-Aligned X Column for Tables ---
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

% --- URL BREAKING ---
\usepackage{xurl}
\usepackage{hyperref}

% --- CONFIGURATION ---
\onehalfspacing
\setlength{\headheight}{15pt}

\definecolor{janusblue}{RGB}{0, 51, 102}
\definecolor{accentgold}{RGB}{204, 153, 51}
\definecolor{codegray}{RGB}{245, 245, 245}
\definecolor{backwardblue}{RGB}{30, 60, 114}

% --- UNICODE CHARACTER DECLARATIONS ---
\newunicodechar{▼}{\ensuremath{\blacktriangledown}}
\newunicodechar{→}{\ensuremath{\rightarrow}}
\newunicodechar{←}{\ensuremath{\leftarrow}}
\newunicodechar{↔}{\ensuremath{\leftrightarrow}}
\newunicodechar{⇒}{\ensuremath{\Rightarrow}}
\newunicodechar{…}{\ldots}
\newunicodechar{≥}{\ensuremath{\geq}}
\newunicodechar{≤}{\ensuremath{\leq}}
\newunicodechar{≠}{\ensuremath{\neq}}
\newunicodechar{≈}{\ensuremath{\approx}}
\newunicodechar{∈}{\ensuremath{\in}}
\newunicodechar{∉}{\ensuremath{\notin}}
\newunicodechar{∧}{\ensuremath{\wedge}}
\newunicodechar{∨}{\ensuremath{\vee}}
\newunicodechar{¬}{\ensuremath{\neg}}
\newunicodechar{×}{\ensuremath{\times}}
\newunicodechar{÷}{\ensuremath{\div}}
\newunicodechar{∞}{\ensuremath{\infty}}
\newunicodechar{∑}{\ensuremath{\sum}}
\newunicodechar{∏}{\ensuremath{\prod}}
\newunicodechar{∫}{\ensuremath{\int}}
\newunicodechar{√}{\ensuremath{\sqrt}}
\newunicodechar{∂}{\ensuremath{\partial}}
\newunicodechar{∇}{\ensuremath{\nabla}}
\newunicodechar{α}{\ensuremath{\alpha}}
\newunicodechar{β}{\ensuremath{\beta}}
\newunicodechar{γ}{\ensuremath{\gamma}}
\newunicodechar{δ}{\ensuremath{\delta}}
\newunicodechar{ε}{\ensuremath{\epsilon}}
\newunicodechar{θ}{\ensuremath{\theta}}
\newunicodechar{λ}{\ensuremath{\lambda}}
\newunicodechar{μ}{\ensuremath{\mu}}
\newunicodechar{π}{\ensuremath{\pi}}
\newunicodechar{σ}{\ensuremath{\sigma}}
\newunicodechar{τ}{\ensuremath{\tau}}
\newunicodechar{φ}{\ensuremath{\phi}}
\newunicodechar{ω}{\ensuremath{\omega}}
\newunicodechar{Δ}{\ensuremath{\Delta}}
\newunicodechar{Σ}{\ensuremath{\Sigma}}
\newunicodechar{Π}{\ensuremath{\Pi}}
\newunicodechar{Ω}{\ensuremath{\Omega}}

\hypersetup{
    colorlinks=true,
    linkcolor=janusblue,
    citecolor=janusblue,
    urlcolor=accentgold,
    pdftitle={JANUS Backward: Sleep State Memory Management},
    pdfauthor={Jordan Smith}
}

% --- HEADER & FOOTER ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textbf{JANUS Backward v1.0}}
\fancyhead[R]{\textit{Sleep State}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% --- SECTION STYLING ---
\titleformat{\section}
  {\color{backwardblue}\normalfont\Large\bfseries}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\color{backwardblue}\normalfont\large\bfseries}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\color{backwardblue}\normalfont\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% --- CODE SNIPPET STYLE ---
\lstdefinelanguage{Rust}{
    morekeywords={fn, let, mut, pub, struct, enum, impl, trait, use, mod, crate, async, await, match, if, else, while, for, loop, return, break, continue, const, static, type, where, self, Self, super, unsafe, extern, dyn, Box, Vec, Option, Result, Some, None, Ok, Err},
    sensitive=true,
    morecomment=[l]{//},
    morecomment=[s]{/*}{*/},
    morestring=[b]",
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    extendedchars=true,
    literate={▼}{{\$\blacktriangledown\$}}1
             {─}{{-}}1
             {│}{{|}}1
             {├}{{+}}1
             {└}{{`}}1
             {→}{{->}}1
             {…}{{...}}1
}

% --- MATH OPERATORS ---
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\sigmoid}{sigmoid}

% --- DOCUMENT START ---
\begin{document}

% =============================================================================
% TITLE PAGE
% =============================================================================
\begin{titlepage}
    \pagenumbering{gobble}
    \centering
    \vspace*{3cm}

    {\Huge \textbf{\textcolor{backwardblue}{JANUS BACKWARD}}} \\[0.5cm]
    {\LARGE \textbf{Sleep State: Memory Management}} \\[1.5cm]

    {\Large \textit{Knowledge Consolidation, Schema Formation, and Long-Term Learning}} \\[3cm]

    \textbf{\Large Classification: Technical Implementation Guide} \\[0.5cm]
    \textbf{\Large Version: 1.0 (Implementation-Ready)} \\[3cm]

    \textbf{Author:} Jordan Smith \\
    \textit{github.com/nuniesmith} \\[0.5cm]
    \textbf{Date:} \today

    \vfill
    \begin{tcolorbox}[colback=codegray, colframe=backwardblue, width=0.8\textwidth]
    \centering
    \textbf{JANUS Backward Overview:}
    \begin{itemize}[leftmargin=*]
        \item \textbf{Purpose:} Offline memory consolidation and schema learning
        \item \textbf{Cold Path:} Batch processing during market closure
        \item \textbf{Components:} Three-timescale memory, prioritized replay, UMAP visualization
        \item \textbf{Goal:} Transform raw experiences into abstract knowledge structures
    \end{itemize}
    \end{tcolorbox}
    \vfill
\end{titlepage}

% =============================================================================
% ABSTRACT
% =============================================================================
\newpage
\pagenumbering{arabic}
\thispagestyle{plain}
\section*{Abstract}

JANUS Backward represents the "sleep state" of the JANUS trading system, responsible for offline memory consolidation, schema formation, and long-term learning during market closure. This document provides a comprehensive mathematical and implementation specification for the Backward service, which implements:

\begin{itemize}
    \item \textbf{Three-Timescale Memory Architecture} spanning short-term (hippocampus), medium-term (SWR replay), and long-term (neocortex) storage
    \item \textbf{Prioritized Experience Replay} using TD-error, logical violation scores, and reward magnitude
    \item \textbf{Sharp Wave Ripple Simulation} for time-compressed memory consolidation
    \item \textbf{Recall-Gated Learning} that filters updates based on familiarity and logical validity
    \item \textbf{UMAP Visualization} for real-time cognitive monitoring and anomaly detection
\end{itemize}

The Backward service operates on a cold path with no strict latency requirements, enabling sophisticated batch processing and offline optimization that would be infeasible during live trading.

\newpage
% =============================================================================
% TABLE OF CONTENTS
% =============================================================================
\tableofcontents
\newpage

% =============================================================================
% SECTION 1: MEMORY HIERARCHY
% =============================================================================
\section{Memory Hierarchy: Three-Timescale Architecture}
\label{sec:memory}

The memory system is organized into three distinct timescales, each with specialized functions and computational properties.

\subsection{Short-Term Memory (Hippocampus)}

The hippocampal subsystem provides rapid encoding of recent experiences with pattern separation to prevent interference.

\subsubsection{Episodic Buffer}
The hippocampus maintains an episodic buffer of recent transitions:
\begin{equation}
    \mathcal{B}_{\text{STM}} = \{(\mathbf{s}_t, a_t, r_t, \mathbf{s}_{t+1})\}_{t=1}^{T_{\text{episode}}}
\end{equation}
where each tuple represents a state-action-reward-nextstate transition.

\textbf{Implementation Details:}
\begin{itemize}
    \item Maximum capacity: $|\mathcal{B}_{\text{STM}}| \leq 10,000$ transitions
    \item FIFO replacement policy when capacity exceeded
    \item Indexed by timestamp for temporal queries
\end{itemize}

\subsubsection{Pattern Separation}
To prevent catastrophic interference between similar market states, the hippocampus implements pattern separation:
\begin{equation}
    \mathbf{h}_{\text{separated}} = \text{ReLU}(\mathbf{W}_{\text{sep}} \mathbf{s} + \mathbf{b}_{\text{sep}})
\end{equation}
where $\mathbf{W}_{\text{sep}} \in \mathbb{R}^{d_h \times d_s}$ is initialized to promote orthogonality.

\textbf{Orthogonality Initialization:}
\begin{equation}
    \mathbf{W}_{\text{sep}} \sim \mathcal{N}(0, \sigma^2), \quad \text{where } \sigma = \sqrt{\frac{2}{d_s + d_h}}
\end{equation}

During training, add orthogonality regularization:
\begin{equation}
    \mathcal{L}_{\text{ortho}} = \lambda_{\text{ortho}} \cdot ||\mathbf{W}_{\text{sep}}^\top \mathbf{W}_{\text{sep}} - \mathbf{I}||_F^2
\end{equation}

\subsubsection{Sparse Encoding}
The hippocampus uses sparse representations to maximize information capacity:
\begin{equation}
    \mathbf{c}_{\text{sparse}} = \text{TopK}(\mathbf{h}_{\text{separated}}, k)
\end{equation}
where TopK selects the $k$ largest activations and zeros others.

\textbf{Sparsity Level:}
\begin{equation}
    k = \lceil \rho \cdot d_h \rceil, \quad \rho \in [0.05, 0.15]
\end{equation}
Typically $\rho = 0.1$ (10\% activation).

\subsection{Medium-Term Consolidation (SWR Simulator)}

The Sharp Wave Ripple (SWR) simulator implements prioritized replay with time compression, mimicking biological memory consolidation during sleep.

\subsubsection{Replay Prioritization}
Each transition is assigned a priority score combining three components:
\begin{equation}
    p_i = |\delta_i| + \lambda_{\text{logic}} \cdot v_i + \lambda_{\text{reward}} \cdot |r_i|
\end{equation}
where:
\begin{itemize}
    \item $\delta_i$ = TD-error: $r_i + \gamma Q(\mathbf{s}_{i+1}, a_{i+1}) - Q(\mathbf{s}_i, a_i)$
    \item $v_i$ = logical violation score from LTN (higher = more constraint violations)
    \item $r_i$ = reward magnitude (prioritize high-reward experiences)
    \item $\lambda_{\text{logic}} = 2.0$ (weight for constraint violations)
    \item $\lambda_{\text{reward}} = 0.5$ (weight for reward magnitude)
\end{itemize}

\textbf{Rationale:}
\begin{itemize}
    \item High TD-error → surprising transitions that require learning
    \item High violation score → dangerous patterns to avoid
    \item High reward → successful strategies to reinforce
\end{itemize}

\subsubsection{Sampling Probability}
Transitions are sampled stochastically with probability proportional to priority:
\begin{equation}
    P(i) = \frac{p_i^\alpha}{\sum_{j=1}^{|\mathcal{B}_{\text{STM}}|} p_j^\alpha}
\end{equation}
where $\alpha \in [0, 1]$ controls prioritization strength:
\begin{itemize}
    \item $\alpha = 0$ → uniform sampling
    \item $\alpha = 1$ → greedy prioritization
    \item $\alpha = 0.6$ → recommended default (balanced)
\end{itemize}

\subsubsection{Importance Sampling Correction}
To correct for sampling bias, apply importance-sampling weights:
\begin{equation}
    w_i = \left(\frac{1}{|\mathcal{B}_{\text{STM}}|} \cdot \frac{1}{P(i)}\right)^\beta
\end{equation}
where $\beta \in [0, 1]$ is annealed from 0.4 to 1.0 during training.

Normalized weights:
\begin{equation}
    \bar{w}_i = \frac{w_i}{\max_j w_j}
\end{equation}

Gradients are scaled by importance weights:
\begin{equation}
    \nabla_\theta \mathcal{L}(\tau_i) \gets \bar{w}_i \cdot \nabla_\theta \mathcal{L}(\tau_i)
\end{equation}

\subsubsection{Time Compression}
During replay, transitions are replayed at $C \times$ speed to accelerate consolidation:
\begin{equation}
    \Delta t_{\text{replay}} = \frac{\Delta t_{\text{original}}}{C}
\end{equation}
where $C \in [10, 20]$ is the compression factor (typically $C = 15$).

\textbf{Biological Motivation:} Real hippocampal replay occurs at 10-20× speed during sleep.

\subsubsection{SWR Replay Algorithm}
\begin{algorithm}[H]
\caption{Sharp Wave Ripple Replay}
\begin{algorithmic}[1]
\Require Buffer $\mathcal{B}_{\text{STM}}$, compression factor $C$, batch size $B$, prioritization exponent $\alpha$
\Ensure Replay batch $\mathcal{B}_{\text{replay}}$, importance weights $\mathbf{w}$
\State Compute TD-errors $\delta_i$ for all transitions
\State Compute logical violations $v_i$ via LTN evaluation
\State Compute priorities $p_i = |\delta_i| + \lambda_{\text{logic}} \cdot v_i + \lambda_{\text{reward}} \cdot |r_i|$
\State Compute sampling probabilities $P(i) = p_i^\alpha / \sum_j p_j^\alpha$
\State Sample $B$ transition indices with probabilities $P(i)$
\State Compute importance weights $w_i = (1/(|\mathcal{B}_{\text{STM}}| \cdot P(i)))^\beta$
\State Normalize weights $\bar{w}_i = w_i / \max_j w_j$
\For{each sampled transition $\tau_i = (\mathbf{s}_t, a_t, r_t, \mathbf{s}_{t+1})$}
    \State Compress time: $\Delta t \gets \Delta t / C$
    \State Add $(\tau_i, \bar{w}_i)$ to $\mathcal{B}_{\text{replay}}$
\EndFor
\State \Return $\mathcal{B}_{\text{replay}}, \mathbf{w}$
\end{algorithmic}
\end{algorithm}

\subsection{Long-Term Memory (Neocortex)}

The neocortical subsystem maintains abstract schemas—statistical summaries of recurring market patterns.

\subsubsection{Schema Representation}
Each schema $k$ is represented as a Gaussian distribution:
\begin{equation}
    \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{equation}
where:
\begin{equation}
    \boldsymbol{\mu}_k = \frac{1}{|\mathcal{S}_k|} \sum_{\mathbf{s} \in \mathcal{S}_k} \mathbf{s}
\end{equation}
\begin{equation}
    \boldsymbol{\Sigma}_k = \frac{1}{|\mathcal{S}_k|} \sum_{\mathbf{s} \in \mathcal{S}_k} (\mathbf{s} - \boldsymbol{\mu}_k)(\mathbf{s} - \boldsymbol{\mu}_k)^\top
\end{equation}

$\mathcal{S}_k$ is the set of all states assigned to schema $k$.

\subsubsection{Schema Assignment}
New states are assigned to schemas via maximum likelihood:
\begin{equation}
    k^* = \argmax_k \mathcal{N}(\mathbf{s}; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{equation}

If $\max_k \mathcal{N}(\mathbf{s}; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) < \tau_{\text{schema}}$, create a new schema.

\subsubsection{Recall-Gated Consolidation}
Updates to long-term memory are gated by two factors: recall strength (familiarity) and logical validity.

\textbf{Recall Strength:}
\begin{equation}
    g(r_{\text{STM}}(\tau)) = \sigmoid(\mathbf{W}_r \mathbf{r}_{\text{STM}} + b_r)
\end{equation}
where $\mathbf{r}_{\text{STM}}$ is the hippocampal representation of transition $\tau$.

\textbf{Logical Validity:}
\begin{equation}
    g_{\text{sym}}(\tau) = \text{SatAgg}(\mathcal{K}_{\text{episode}})
\end{equation}
where $\mathcal{K}_{\text{episode}}$ is the knowledge base evaluated on the episode containing $\tau$.

\textbf{Gated Update Rule:}
\begin{equation}
    \Delta \mathbf{W}_{\text{LTM}} = \eta_{\text{sleep}} \cdot g(r_{\text{STM}}(\tau)) \cdot g_{\text{sym}}(\tau) \cdot \nabla_{\mathbf{W}} \mathcal{L}_{\text{policy}}(\tau)
\end{equation}

Only update if both gates exceed thresholds:
\begin{equation}
    \text{Update if: } g(r_{\text{STM}}) > \tau_{\text{recall}} \text{ AND } g_{\text{sym}} > \tau_{\text{logic}}
\end{equation}
Typical thresholds: $\tau_{\text{recall}} = 0.3$, $\tau_{\text{logic}} = 0.7$.

\subsubsection{Consolidation Update Rule}
For schema $k$, update mean and covariance:
\begin{equation}
    \boldsymbol{\mu}_k^{(t+1)} = (1 - \eta_{\text{schema}}) \boldsymbol{\mu}_k^{(t)} + \eta_{\text{schema}} \cdot \mathbf{s}_{\text{new}}
\end{equation}
\begin{equation}
    \boldsymbol{\Sigma}_k^{(t+1)} = (1 - \eta_{\text{schema}}) \boldsymbol{\Sigma}_k^{(t)} + \eta_{\text{schema}} \cdot (\mathbf{s}_{\text{new}} - \boldsymbol{\mu}_k^{(t)})(\mathbf{s}_{\text{new}} - \boldsymbol{\mu}_k^{(t)})^\top
\end{equation}
where $\eta_{\text{schema}}$ is the schema learning rate (typically 0.01).

% =============================================================================
% SECTION 2: UMAP VISUALIZATION
% =============================================================================
\section{UMAP Visualization: Cognitive Dashboard}
\label{sec:umap}

UMAP (Uniform Manifold Approximation and Projection) provides a real-time 3D visualization of the system's internal knowledge structure.

\subsection{AlignedUMAP for Schema Formation}

AlignedUMAP ensures consistency across multiple sleep cycles, enabling tracking of schema evolution over time.

\subsubsection{Objective Function}
AlignedUMAP minimizes:
\begin{equation}
    \mathcal{L}_{\text{AlignedUMAP}} = \sum_{t=1}^{T_{\text{cycles}}} \left[\mathcal{L}_{\text{UMAP}}(\mathbf{X}_t) + \lambda_{\text{align}} \sum_{i,j} w_{ij} ||\mathbf{y}_i^{(t)} - \mathbf{y}_j^{(t-1)}||^2\right]
\end{equation}
where:
\begin{itemize}
    \item $\mathbf{X}_t \in \mathbb{R}^{N_t \times d}$ = high-dimensional embeddings at sleep cycle $t$
    \item $\mathbf{y}_i^{(t)} \in \mathbb{R}^3$ = 3D projection of point $i$ at cycle $t$
    \item $w_{ij}$ = alignment weights (higher for points in same schema)
    \item $\lambda_{\text{align}} = 0.1$ = alignment strength
\end{itemize}

\textbf{Standard UMAP Loss:}
\begin{equation}
    \mathcal{L}_{\text{UMAP}}(\mathbf{X}) = \sum_{i,j} \left[v_{ij} \log \frac{v_{ij}}{w_{ij}} + (1 - v_{ij}) \log \frac{1 - v_{ij}}{1 - w_{ij}}\right]
\end{equation}
where $v_{ij}$ is high-dimensional similarity and $w_{ij}$ is low-dimensional similarity.

\subsubsection{Alignment Weights}
\begin{equation}
    w_{ij} = \begin{cases}
        1.0 & \text{if schema}(i) = \text{schema}(j) \\
        0.1 & \text{otherwise}
    \end{cases}
\end{equation}

\subsubsection{Schema Cluster Detection}
Schemas are identified as dense clusters in UMAP space using DBSCAN:
\begin{equation}
    \text{Schema}_k = \{\mathbf{y}_i : ||\mathbf{y}_i - \boldsymbol{\mu}_k|| < \tau_{\text{cluster}}\}
\end{equation}
where $\tau_{\text{cluster}}$ is the cluster radius (typically 0.5 in normalized UMAP space).

\subsection{Parametric UMAP for Real-Time Monitoring}

Parametric UMAP learns a neural network mapping for fast projection of new points during live trading.

\subsubsection{Neural Network Projection}
A feedforward network $f_{\text{UMAP}}: \mathbb{R}^d \rightarrow \mathbb{R}^3$ is trained to approximate UMAP projection:
\begin{equation}
    \mathbf{y} = f_{\text{UMAP}}(\mathbf{e}; \theta_{\text{UMAP}})
\end{equation}

\textbf{Architecture:}
\begin{align}
    \mathbf{h}_1 &= \text{ReLU}(\mathbf{W}_1 \mathbf{e} + \mathbf{b}_1), \quad \mathbf{h}_1 \in \mathbb{R}^{256} \\
    \mathbf{h}_2 &= \text{ReLU}(\mathbf{W}_2 \mathbf{h}_1 + \mathbf{b}_2), \quad \mathbf{h}_2 \in \mathbb{R}^{128} \\
    \mathbf{y} &= \mathbf{W}_3 \mathbf{h}_2 + \mathbf{b}_3, \quad \mathbf{y} \in \mathbb{R}^3
\end{align}

\textbf{Training Objective:}
\begin{equation}
    \mathcal{L}_{\text{ParametricUMAP}} = \sum_i ||\mathbf{y}_i - f_{\text{UMAP}}(\mathbf{e}_i)||^2 + \mathcal{L}_{\text{UMAP}}
\end{equation}

\subsubsection{Anomaly Detection}
During live trading, a point is flagged as anomalous if it falls outside all known schemas:
\begin{equation}
    \text{Anomaly}(\mathbf{y}) = \mathbb{1}\left[\min_k ||\mathbf{y} - \boldsymbol{\mu}_k|| > \tau_{\text{anomaly}}\right]
\end{equation}
where $\tau_{\text{anomaly}} = 2.0$ (units in UMAP space).

\textbf{Response to Anomalies:}
\begin{itemize}
    \item Log anomaly with full context
    \item Increase risk threshold temporarily
    \item Alert human operator if anomaly persists
    \item Add to high-priority replay buffer
\end{itemize}

% =============================================================================
% SECTION 3: INTEGRATION WITH VECTOR DATABASE
% =============================================================================
\section{Integration with Vector Database (Qdrant)}
\label{sec:vectordb}

Long-term memory schemas are persisted in Qdrant for efficient similarity search and retrieval.

\subsection{Schema Storage}

Each schema is stored as a point in Qdrant:
\begin{itemize}
    \item \textbf{Vector:} $\boldsymbol{\mu}_k \in \mathbb{R}^d$ (schema centroid)
    \item \textbf{Payload:}
    \begin{itemize}
        \item \texttt{schema\_id}: Unique identifier
        \item \texttt{covariance}: Flattened $\boldsymbol{\Sigma}_k$
        \item \texttt{num\_points}: $|\mathcal{S}_k|$
        \item \texttt{avg\_reward}: Mean reward for transitions in schema
        \item \texttt{created\_at}: Timestamp
        \item \texttt{last\_updated}: Timestamp
    \end{itemize}
\end{itemize}

\subsection{Similarity Search}

Given a new state $\mathbf{s}_{\text{new}}$, retrieve top-$k$ similar schemas:
\begin{equation}
    \text{TopK}(\mathbf{s}_{\text{new}}) = \argmax_{k, |K| = k} \left\{\text{cosine}(\mathbf{s}_{\text{new}}, \boldsymbol{\mu}_i)\right\}_{i=1}^{N_{\text{schemas}}}
\end{equation}

\textbf{Use Cases:}
\begin{itemize}
    \item Retrieve historical context during decision-making
    \item Find similar market conditions for transfer learning
    \item Identify schema membership for new states
\end{itemize}

\subsection{Periodic Schema Pruning}

Remove low-quality schemas to prevent memory bloat:
\begin{equation}
    \text{Prune if: } |\mathcal{S}_k| < \tau_{\text{min\_points}} \text{ OR } \text{age}(k) > \tau_{\text{max\_age}}
\end{equation}
where $\tau_{\text{min\_points}} = 10$ and $\tau_{\text{max\_age}} = 90$ days.

% =============================================================================
% SECTION 4: SLEEP CYCLE ALGORITHM
% =============================================================================
\section{Sleep Cycle: Complete Algorithm}
\label{sec:sleepcycle}

The sleep cycle runs nightly (or after market close) to consolidate the day's experiences.

\begin{algorithm}[H]
\caption{JANUS Backward Sleep Cycle}
\begin{algorithmic}[1]
\Require Short-term buffer $\mathcal{B}_{\text{STM}}$, long-term schemas $\{\mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\}_k$
\Ensure Updated schemas, trained policy
\State \textbf{Phase 1: Prioritized Replay (SWR Simulation)}
\For{$n_{\text{replays}}$ iterations (e.g., 1000)}
    \State Sample batch $\mathcal{B}_{\text{replay}}$ using SWR algorithm
    \State Compute losses: $\mathcal{L}_{\text{policy}}, \mathcal{L}_{\text{logic}}$
    \State Update policy: $\theta \gets \theta - \eta \cdot \bar{w}_i \cdot \nabla_\theta \mathcal{L}_{\text{total}}$
    \State Update priorities: $p_i \gets |\delta_i| + \lambda_{\text{logic}} v_i + \lambda_{\text{reward}} |r_i|$
\EndFor
\State
\State \textbf{Phase 2: Schema Consolidation}
\For{each transition $\tau_i \in \mathcal{B}_{\text{STM}}$}
    \State Compute recall gate: $g_{\text{recall}} = \sigmoid(\mathbf{W}_r \mathbf{r}_{\text{STM}} + b_r)$
    \State Compute logic gate: $g_{\text{logic}} = \text{SatAgg}(\mathcal{K})$
    \If{$g_{\text{recall}} > \tau_{\text{recall}}$ AND $g_{\text{logic}} > \tau_{\text{logic}}$}
        \State Find matching schema: $k^* = \argmax_k \mathcal{N}(\mathbf{s}_i; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$
        \If{$\mathcal{N}(\mathbf{s}_i; \boldsymbol{\mu}_{k^*}, \boldsymbol{\Sigma}_{k^*}) < \tau_{\text{schema}}$}
            \State Create new schema: $\boldsymbol{\mu}_{\text{new}} \gets \mathbf{s}_i$, $\boldsymbol{\Sigma}_{\text{new}} \gets \epsilon \mathbf{I}$
        \Else
            \State Update schema $k^*$ using consolidation rule
        \EndIf
    \EndIf
\EndFor
\State
\State \textbf{Phase 3: UMAP Update}
\State Extract all schema centroids: $\{\boldsymbol{\mu}_k\}_k$
\State Fit AlignedUMAP with previous cycle alignment
\State Update parametric UMAP network
\State Detect new clusters via DBSCAN
\State
\State \textbf{Phase 4: Vector Database Sync}
\State Upsert updated schemas to Qdrant
\State Prune low-quality schemas
\State Create snapshot for recovery
\State
\State \textbf{Phase 5: Metrics \& Logging}
\State Compute and log:
\begin{itemize}
    \item Number of schemas: $N_{\text{schemas}}$
    \item Mean schema size: $\mathbb{E}[|\mathcal{S}_k|]$
    \item Constraint satisfaction rate: $\mathbb{E}[\text{SatAgg}(\mathcal{K})]$
    \item Average TD-error improvement
    \item UMAP cluster count
\end{itemize}
\end{algorithmic}
\end{algorithm}

% =============================================================================
% SECTION 5: IMPLEMENTATION CHECKLIST
% =============================================================================
\newpage
\section{Implementation Checklist}
{sec:checklist}

This section provides a sequential checklist for implementing JANUS Backward.

\subsection{Core Components}

\begin{enumerate}
    \item \textbf{Short-Term Memory (Hippocampus)}
    \begin{itemize}
        \item[$\square$] Implement episodic buffer with FIFO eviction
        \item[$\square$] Implement pattern separation layer
        \item[$\square$] Add orthogonality regularization
        \item[$\square$] Implement TopK sparse encoding
        \item[$\square$] Add timestamp indexing for temporal queries
        \item[$\square$] Test buffer operations (insert, retrieve, evict)
    \end{itemize}

    \item \textbf{Sharp Wave Ripple (SWR) Simulator}
    \begin{itemize}
        \item[$\square$] Implement TD-error computation
        \item[$\square$] Implement logical violation scoring via LTN
        \item[$\square$] Implement composite priority function
        \item[$\square$] Implement prioritized sampling with importance weights
        \item[$\square$] Add time compression simulation
        \item[$\square$] Test replay batch generation
        \item[$\square$] Validate importance weight correction
    \end{itemize}

    \item \textbf{Long-Term Memory (Neocortex)}
    \begin{itemize}
        \item[$\square$] Implement schema representation (Gaussian)
        \item[$\square$] Implement schema assignment via maximum likelihood
        \item[$\square$] Implement recall gate computation
        \item[$\square$] Implement logical validity gate
        \item[$\square$] Implement gated consolidation update
        \item[$\square$] Add schema creation logic
        \item[$\square$] Test schema updates with edge cases
    \end{itemize}

    \item \textbf{UMAP Visualization}
    \begin{itemize}
        \item[$\square$] Implement AlignedUMAP objective
        \item[$\square$] Add alignment weight computation
        \item[$\square$] Implement parametric UMAP network
        \item[$\square$] Implement DBSCAN cluster detection
        \item[$\square$] Add anomaly detection logic
        \item[$\square$] Test visualization updates across cycles
        \item[$\square$] Validate cluster stability
    \end{itemize}
\end{enumerate}

\subsection{Integration \& Storage}

\begin{enumerate}
    \item \textbf{Vector Database Integration (Qdrant)}
    \begin{itemize}
        \item[$\square$] Set up Qdrant connection
        \item[$\square$] Define schema collection structure
        \item[$\square$] Implement schema upsert operations
        \item[$\square$] Implement similarity search queries
        \item[$\square$] Add periodic pruning logic
        \item[$\square$] Implement backup/restore functionality
        \item[$\square$] Test concurrent access patterns
    \end{itemize}

    \item \textbf{Sleep Cycle Orchestration}
    \begin{itemize}
        \item[$\square$] Implement 5-phase sleep cycle algorithm
        \item[$\square$] Add progress tracking and logging
        \item[$\square$] Implement graceful shutdown on errors
        \item[$\square$] Add checkpoint/resume capability
        \item[$\square$] Test full sleep cycle end-to-end
        \item[$\square$] Validate schema evolution over cycles
    \end{itemize}
\end{enumerate}

\subsection{Monitoring \& Debugging}

\begin{enumerate}
    \item \textbf{Metrics Collection}
    \begin{itemize}
        \item[$\square$] Track number of schemas over time
        \item[$\square$] Monitor mean schema size
        \item[$\square$] Track constraint satisfaction rates
        \item[$\square$] Monitor TD-error distribution
        \item[$\square$] Track UMAP cluster count
        \item[$\square$] Log replay batch statistics
    \end{itemize}

    \item \textbf{Visualization \& Debugging}
    \begin{itemize}
        \item[$\square$] Export UMAP projections for visualization
        \item[$\square$] Add schema evolution timeline
        \item[$\square$] Visualize priority distributions
        \item[$\square$] Plot constraint satisfaction heatmaps
        \item[$\square$] Add interactive schema browser
    \end{itemize}
\end{enumerate}

\subsection{Performance Optimization}

\begin{enumerate}
    \item \textbf{Batch Processing}
    \begin{itemize}
        \item[$\square$] Parallelize TD-error computation
        \item[$\square$] Vectorize schema likelihood calculations
        \item[$\square$] Batch Qdrant upsert operations
        \item[$\square$] Use GPU for UMAP fitting (if available)
        \item[$\square$] Profile and optimize bottlenecks
        \item[$\square$] Target: <10 minutes for 10k transitions
    \end{itemize}
\end{enumerate}

% =============================================================================
% SECTION 6: RUST IMPLEMENTATION NOTES
% =============================================================================
\section{Rust Implementation Considerations}
{sec:rust}

\subsection{Cold Path Optimization}

Unlike Forward, Backward has no strict latency requirements, allowing focus on throughput and correctness.

\begin{itemize}
    \item \textbf{Batch parallelism:} Use \texttt{rayon} for parallel replay processing
    \item \textbf{Memory efficiency:} Use \texttt{ndarray} for linear algebra operations
    \item \textbf{Async I/O:} Use \texttt{tokio} for non-blocking Qdrant operations
    \item \textbf{Checkpointing:} Serialize intermediate state with \texttt{serde}
\end{itemize}

\subsection{Data Structures}

\subsubsection{Episodic Buffer}
\begin{lstlisting}[language=Rust]
use std::collections::VecDeque;

#[derive(Clone, Debug)]
pub struct Transition {
    pub state: Array1<f32>,
    pub action: Action,
    pub reward: f32,
    pub next_state: Array1<f32>,
    pub timestamp: u64,
}

pub struct EpisodicBuffer {
    buffer: VecDeque<Transition>,
    capacity: usize,
}

impl EpisodicBuffer {
    pub fn new(capacity: usize) -> Self {
        Self {
            buffer: VecDeque::with_capacity(capacity),
            capacity,
        }
    }

    pub fn push(&mut self, transition: Transition) {
        if self.buffer.len() >= self.capacity {
            self.buffer.pop_front();
        }
        self.buffer.push_back(transition);
    }

    pub fn sample_prioritized(
        &self,
        priorities: &[f32],
        batch_size: usize,
        alpha: f32,
    ) -> (Vec<Transition>, Vec<f32>) {
        // Prioritized sampling implementation
        todo!()
    }
}
\end{lstlisting}

\subsubsection{Schema Representation}
\begin{lstlisting}[language=Rust]
use ndarray::{Array1, Array2};

#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct Schema {
    pub id: uuid::Uuid,
    pub mean: Array1<f32>,
    pub covariance: Array2<f32>,
    pub num_points: usize,
    pub avg_reward: f32,
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub last_updated: chrono::DateTime<chrono::Utc>,
}

impl Schema {
    pub fn likelihood(&self, state: &Array1<f32>) -> f32 {
        // Compute Gaussian likelihood
        let diff = state - &self.mean;
        let inv_cov = self.covariance.inv().unwrap();
        let exponent = -0.5 * diff.dot(&inv_cov.dot(&diff));
        exponent.exp()
    }

    pub fn update(
        &mut self,
        new_state: &Array1<f32>,
        learning_rate: f32,
    ) {
        let diff = new_state - &self.mean;
        self.mean = &self.mean + learning_rate * &diff;
        // Update covariance (outer product)
        let outer = diff.clone().insert_axis(Axis(1))
            .dot(&diff.clone().insert_axis(Axis(0)));
        self.covariance = (1.0 - learning_rate) * &self.covariance
            + learning_rate * outer;
        self.num_points += 1;
        self.last_updated = chrono::Utc::now();
    }
}
\end{lstlisting}

\subsection{Error Handling}

\begin{lstlisting}[language=Rust]
#[derive(Debug, thiserror::Error)]
pub enum BackwardError {
    #[error("Insufficient data for replay: {0} transitions")]
    InsufficientData(usize),

    #[error("Schema update failed: {0}")]
    SchemaUpdateError(String),

    #[error("UMAP fitting failed: {0}")]
    UmapError(String),

    #[error("Qdrant operation failed: {0}")]
    VectorDbError(#[from] qdrant_client::QdrantError),

    #[error("Linear algebra error: {0}")]
    LinalgError(String),
}

pub type BackwardResult<T> = Result<T, BackwardError>;
\end{lstlisting}

% =============================================================================
% BIBLIOGRAPHY
% =============================================================================
\newpage
\begin{thebibliography}{99}
\raggedright

\bibitem{janus_main} Jordan Smith, "Project JANUS: Implementation Guide v1.0," 2025.

\bibitem{per_paper} Schaul et al., "Prioritized Experience Replay," ICLR 2016.

\bibitem{swr_dynamics} "A Unified Dynamic Model for Learning, Replay, and Ripples," 2015/2025.

\bibitem{hippocampal_replay} Foster, Wilson, "Reverse Replay of Behavioural Sequences in Hippocampal Place Cells," Nature 2006.

\bibitem{schema_theory} Tse et al., "Schema-Dependent Gene Activation and Memory Encoding in Neocortex," Science 2011.

\bibitem{umap} McInnes et al., "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction," arXiv:1802.03426, 2018.

\bibitem{aligned_umap} Aynaud et al., "AlignedUMAP: Temporal Alignment for Multi-Dataset Visualization," bioRxiv, 2020.

\bibitem{qdrant} Qdrant Team, "Qdrant Vector Database Documentation," 2024.

\end{thebibliography}

\end{document}
