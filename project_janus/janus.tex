\documentclass[12pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{newunicodechar}
\usepackage{csquotes}
\usepackage[english]{babel}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{setspace}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

% --- BIBLIOGRAPHY ---
\usepackage[style=authoryear,backend=biber,natbib=true,sorting=nyt,maxbibnames=99]{biblatex}
\addbibresource{janus.bib}

% --- Define Left-Aligned X Column for Tables ---
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

% --- URL BREAKING ---
\usepackage{xurl}
\usepackage[hypertexnames=false]{hyperref}

% --- CONFIGURATION ---
\onehalfspacing
\setlength{\headheight}{18pt}
\emergencystretch=3em          % Allow extra inter-word stretch before creating overfull hboxes
\tolerance=2000                % Accept slightly looser line breaks to avoid overflow
\hbadness=10000                % Suppress cosmetically insignificant underfull hbox warnings
\hfuzz=2pt                     % Ignore overfull hboxes smaller than 2pt

\definecolor{janusblue}{RGB}{0, 51, 102}
\definecolor{accentgold}{RGB}{204, 153, 51}
\definecolor{warnred}{RGB}{153, 0, 0}
\definecolor{forwardblue}{RGB}{41, 98, 255}
\definecolor{backwardpurple}{RGB}{138, 43, 226}
\definecolor{neurogreen}{RGB}{34, 139, 34}
\definecolor{rustorange}{RGB}{255, 140, 0}
\definecolor{codegray}{RGB}{245, 245, 245}
\definecolor{rustcolor}{RGB}{70, 130, 180}

% --- UNICODE CHARACTER DECLARATIONS ---
% Some characters below are already defined by inputenc/babel/amssymb.
% Use a helper that silently redefines if already present.
\makeatletter
\newcommand{\safeunicodechar}[2]{%
  \@ifundefined{u8:\detokenize{#1}}%
    {\newunicodechar{#1}{#2}}%
    {\begingroup\let\PackageWarning\@gobbletwo\newunicodechar{#1}{#2}\endgroup}%
}
\makeatother
\safeunicodechar{▼}{\ensuremath{\blacktriangledown}}
\safeunicodechar{→}{\ensuremath{\rightarrow}}
\safeunicodechar{←}{\ensuremath{\leftarrow}}
\safeunicodechar{↔}{\ensuremath{\leftrightarrow}}
\safeunicodechar{⇒}{\ensuremath{\Rightarrow}}
\safeunicodechar{…}{\ldots}
\safeunicodechar{≥}{\ensuremath{\geq}}
\safeunicodechar{≤}{\ensuremath{\leq}}
\safeunicodechar{≠}{\ensuremath{\neq}}
\safeunicodechar{≈}{\ensuremath{\approx}}
\safeunicodechar{∈}{\ensuremath{\in}}
\safeunicodechar{∉}{\ensuremath{\notin}}
\safeunicodechar{∧}{\ensuremath{\wedge}}
\safeunicodechar{∨}{\ensuremath{\vee}}
\safeunicodechar{¬}{\ensuremath{\neg}}
\safeunicodechar{×}{\ensuremath{\times}}
\safeunicodechar{÷}{\ensuremath{\div}}
\safeunicodechar{∞}{\ensuremath{\infty}}
\safeunicodechar{∑}{\ensuremath{\sum}}
\safeunicodechar{∏}{\ensuremath{\prod}}
\safeunicodechar{∫}{\ensuremath{\int}}
\safeunicodechar{√}{\ensuremath{\sqrt}}
\safeunicodechar{∂}{\ensuremath{\partial}}
\safeunicodechar{∇}{\ensuremath{\nabla}}
\safeunicodechar{α}{\ensuremath{\alpha}}
\safeunicodechar{β}{\ensuremath{\beta}}
\safeunicodechar{γ}{\ensuremath{\gamma}}
\safeunicodechar{δ}{\ensuremath{\delta}}
\safeunicodechar{ε}{\ensuremath{\epsilon}}
\safeunicodechar{θ}{\ensuremath{\theta}}
\safeunicodechar{λ}{\ensuremath{\lambda}}
\safeunicodechar{μ}{\ensuremath{\mu}}
\safeunicodechar{π}{\ensuremath{\pi}}
\safeunicodechar{σ}{\ensuremath{\sigma}}
\safeunicodechar{τ}{\ensuremath{\tau}}
\safeunicodechar{φ}{\ensuremath{\phi}}
\safeunicodechar{ω}{\ensuremath{\omega}}
\safeunicodechar{Δ}{\ensuremath{\Delta}}
\safeunicodechar{Σ}{\ensuremath{\Sigma}}
\safeunicodechar{Π}{\ensuremath{\Pi}}
\safeunicodechar{Ω}{\ensuremath{\Omega}}

% --- MATH OPERATORS ---
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\sigmoid}{sigmoid}

% --- HYPERREF CONFIGURATION ---
% Prevent \@ internals from leaking into PDF bookmark strings
\makeatletter
\pdfstringdefDisableCommands{%
  \def\@{}%
  \def\spacefactor{}%
}
\makeatother
\hypersetup{
    colorlinks=true,
    linkcolor=janusblue,
    filecolor=accentgold,
    urlcolor=janusblue,
    citecolor=janusblue,
    pdftitle={Project JANUS: A Neuromorphic Architecture for Crowding-Resistant Trading},
    pdfauthor={Jordan Smith},
    pdfsubject={Neuromorphic Trading Intelligence},
    pdfkeywords={neuromorphic computing, algorithmic trading, machine learning, Rust, neuroscience},
    bookmarksopen=true,
    bookmarksnumbered=true
}

% --- HEADER/FOOTER ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textcolor{janusblue}{\textbf{Project JANUS}}}
\fancyhead[R]{\textcolor{accentgold}{Technical Specification \& Theoretical Foundations}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% --- SECTION FORMATTING ---
\titleformat{\section}
  {\Large\bfseries\color{janusblue}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{janusblue}}
  {\thesubsection}{1em}{}

% --- CODE LISTINGS STYLE ---
\lstdefinestyle{rust}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{rustcolor},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    morekeywords={fn, let, mut, pub, struct, impl, use, mod, trait, enum, match, if, else, return, self, Self, true, false, async, await}
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\begin{document}

% --- TITLE PAGE ---
\begin{titlepage}
	\centering
	\vspace*{3cm}

	{\Huge\bfseries\color{janusblue} Project JANUS\par}
	\vspace{0.5cm}
	{\Large\color{accentgold} A Neuromorphic Architecture for Autonomous Trading\par}
	\vspace{2cm}

	{\LARGE\bfseries Technical Specification \& Theoretical Foundations\par}
	\vspace{1cm}

	{\large\itshape Brain-Inspired Multi-Region Architecture Integrating\\Neuro-Symbolic Reasoning, Complementary Learning Systems,\\and Engineered Heterogeneity\/\par}
	\vspace{3cm}

	\begin{tcolorbox}[colback=janusblue!5, colframe=janusblue, title=\textbf{Unified Documentation}, width=0.85\textwidth]
		This document consolidates all theoretical foundations and technical specifications of Project JANUS\@:
		\begin{enumerate}[leftmargin=2cm]
			\item \textbf{\textcolor{janusblue}{Theoretical Foundations}} — Crowding resistance, neuroscience framework, and dual-process architecture
			      \vspace{0.2cm}
			\item \textbf{\textcolor{forwardblue}{Forward Service}} — Real-time decision-making and execution
			      \vspace{0.2cm}
			\item \textbf{\textcolor{backwardpurple}{Backward Service}} — Memory consolidation and learning
			      \vspace{0.2cm}
			\item \textbf{\textcolor{neurogreen}{Neuromorphic Architecture}} — Brain-region mapping
			      \vspace{0.2cm}
			\item \textbf{\textcolor{rustorange}{Rust Implementation}} — Production deployment guide
			      \vspace{0.2cm}
			\item \textbf{\textcolor{warnred}{Limitations \& Open Problems}} — Honest engagement with challenges
			      \vspace{0.2cm}
			\item \textbf{\textcolor{janusblue}{Validation Framework}} — Experimental design and benchmarks
		\end{enumerate}%
	\end{tcolorbox}

	\vfill

	{\large\bfseries Author\par}
	{\large Jordan Smith\par}
	\vspace{0.5cm}

	{\large\bfseries Date\par}
	{\large \today\par}
	\vspace{1cm}

	{\small\itshape ``The god of beginnings and transitions, looking simultaneously to the future and the past.''\/\par} % chktex 38

\end{titlepage}
\setcounter{page}{2}

% --- ABSTRACT ---
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

Project JANUS is a neuromorphic trading architecture that maps ten functionally distinct brain regions---basal ganglia, hippocampus, cerebellum, thalamus, hypothalamus, prefrontal cortex, amygdala, visual cortex, neocortex, and central nervous system---to specialized trading subsystems within a unified autonomous agent. Grounded in the Doya--Hassabis framework of neuroscience-inspired AI \citep{doya1999computations, hassabis2017neuroscience}, JANUS implements a dual-process architecture separating real-time perception and action (System~1, Forward Service) from offline memory consolidation and schema formation (System~2, Backward Service), following Complementary Learning Systems theory \citep{mcclelland1995complementary, kumaran2016cls}.

A central design objective is \textit{crowding resistance}: the mitigation of co-impact costs \citep{bucci2020coimpact} and strategy crowding risk \citep{khandani2011quants, stein2009crowding} through engineered heterogeneity across deployments. Each JANUS instance is parameterized to produce idiosyncratic order flow, reducing cross-instance correlation and increasing effective market capacity \citep{demiguel2021crowding, wagner2011diversity}. The system integrates neuro-symbolic reasoning via Logic Tensor Networks \citep{badreddine2022logic} for differentiable constraint satisfaction, video vision transformers for visual pattern recognition \citep{arnab2021vivit}, Opponent Actor Learning for basal ganglia decision-making \citep{collins2014opponent, jaskir2023opal}, and a three-timescale memory hierarchy inspired by hippocampal sharp-wave ripple replay \citep{wilson1994replay, masset2025multitimescale}.

No prior published system maps multiple brain regions to distinct trading subsystems. This paper presents the theoretical foundations, mathematical specifications, neuromorphic architecture, and Rust implementation of JANUS, along with a candid assessment of limitations and a proposed validation framework.

\vspace{0.5cm}
\noindent\textbf{Keywords:} neuromorphic computing, algorithmic trading, neuro-symbolic AI, strategy crowding, co-impact, complementary learning systems, reinforcement learning, Rust

% --- TABLE OF CONTENTS ---
\newpage
\tableofcontents
\newpage

% ============================================
% PART 1: MAIN ARCHITECTURE
% ============================================
\part{Main Architecture}
\label{part:main}
\setcounter{section}{0}
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\Roman{section}.\arabic{subsection}}

\section{The Epistemological Transition to Quant 4.0}
\label{sec:quant4}

The trajectory of algorithmic trading has historically been defined by a tension between interpretability and capability.
We are currently witnessing a phase transition from the ``black box'' empiricism of deep learning \citep{lecun2015deep} toward a new paradigm of Neuro-Symbolic integration \citep{marra2024neurosymbolic, garcez2023neurosymbolic}.
Project JANUS stands at the vanguard of this transition, termed \textbf{Quant 4.0}.
This architecture reimagines the financial agent as a biological entity---one that perceives, reasons, remembers, and fears.

The intellectual foundation rests on three convergent programs:
(i)~neuroscience-inspired AI \citep{hassabis2017neuroscience}, which argues that understanding the computational principles of the brain remains the richest source of algorithmic innovation;
(ii)~the Adaptive Markets Hypothesis \citep{lo2004adaptive, lo2017adaptive}, which replaces the Efficient Markets assumption with an evolutionary framework where market agents adapt, compete, and are selected; and
(iii)~neuro-symbolic reasoning \citep{garcez2023neurosymbolic, badreddine2022logic}, which bridges the gap between statistical pattern recognition and logical constraint satisfaction.

\subsection*{Historical Evolution of Quantitative Finance}

\begin{itemize}
	\item \textbf{Quant 1.0 (1980s--1990s):} Era of heuristics and expert systems with high interpretability but extreme rigidity
	\item \textbf{Quant 2.0 (1990s--2000s):} Statistical rigour through mean reversion, cointegration, and factor models \citep{fama1993common}
	\item \textbf{Quant 3.0 (2010s--Present):} Deep learning hegemony with LSTMs, Transformers \citep{vaswani2017attention}, and deep reinforcement learning \citep{sutton2018reinforcement}
	\item \textbf{Quant 4.0 (JANUS):} Neuro-Symbolic AI achieving adaptability of deep learning with reliability of rule-based systems, grounded in neuroscience
\end{itemize}

\section{The Doya--Hassabis Framework: Neuroscience-Inspired Architecture}
\label{sec:doya_hassabis}

JANUS's multi-region architecture applies functional brain-region decomposition to trading system design, grounded in the Doya--Hassabis framework of neuroscience-inspired AI \citep{doya1999computations, hassabis2017neuroscience}.

\subsection{Functional Brain-Region Decomposition}

\citet{doya1999computations} proposed the canonical functional decomposition of brain regions by learning type:

\begin{itemize}
	\item \textbf{Cerebellum} $\rightarrow$ supervised learning (forward models, error correction)
	\item \textbf{Basal ganglia} $\rightarrow$ reinforcement learning (reward-driven action selection)
	\item \textbf{Cerebral cortex} $\rightarrow$ unsupervised learning (schema formation, pattern discovery)
\end{itemize}

\citet{doya2002metalearning} extended this framework to show that neuromodulators (dopamine, serotonin, norepinephrine, acetylcholine) regulate meta-parameters of learning---learning rate, discount factor, exploration--exploitation balance---providing a biologically grounded mechanism for adaptive parameter control. \citet{caligiore2019superlearning} formalized the ``super-learning hypothesis,'' demonstrating that the integration of learning processes \textit{across} cortex, cerebellum, and basal ganglia produces capabilities exceeding any single region.

\subsection{From Neuroscience to Architecture}

\citet{hassabis2017neuroscience} articulated the programmatic statement for neuroscience-inspired AI: the goal is \textit{functional} inspiration, not biological simulation. JANUS adopts this position, with brain-region mappings in Part~\ref{part:neuro} justified by functional analogy---each region implements a computational principle (e.g., supervised error correction, reinforcement-driven action selection, episodic memory replay) that addresses a specific trading requirement.

This approach is further validated by \citet{yamakawa2021whole}, who proposed the ``whole brain architecture'' approach as a systematic method for developing artificial general intelligence by referencing brain structure, and by \citet{caligiore2017consensus}, who established consensus on the functional interactions between cerebellum, basal ganglia, and cortex.

\subsection{Complementary Learning Systems}

The separation of JANUS into Forward (System~1) and Backward (System~2) services directly implements Complementary Learning Systems (CLS) theory \citep{mcclelland1995complementary}. \citet{kumaran2016cls} updated CLS theory (``CLS 2.0''), establishing that intelligent agents require both a fast-learning system (hippocampus, for rapid encoding of specific experiences) and a slow-learning system (neocortex, for gradual extraction of statistical regularities). JANUS maps these to:

\begin{itemize}
	\item \textbf{Forward Service (hippocampal analogue):} Rapid encoding of each trading episode with episodic specificity
	\item \textbf{Backward Service (neocortical analogue):} Slow consolidation of episodic memories into generalized market schemas through sharp-wave ripple replay \citep{wilson1994replay, buzsaki2015hippocampal}
\end{itemize}

\citet{masset2025multitimescale} recently demonstrated that dopaminergic neurons encode reward prediction errors at diverse temporal discount factors, providing direct biological validation for JANUS's three-timescale memory architecture (short-term hippocampal buffer, medium-term SWR consolidation, long-term neocortical schemas).

\section{The Dual-Process Architecture}
\label{sec:dual_process}

JANUS separates real-time perception and action from offline memory consolidation, implementing Dual-Process Theory \citep{kahneman2011thinking, evans2008dual, evans2013dualprocess} as a concrete engineering design. The system divides into two complementary services:

\begin{table}[htbp]
	\centering
	\begin{tabularx}{\textwidth}{|l|l|L|L|}
		\hline
		\textbf{Service} & \textbf{Persona} & \textbf{Cognitive Role}         & \textbf{Biological Analogue} \\
		\hline
		Forward Service  & Janus Bifrons    & Perception \& Action (System~1) & Basal Ganglia \& Thalamus    \\
		Backward Service & Janus Consivius  & Memory \& Learning (System~2)   & Hippocampus \& Neocortex     \\
		\hline
	\end{tabularx}
\end{table}

This separation allows JANUS to optimize for latency on the hot path (Forward Service) while reserving heavy computational resources for consolidation and schema formation on the cold path (Backward Service), implementing CLS theory \citep{mcclelland1995complementary, kumaran2016cls}.

In addition to the core Forward and Backward services, the production system deploys three supporting services: an \textbf{Execution Service} for multi-exchange order routing, a \textbf{Data Service} for centralized market data management, and a \textbf{CNS (Central Nervous System) Service} for system-wide health monitoring and preflight validation (Section~\ref{sec:cns}).

\textbf{Note:} The detailed mathematical specifications for each component are presented in Parts~\ref{part:forward}--\ref{part:rust} below, followed by limitations (Part~\ref{part:limitations}) and validation framework (Part~\ref{part:validation}).

\section{Resilience to Strategy Crowding and Co-Impact}
\label{sec:crowding}

Strategy crowding---the convergence of multiple algorithmic trading systems on similar positions---poses systemic risk through amplified co-impact costs and correlated drawdowns. JANUS's multi-region, heterogeneous architecture is designed to resist these effects.

\subsection{Co-Impact and Algorithmic Herding}

The \textit{square-root impact law}---stating that the price impact of a metaorder scales as $\Delta p \sim \sigma \sqrt{Q/V}$, where $Q$ is order volume and $V$ is daily volume---is one of the most robust empirical regularities in market microstructure \citep{toth2011anomalous, bouchaud2018trades, bucci2019crossover}. \citet{bucci2020coimpact} extended this framework to show how \textit{simultaneous} institutional metaorders interact through net order flow: when $N$ agents each trade $Q/N$, the market responds to aggregate flow without distinguishing individual metaorders. Critically, co-impact introduces a finite intercept $I_0$ that grows with sign correlation among agents---meaning correlated (crowded) trading amplifies impact costs beyond what single-agent models predict.

The canonical empirical demonstration is the August 2007 quant meltdown \citep{khandani2011quants}, where quantitative equity funds that had developed models independently suffered coordinated losses because their strategies had converged on similar factor exposures. \citet{stein2009crowding} formalized this as a coordination problem: individual traders cannot observe how many others pursue the same strategy, creating negative externalities through crowded-trade effects and leverage decisions.

Subsequent empirical work has confirmed crowding's destructive potential.
\citet{vankralingen2021crowded} showed that market clustering---measured against a maximum-entropy null model---has a \textit{causal} effect on tail risk, even after controlling for standard risk drivers.
\citet{brown2022crowded} demonstrated that crowded stocks outperform non-crowded stocks on average but with substantially elevated tail risk.
\citet{lou2022comomentum} proposed ``comomentum'' (high-frequency abnormal return correlation among momentum stocks) as a crowding measure, finding sharp reversals when comomentum is high.

\subsection{Vulnerability in Current Systems}

Contemporary algorithmic trading systems are particularly susceptible to crowding for three reasons:

\begin{enumerate}
	\item \textbf{Shared foundation models:} As firms adopt similar pre-trained architectures, output homogeneity increases regardless of nominal strategy differences
	\item \textbf{Alternative risk premia (ARP) crowding:} \citet{baltas2019arp} classifies ARP into \textit{divergence premia} (e.g., momentum---self-reinforcing, no fundamental anchor) and \textit{convergence premia} (e.g., value---self-correcting), showing divergence premia underperform following crowded periods. \citet{volpati2020zooming} identifies significant and increasing crowding in Fama--French factors
	\item \textbf{AI crowd effects:} \citet{stillman2024neurosymbolic} demonstrates that groups of homogeneous neuro-symbolic traders in virtual markets produce \textit{price suppression}, directly illustrating the risks of AI crowd homogeneity
\end{enumerate}

\subsection{JANUS's Defense: Engineered Heterogeneity}

JANUS addresses crowding through \textit{engineered heterogeneity}: each deployment is configured to produce idiosyncratic order flow that minimizes cross-instance correlation. This is grounded in two key theoretical results:

\begin{itemize}
	\item \citet{wagner2011diversity}: Rational agents should choose heterogeneous portfolios and forgo diversification benefits to avoid joint liquidation risk. Portfolio heterogeneity directly reduces systemic risk.
	\item \citet{demiguel2021crowding}: ``Trading diversification''---institutions exploiting different characteristics---increases capacity by 45\%, optimal investment by 43\%, and profits by 22\%.
\end{itemize}

The heterogeneity is implemented across multiple brain regions:

\begin{table}[htbp]
	\centering
	\caption{Sources of engineered heterogeneity across brain regions}
	\label{tab:heterogeneity}
	\begin{tabularx}{\textwidth}{|l|L|L|}
		\hline
		\textbf{Brain Region} & \textbf{Heterogeneity Mechanism}                         & \textbf{Effect on Order Flow}                \\
		\hline
		Thalamus              & Modality attention weights randomized at initialization  & Different signal weighting per instance      \\
		\hline
		Hypothalamus          & Risk setpoints drawn from individualized distributions   & Different position sizing and risk tolerance \\
		\hline
		Basal Ganglia         & Dopamine sensitivity and discount factors varied         & Different action selection thresholds        \\
		\hline
		Prefrontal            & LTN axiom weights and hedge parameters varied            & Different constraint priorities              \\
		\hline
		Hippocampus           & Replay priorities and consolidation schedules varied     & Different learning from the same experience  \\
		\hline
		Cerebellum            & Execution timing models calibrated to different horizons & Different order placement patterns           \\
		\hline
	\end{tabularx}
\end{table}

\textbf{Important caveat:} The literature supports heterogeneity as a mechanism for crowding \textit{resistance}, not crowding \textit{immunity}. The 2007 quant quake demonstrated that independently developed strategies can converge on similar factor exposures despite nominal diversity \citep{khandani2011quants}. Evolutionary dynamics research shows that crowding effects can emerge even in heterogeneous populations when memory sizes are small. We therefore claim crowding resistance, not elimination, and note that empirical validation via multi-agent simulation is required (see Part~\ref{part:validation}).

% ============================================
% PART 2: FORWARD SERVICE
% ============================================
\newpage
\part{Forward Service (Janus Bifrons)}
\label{part:forward}
\setcounter{section}{0}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}

% Forward Service Content
\section*{Abstract}

JANUS Forward represents the ``wake state'' of the JANUS trading system, responsible for all real-time decision-making during market hours. This service combines:

\begin{itemize}
	\item \textbf{Visual Pattern Recognition} using Gramian Angular Fields (GAF) \citep{wang2015imaging} with LSTM and Video Vision Transformer (ViViT) \citep{arnab2021vivit} temporal modeling
	\item \textbf{Symbolic Reasoning} via Logic Tensor Networks (LTN) \citep{badreddine2022logic} for constraint satisfaction
	\item \textbf{Multimodal Fusion} integrating time series, visual, order book, and sentiment data through specialized fusion engines
	\item \textbf{Dual-Pathway Decision Making} inspired by basal ganglia architecture \citep{collins2014opponent}
	\item \textbf{Ensemble Regime Detection} combining Hidden Markov Models, statistical, and technical methods for market state identification
\end{itemize}

The Forward service operates on a hot path with strict latency requirements, implementing a six-stage neural pipeline (\texttt{regime → hypothalamus → amygdala → gating → correlation → execution}) that routes signals through brain regions in real time. FPGA acceleration \citep{marino2023mevit, vemeko2023fpga} is planned for nanosecond-level latency in future high-frequency trading applications.

\section{Visual Pattern Recognition: DiffGAF and Vision Models}
\label{sec:visual_forward}

The visual subsystem transforms time series data into spatiotemporal images, enabling the system to ``see'' market patterns that traditional numerical methods miss. This approach is grounded in the work of \citet{wang2015imaging}, who demonstrated that imaging time series significantly improves classification and imputation tasks by exposing temporal correlations to the inductive biases of convolutional neural networks.

\subsection{Mathematical Foundation: Gramian Angular Fields}

Time series are encoded into polar coordinates and projected onto Gramian matrices, creating 2D representations that preserve temporal correlations \citep{wang2015imaging}.
Research by \citet{chen2020encoding} has validated that GAF-based encodings of financial candlestick data substantially outperform raw time-series inputs in classification tasks by capturing multi-scale temporal structures.

\subsubsection{Input Preprocessing}

Given raw market data $X = \{x_1, x_2, \ldots, x_T\}$ where $x_t \in \mathbb{R}^D$ (multi-feature time series), we first apply feature selection to extract $F$ relevant features.

\subsubsection{Step 1: Normalization}

For inference, we apply min-max normalization to the domain $[-1, 1]$, followed by Piecewise Aggregate Approximation (PAA) for temporal resizing:
\begin{equation}
	\tilde{x}_t = 2 \cdot \frac{x_t - x_{\min}}{x_{\max} - x_{\min}} - 1
\end{equation}
ensuring the subsequent $\arccos$ operation is well-defined with $\tilde{x}_t \in [-1, 1]$.

For the training pipeline, we employ learnable affine transformations with domain constraints:
\begin{equation}
	\tilde{x}_t = \tanh\left(\gamma \odot \frac{x_t - \mu}{\sigma} + \beta\right)
\end{equation}
where $\gamma, \beta \in \mathbb{R}^F$ are learned parameters, and $\mu, \sigma$ are running statistics. The $\tanh$ function guarantees $\tilde{x}_t \in (-1, 1)$.

\subsubsection{Step 2: Polar Coordinate Transformation}

Map normalized values to angular space:
\begin{align}
	\phi_t & = \arccos(\tilde{x}_t) \in [0, \pi]               \\
	r_t    & = \frac{t}{T} \quad \text{(normalized timestamp)}
\end{align}

\subsubsection{Step 3: Gramian Field Generation}

Construct the Gramian Angular Summation Field (GASF):
\begin{equation}
	\mathbf{G}_{ij} = \cos(\phi_i + \phi_j) = \tilde{x}_i \tilde{x}_j - \sqrt{1-\tilde{x}_i^2}\sqrt{1-\tilde{x}_j^2}
\end{equation}

Or the Gramian Angular Difference Field (GADF), which JANUS employs to encode velocity of price changes as visual textures:
\begin{equation}
	\mathbf{G}_{ij} = \sin(\phi_i - \phi_j) = \sqrt{1-\tilde{x}_i^2}\tilde{x}_j - \tilde{x}_i\sqrt{1-\tilde{x}_j^2}
\end{equation}
This transformation allows the system to visually perceive volatility regimes and microstructure dynamics \citep{wang2015imaging}.

\subsubsection{Differentiable GAF (DiffGAF)}

The Rust implementation provides a fully differentiable GAF engine with analytically computed Jacobians for end-to-end gradient flow. The key derivative through the polar mapping is:
\begin{equation}
	\frac{d\phi_k}{d\tilde{x}_k} = \frac{-1}{\sqrt{1 - \tilde{x}_k^2}}
\end{equation}
which enables backpropagation through the entire GAF encoding pipeline. Numerical gradient verification tests confirm correctness within $10^{-4}$ tolerance.

\subsection{3D Spatiotemporal Manifolds: GAF Video}

To capture temporal dynamics, we generate a sequence of GAF frames using sliding windows.

\subsubsection{Sliding Window GAF Video Generation}

Given a time series of length $T$, window size $W$, and stride $S$:
\begin{enumerate}
	\item Extract windows: $X_k = \{x_{(k-1)S+1}, \ldots, x_{(k-1)S+W}\}$ for $k = 1, \ldots, N$
	\item Generate GAF for each window: $\mathbf{G}_k = \text{GAF}(X_k) \in \mathbb{R}^{W \times W}$
	\item Stack into video: $\mathbf{V} = [\mathbf{G}_1, \mathbf{G}_2, \ldots, \mathbf{G}_N] \in \mathbb{R}^{N \times W \times W}$
\end{enumerate}

The data ingestion pipeline handles windowed buffering and streaming through dedicated preprocessing modules.

\subsection{Vision Model Architecture}

\subsubsection{DiffGAF-LSTM}

The DiffGAF-LSTM vision model pairs the DiffGAF encoding with an LSTM temporal model. GAF frames are encoded and fed sequentially into an LSTM network that captures inter-frame temporal dependencies. This architecture provides robust performance while maintaining the differentiability required for end-to-end training.

\subsubsection{Video Vision Transformer (ViViT)}

The ViViT model uses a factorized spatiotemporal transformer \citep{arnab2021vivit}. Unlike standard Vision Transformers \citep{dosovitskiy2020image} which process static images, ViViT factorizes attention across both space (price/volume levels of the limit order book) and time (sequences of LOB snapshots), enabling the system to track dynamic microstructure events.

\textbf{Patch Embedding:}
Divide each frame $\mathbf{G}_k$ into non-overlapping patches:
\begin{equation}
	\mathbf{P}_k = \text{Reshape}(\mathbf{G}_k) \in \mathbb{R}^{P \times (p^2)}
\end{equation}
where $P = {(W/p)}^2$ is the number of patches per frame.

\textbf{Spatial Attention:}
Apply self-attention within each frame:
\begin{equation}
	\mathbf{Z}^{(l)}_k = \text{MSA}(\text{LN}(\mathbf{Z}^{(l-1)}_k)) + \mathbf{Z}^{(l-1)}_k
\end{equation}

\textbf{Temporal Attention:}
Apply attention across frames:
\begin{equation}
	\mathbf{H}^{(l)} = \text{MSA}(\text{LN}([\mathbf{Z}^{(l)}_1, \ldots, \mathbf{Z}^{(l)}_N]))
\end{equation}

Both the DiffGAF-LSTM and ViViT architectures are implemented natively in Rust, with the system selecting the appropriate model based on configuration and available resources. The ViViT model gracefully degrades to the DiffGAF-LSTM model when computational constraints require it.

\section{Ensemble Regime Detection}
\label{sec:regime_detection}

Market regime identification is a critical upstream component that conditions all downstream decision-making. JANUS employs an ensemble approach combining multiple detection methods to robustly classify market states.

\subsection{Detection Methods}

\subsubsection{Hidden Markov Model (HMM)}

A Gaussian HMM models latent regime states with observable market features:
\begin{equation}
	P(\mathbf{x}_t \mid z_t = k) = \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{equation}
where $z_t \in \{1, \ldots, K\}$ is the latent regime state and the transition matrix $\mathbf{A}_{ij} = P(z_{t+1} = j \mid z_t = i)$ captures regime persistence and switching dynamics.

\subsubsection{Statistical Methods}

Complementing the HMM, JANUS applies rolling statistical tests including variance ratio tests for mean-reversion detection, Hurst exponent estimation for trend persistence, and distributional tests for fat-tail identification.

\subsubsection{Technical Methods}

Classical technical analysis signals (trend strength indicators, volatility measures, momentum oscillators) are fused with the statistical methods to provide corroborating evidence for regime classification.

\subsection{Regime Categories}

The ensemble classifier identifies seven market regimes, each mapped to distinct trading behavior profiles:

\begin{table}[htbp]
	\centering
	\small
	\begin{tabularx}{\textwidth}{|l|L|L|}
		\hline
		\textbf{Regime} & \textbf{Characteristics}                    & \textbf{System Behavior}           \\
		\hline
		Bull            & Sustained upward trend, expanding volume    & Amplify Direct (Go) pathway        \\
		\hline
		Bear            & Sustained downward trend, risk-off          & Amplify Indirect (No-Go) pathway   \\
		\hline
		Ranging         & Low directional conviction, bounded action  & Favor mean-reversion strategies    \\
		\hline
		Crisis          & Extreme volatility, correlation breakdown   & Engage Amygdala circuit breakers   \\
		\hline
		Recovery        & Post-crisis normalization, lower volatility & Gradually release risk constraints \\
		\hline
		Bubble          & Parabolic acceleration, euphoric sentiment  & Heighten caution via Hypothalamus  \\
		\hline
		Deflation       & Sustained contraction, liquidity withdrawal & Reduce position sizing             \\
		\hline
	\end{tabularx}
\end{table}

The detected regime feeds into the Hypothalamus module for homeostatic regulation and the Basal Ganglia for dopamine-modulated action selection, forming the first stage of the brain wiring pipeline.

\section{Logic Tensor Networks: Symbolic Reasoning Engine}

LTNs bridge neural networks and first-order logic, enabling differentiable constraint satisfaction \citep{badreddine2022logic, serafini2016ltn}.
This neuro-symbolic approach allows JANUS to enforce regulatory and risk constraints directly within gradient descent optimization, a capability absent in pure deep learning systems \citep{marra2024neurosymbolic, garcez2023neurosymbolic}.
No prior work applies LTN to financial decision-making; this represents a novel contribution of~JANUS.

\subsection{Mathematical Foundation}

The central innovation of LTNs \citep{badreddine2022logic} is the ability to make Boolean logic differentiable using Real Logic, specifically Łukasiewicz t-norms.

\subsubsection{Grounding Function}

Map logical constants to real vectors:
\begin{equation}
	\mathcal{G}: \mathcal{C} \to \mathbb{R}^d
\end{equation}

\subsubsection{Predicate Grounding}

A predicate $P(x)$ is grounded as a neural network $f_\theta: \mathbb{R}^d \to [0,1]$, where truth values range continuously from 0 to 1 rather than being discrete binary values.

\subsection{Łukasiewicz T-Norm Operations}

Following \citet{badreddine2022logic}, we employ fuzzy logic operators based on Łukasiewicz t-norms that are differentiable and thus compatible with backpropagation.

\subsubsection{Conjunction (AND)}

For training, we use Product Logic to ensure smooth~gradients:
\begin{equation}
	u \land v = u \cdot v
\end{equation}

For inference/evaluation, standard Łukasiewicz logic is used:
\begin{equation}
	u \land v = \max(0, u + v - 1)
\end{equation}

\subsubsection{Disjunction (OR)}
\begin{equation}
	u \lor v = \min(1, u + v)
\end{equation}

\subsubsection{Negation (NOT)}
\begin{equation}
	\neg u = 1 - u
\end{equation}

\subsubsection{Implication (IF-THEN)}

For training (Product Logic):
\begin{equation}
	u \Rightarrow v = 1 - u + u \cdot v
\end{equation}

For inference (Łukasiewicz Logic):
\begin{equation}
	u \Rightarrow v = \min(1, 1 - u + v)
\end{equation}

\subsubsection{Bi-Implication (Equivalence)}
\begin{equation}
	u \Leftrightarrow v = 1 - |u - v|
\end{equation}

\subsubsection{Linguistic Hedges}

The implementation extends classical fuzzy logic with linguistic hedge operators for nuanced truth-value modification:
\begin{align}
	\text{very}(x)      & = x^2          \\
	\text{somewhat}(x)  & = \sqrt{x}     \\
	\text{slightly}(x)  & = \sqrt{x} - x \\
	\text{extremely}(x) & = x^3
\end{align}

These hedges allow more expressive constraint formulation (e.g., ``very risky'' vs.\ ``somewhat risky'' conditions).

\subsection{Knowledge Base Formulation}

The knowledge base $\mathcal{KB}$ encodes regulatory constraints and risk management rules as logical predicates. The implementation provides a comprehensive compliance engine covering multiple constraint categories.

\subsubsection{Wash Sale Constraint}

The Wash Sale Rule \citep{irs2024wash}—a critical regulatory constraint for active traders—prevents claiming tax losses on securities sold and repurchased within 30 days. The implementation enforces the full 30-day window both before and after the sale, tracks loss sales per symbol, computes disallowed loss amounts, and blocks violating trades:
\begin{equation}
	\forall t: \text{Sell}(t) \land \text{Buy}(t') \land |t-t'| < 30 \Rightarrow \neg \text{TaxLoss}(t)
\end{equation}

\subsubsection{Almgren-Chriss Risk Constraint}

Following the optimal execution framework of \citet{almgren2001optimal}, we constrain market impact relative to volatility:
\begin{equation}
	\forall \text{order}: \text{Execute}(\text{order}) \Rightarrow \text{Slippage}(\text{order}) < \lambda \cdot \text{Volatility}
\end{equation}
This ensures trades remain within the efficient frontier between expected cost and risk.

\subsubsection{Additional Constraint Categories}

Beyond the foundational constraints above, the production knowledge base includes:

\begin{itemize}
	\item \textbf{Position Limits:} Maximum exposure per asset and aggregate portfolio
	\item \textbf{Capital Allocation:} Constraints on capital deployment across strategies
	\item \textbf{Risk Limits:} Value-at-Risk, maximum drawdown, and volatility thresholds
	\item \textbf{Proprietary Firm Rules:} Compliance with specific prop trading firm requirements (daily loss limits, trailing drawdown, consistency rules)
\end{itemize}

\subsection{Logical Loss Function}

\subsubsection{Satisfiability Aggregation}
\begin{equation}
	\text{SAT}(\mathcal{KB}) = \text{p-mean}_{i=1}^{|\mathcal{KB}|}(\phi_i)
\end{equation}

The evaluation context maintains EMA-smoothed satisfaction scores with per-constraint violation tracking for monitoring and diagnostics.

\subsubsection{Logical Loss}
\begin{equation}
	\mathcal{L}_{\text{logic}} = 1 - \text{SAT}(\mathcal{KB})
\end{equation}

\section{Multimodal Fusion: Specialized Gated Attention}

JANUS integrates multiple data modalities through gated cross-attention \citep{alayrac2022flamingo}, allowing the system to dynamically weight inputs based on their predictive uncertainty. This is the machine-learning analogue of thalamic attentional gating in biological systems \citep{halassa2017thalamic, george2025thalamic}.

\subsection{Input Modalities}

The production system processes four specialized data streams through dedicated fusion engines:

\begin{itemize}
	\item \textbf{Order Book:} Full limit order book depth, bid-ask dynamics, and microstructure features
	\item \textbf{Price:} Multi-timeframe price action including OHLCV and derived indicators, with Chronos foundation model forecasting \citep{ansari2024chronos, ansari2024chronos2}
	\item \textbf{Volume:} Volume profile analysis, volume-weighted metrics, and participation rates
	\item \textbf{Sentiment:} BERT/FinBERT embeddings \citep{candle2024} from news feeds (NewsAPI, CryptoPanic) and social sentiment signals, with Qdrant-backed similarity search for regime-aware aggregation
\end{itemize}

Additionally, the system integrates supplementary data sources including weather data (via OpenWeatherMap) and space weather/celestial data for correlation analysis with commodity and energy markets.

\subsection{Gated Cross-Attention Mechanism}

\subsubsection{Attention Computation}

Following the attention mechanism of \citet{vaswani2017attention}, with support for causal masking and multi-head configuration:
\begin{equation}
	\text{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \softmax\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\end{equation}

\subsubsection{Gating Mechanism}

The gating function suppresses noise and amplifies signal, similar to thalamic regulation \citep{halassa2017thalamic}:
\begin{equation}
	g = \sigmoid(\mathbf{W}_g[\mathbf{v}; \mathbf{t}; \mathbf{s}] + \mathbf{b}_g)
\end{equation}

Each modality-specific fusion engine applies this gating independently, allowing the system to dynamically suppress noisy order book data during low-liquidity periods while amplifying sentiment signals during news-driven regimes.

\section{Decision Engine: Basal Ganglia Pathways}
\label{sec:decision_engine}

Action selection in JANUS is mediated by a model of the Basal Ganglia, comprising Direct (``Go'') and Indirect (``No-Go'') pathways \citep{collins2014opponent, foster2013hierarchical}. This architecture, known as Opponent Actor Learning (OpAL), creates dynamic risk tolerance that adapts to market conditions.

\subsection{Praxeological Motor: Dual Pathways}

\subsubsection{Direct Pathway (Go Signal)}

Encodes the benefits of an action, amplified by dopamine during high-confidence regimes \citep{schultz1997reward}:
\begin{equation}
	Q^+_\theta(s, a) = \mathbb{E}[\text{Reward} \mid s, a]
\end{equation}

The implementation includes dopamine sensitivity parameters, learning rates for value updates, decay rates, and threshold-based action release.

\subsubsection{Indirect Pathway (No-Go Signal)}

Encodes the costs and risks, amplified during uncertainty \citep{collins2014opponent}:
\begin{equation}
	Q^-_\theta(s, a) = \mathbb{E}[\text{Risk} \mid s, a]
\end{equation}

The implementation provides risk assessment, inhibition generation, and caution scoring.

\subsection{Action Selection}

The final action is determined by the competition between pathways:
\begin{equation}
	\mathbf{a}_t = \softmax(\mathbf{d}_{\text{direct}} - \lambda \cdot \mathbf{d}_{\text{indirect}})
\end{equation}
where $\lambda > 0$ is the inhibition weight parameter, and each pathway is computed as:
\begin{align}
	\mathbf{d}_{\text{direct}}   & = \text{ReLU}(\mathbf{W}_{\text{direct}} \mathbf{h} + \mathbf{b}_{\text{direct}})     \\
	\mathbf{d}_{\text{indirect}} & = \text{ReLU}(\mathbf{W}_{\text{indirect}} \mathbf{h} + \mathbf{b}_{\text{indirect}})
\end{align}
where $\mathbf{h}$ is the fused state representation from the Thalamus.

An actor-critic framework ties both pathways together, with decision confidence scoring determining whether the action is released for execution.

\subsection{Brain Wiring Pipeline}

In production, decisions traverse a six-stage pipeline that chains brain regions together:

\begin{enumerate}
	\item \textbf{Regime Detection:} Ensemble classifier identifies the current market state
	\item \textbf{Hypothalamus:} Adjusts position sizing and risk appetite based on regime and portfolio homeostasis
	\item \textbf{Amygdala:} Evaluates threat signals; may trigger circuit breakers before further processing
	\item \textbf{Gating:} Thalamic attention gates filter and weight modality signals
	\item \textbf{Correlation:} Cross-asset correlation tracking across monitored pairs informs diversification
	\item \textbf{Execution:} Basal ganglia action selection routes to the Execution Service
\end{enumerate}

This pipeline is operationally richer than the abstract dual-pathway model, reflecting the biological reality that action selection involves coordination across multiple brain regions.

\subsection{\texorpdfstring{Cerebellar Forward Model \citep{wolpert1998cerebellum, sokolov2017cerebellum}}{Cerebellar Forward Model}}

The Cerebellar module simulates market dynamics to predict execution outcomes before committing to a trade.

\subsubsection{Market Impact Prediction}

Following the Almgren-Chriss framework \citep{almgren2001optimal, markwick2023almgren}, the model predicts slippage and volatility.
To detect predatory environments, JANUS employs the VPIN (Volume-Synchronized Probability of Informed Trading) metric \citep{easley2011vpin, easley2012flow}, which serves as a proxy for ``Flow Toxicity''---the probability that the counterparty has superior information.
High VPIN levels often precede flash crashes and feed into the Amygdala circuit for threat detection.
\begin{equation}
	\hat{p}_{t+1} = f_{\text{cerebellum}}(\mathbf{s}_t, \mathbf{a}_t)
\end{equation}

\subsubsection{Execution Error Correction}

The Cerebellum also provides closed-loop error correction for execution quality, implemented through a PID controller:
\begin{equation}
	u(t) = K_p \cdot e(t) + K_i \cdot \int_0^t e(\tau)\,d\tau + K_d \cdot \frac{de(t)}{dt}
\end{equation}
where $e(t)$ is the deviation between predicted and realized execution cost. Adaptive correction and feedback loops continuously refine the forward model's predictions based on observed outcomes.

% ============================================
% PART 3: BACKWARD SERVICE
% ============================================
\newpage
\part{Backward Service (Janus Consivius)}
\label{part:backward}
\setcounter{section}{0}

\section*{Abstract}

JANUS Backward represents the ``sleep state'' of the system, responsible for memory consolidation, schema formation, and learning from accumulated experience. This service implements the Complementary Learning Systems (CLS) theory \citep{mcclelland1995complementary}, which posits that intelligent agents require two learning systems: a fast-learning hippocampus for episodic details and a slow-learning neocortex for statistical generalization. This service implements:

\begin{itemize}
	\item \textbf{Three-Timescale Memory Hierarchy} (Hippocampus → SWR → Neocortex) following CLS architecture \citep{mcclelland1995complementary}
	\item \textbf{Sharp-Wave Ripple Simulation} for prioritized experience replay \citep{buzsaki2015hippocampal, schaul2015prioritized}
	\item \textbf{Schema Formation} via feature-range matching with UMAP-based visualization \citep{mcinnes2018umap, mcinnes2018umap_software}
	\item \textbf{Recall-Gated Consolidation} ensuring only successful patterns are promoted \citep{frank2006working}
	\item \textbf{Signal Persistence and Analytics} via PostgreSQL repositories
\end{itemize}

The Backward service runs on a cold path during off-market hours, performing computationally intensive operations to distill daily experiences into long-term knowledge, effectively replicating the biological process of memory consolidation during sleep~\citep{buzsaki2015hippocampal}.

\section{Memory Hierarchy: Three-Timescale Architecture}

The three-tier memory architecture directly implements the Complementary Learning Systems theory \citep{mcclelland1995complementary}, preventing catastrophic forgetting while enabling rapid learning of new market patterns.

\subsection{Short-Term Memory (Hippocampus)}

The hippocampal buffer stores episodic memories of individual trading events, enabling fast learning without interfering with consolidated knowledge \citep{mcclelland1995complementary}.

\subsubsection{Episodic Buffer}

Stores raw experiences during trading, mirroring the role of biological hippocampus in episodic memory formation:
\begin{equation}
	\mathcal{D}_{\text{hippo}} = \{(s_t, a_t, r_t, s_{t+1}, \mathbf{c}_t, \mathbf{e}_t)\}_{t=1}^{T} % chktex 3
\end{equation}
where $\mathbf{c}_t$ contains contextual metadata (volatility, spreads, volume) and $\mathbf{e}_t$ contains emotional tags (fear level, confidence, surprise) that bias consolidation~priority.

\subsubsection{Pattern Separation}

Uses random projections to ensure diverse encoding:
\begin{equation}
	\mathbf{h}_t = \tanh(\mathbf{W}_{\text{rand}} \cdot [s_t; a_t; \mathbf{c}_t])
\end{equation}

\subsubsection{Spatial Mapping}

Experiences are organized into a spatial map that preserves topological relationships between market states, analogous to hippocampal place cells. This enables efficient retrieval of contextually similar experiences during replay.

\subsection{Medium-Term Consolidation (SWR Simulator)}

Consolidation occurs during Sharp-Wave Ripples (SWRs)—high-frequency oscillations that replay compressed sequences of neural activity \citep{buzsaki2015hippocampal}. JANUS mimics this using Prioritized Experience Replay \citep{schaul2015prioritized}, modified to incorporate surprise, emotion, and logical violations.

\subsubsection{Replay Prioritization}

During ``sleep'' (post-market hours), experiences are replayed in priority order. Biological research shows that replay is biased towards salient and novel events \citep{kar2023selection, mattar2018prioritized}, which JANUS replicates through composite priority scoring. This design is directly inspired by hippocampal sharp-wave ripple replay \citep{wilson1994replay, buzsaki2015hippocampal, buzsaki1989twostage}, which the deep RL community adopted as ``experience replay'' \citep{mnih2015human, schaul2015prioritized}.
Compute TD-error based priority:
\begin{equation}
	p_i = |\delta_i| + \epsilon
\end{equation}
where $\delta_i = r_i + \gamma \max_{a'} Q(s_{i+1}, a') - Q(s_i, a_i)$ and $\epsilon = 10^{-6}$ ensures numerical~stability.

\subsubsection{Sampling Probability}
\begin{equation}
	P(i) = \frac{p_i^\alpha}{\sum_j p_j^\alpha}
\end{equation}
where $\alpha \in [0, 1]$ controls prioritization strength (default $\alpha = 0.6$).

\subsubsection{Importance Sampling Correction}
\begin{equation}
	w_i = \left(\frac{1}{N \cdot P{(i)}}\right)^\beta % chktex 3
\end{equation}
where $\beta$ is annealed from $0.4 \to 1.0$ during training via a configurable increment parameter, fully correcting bias at convergence.

\subsubsection{Efficient Sampling via Sum Tree}

The replay buffer uses a sum tree data structure for $\mathcal{O}(\log n)$ sampling, enabling efficient prioritized replay even with large buffer sizes.

\subsubsection{Sleep-Phase Consolidation}

The SWR simulator progresses through biologically inspired phases:
\begin{equation}
	\text{Phase} \in \{\text{Awake} \to \text{Light} \to \text{Deep} \to \text{Integration} \to \text{Transition}\}
\end{equation}
Each phase adjusts replay parameters (compression ratio, replay rate, consolidation threshold), mirroring the empirical finding that different sleep stages serve distinct memory functions.

\subsection{Long-Term Memory (Neocortex)}

\subsubsection{Schema Representation}

Schemas represent learned market regime prototypes. The production implementation uses feature-range matching with weighted multi-criteria scoring rather than pure centroid-based clustering:
\begin{equation}
	\text{match}(\mathbf{x}, \mathcal{S}_k) = \frac{1}{|\mathcal{F}|} \sum_{f \in \mathcal{F}} w_f \cdot \mathbb{1}[x_f \in \text{range}_f^{(k)}]
\end{equation}
where $\mathcal{F}$ is the feature set, $w_f$ is the feature weight, and $\text{range}_f^{(k)}$ is the acceptable range for feature $f$ in schema $k$. This approach is deterministic and interpretable, providing explicit decision boundaries for each regime.

The system supports seven regime schemas (Bull, Bear, Ranging, Crisis, Recovery, Bubble, Deflation), each with a Markov transition matrix for regime prediction:
\begin{equation}
	P(\mathcal{S}_{t+1} = j \mid \mathcal{S}_t = i) = \mathbf{T}_{ij}
\end{equation}

For embedding-based operations (similarity search, schema formation from new experiences), centroid-based representations remain available:
\begin{equation}
	\mathbf{z}_k = \frac{1}{|\mathcal{C}_k|} \sum_{i \in \mathcal{C}_k} \mathbf{h}_i
\end{equation}

\subsubsection{Recall-Gated Consolidation}

Only update schemas from successfully recalled experiences:
\begin{equation}
	\mathbf{z}_k \leftarrow \mathbf{z}_k + \eta \cdot \mathbb{1}[\text{recall\_success}] \cdot (\mathbf{h}_{\text{new}} - \mathbf{z}_k)
\end{equation}

\section{UMAP Visualization: Cognitive Dashboard}

To visualize and manage schema structures, JANUS employs Uniform Manifold Approximation and Projection (UMAP) \citep{mcinnes2018umap}, which preserves both local and global structure better than alternatives like t-SNE\@.

\subsection{AlignedUMAP for Schema Formation}

Track how internal representations evolve over time using AlignedUMAP \citep{mcinnes2018umap, mcinnes2018umap_software}, which aligns manifolds across different time steps to monitor representational drift.
Maintains consistent embeddings across sleep cycles.

\subsubsection{Objective Function}

The full UMAP loss includes both attraction and repulsion terms:
\begin{equation}
	\mathcal{L}_{\text{UMAP}} = \sum_{i \neq j} \left[
		w_{ij} \log(q_{ij}) + (1 - w_{ij}) \log(1 - q_{ij})
		\right]
\end{equation}
where $q_{ij} = {\left(1 + \|\mathbf{y}_i - \mathbf{y}_j\|^2\right)}^{-1}$.

\textbf{Note:} In practice, the repulsion term $(1 - w_{ij})$ is approximated via \emph{negative sampling} to achieve $\mathcal{O}(N)$ complexity. For each positive edge, we sample $k=5$ random negative pairs.

\subsection{Parametric UMAP for Real-Time Monitoring}

Train a neural network to project new experiences:
\begin{equation}
	\mathbf{y}_{\text{new}} = f_\theta(\mathbf{h}_{\text{new}})
\end{equation}

The parametric UMAP implementation uses a multi-layer encoder network trained on the fuzzy simplicial set graph, with configurable distance metrics (Euclidean, Cosine, Manhattan), smooth $k$-nearest-neighbor bandwidth estimation, and a Qdrant bridge for persistent storage of projected embeddings. A drift detection module computes embedding-space displacement between reference and live data, classifying severity (Low/Moderate/High/Critical) for regime monitoring. Regime cluster analysis provides per-cluster statistics (centroid, intra-cluster distances, standard deviation) and inter-cluster separation metrics.

\section{Persistence Layer}

\subsection{Primary Storage: PostgreSQL and Redis}

The production system uses PostgreSQL for ACID-compliant storage of trading records and Redis for low-latency operational state:

\begin{itemize}
	\item \textbf{Signal Repository:} Persists generated trading signals with full metadata
	\item \textbf{Portfolio Repository:} Tracks positions, P\&L, and capital allocation
	\item \textbf{Performance Repository:} Stores performance analytics for backward analysis
	\item \textbf{Redis:} Walk-forward optimizer parameters, kill switch state, hot-reloadable configuration
\end{itemize}

PostgreSQL provides the ACID guarantees essential for financial records, while Redis enables sub-millisecond reads for time-critical operational state.

\subsection{Vector Storage: Qdrant}

For schema similarity search, JANUS integrates Qdrant \citep{qdrant2024}, a high-performance vector similarity search engine. Schemas are stored with L2-normalized centroid vectors for cosine similarity search:

\begin{equation}
	\mathcal{N}_k = \argmax_{k} \text{cosine}(\mathbf{h}_t, \mathbf{z}_k)
\end{equation}

The vector database layer complements the relational storage, serving the specific use case of nearest-neighbor retrieval for regime pattern matching.

% ============================================
% PART 4: NEUROMORPHIC ARCHITECTURE
% ============================================
\newpage
\part{Neuromorphic Architecture}
\label{part:neuro}
\setcounter{section}{0}

\section*{Abstract}

This document maps the computational components of Project JANUS to specific brain regions, following the Doya--Hassabis framework established in Section~\ref{sec:doya_hassabis}. The neuromorphic approach is grounded in functional brain-region decomposition \citep{doya1999computations, hassabis2017neuroscience, caligiore2019superlearning} and provides:

\begin{itemize}
	\item \textbf{Modular Design} with clear functional boundaries mirroring brain organization \citep{yamakawa2021whole}
	\item \textbf{Biological Validation} of architectural decisions based on empirical neuroscience \citep{mcclelland1995complementary, kumaran2016cls}
	\item \textbf{Emergent Intelligence} through brain-inspired interactions and allostatic regulation \citep{sterling2012allostasis}
\end{itemize}

\textbf{Positioning relative to existing work:} The intersection of neuromorphic computing and finance is extremely sparse. \citet{mohan2025snn_portfolio} apply spiking neural networks to cross-market portfolio optimization but use no brain-region mapping. \citet{bi2025financial_connectome} propose a ``Financial Connectome'' using brain connectivity analysis methods on market data, but this applies neuroscience \textit{analysis tools} to markets rather than neuroscience \textit{architectural principles} to trading systems. \citet{ezinwoke2025snn_hft} apply SNNs to HFT price prediction without multi-region architecture. \textbf{No published system maps multiple brain regions to distinct trading subsystems.} JANUS is the first.

\section{Neuromorphic Design Philosophy}

\subsection{Why Brain-Inspired Architecture?}

The brain efficiently solves problems similar to trading \citep{daw2006cortical}, demonstrating capabilities that map directly to trading challenges:
\begin{itemize}
	\item Pattern recognition under uncertainty (visual cortex and hippocampus \citep{buzsaki2015hippocampal})
	\item Fast decision-making with delayed rewards (basal ganglia \citep{collins2014opponent})
	\item Continual learning without catastrophic forgetting (complementary learning systems \citep{mcclelland1995complementary})
	\item Multi-timescale memory consolidation (hippocampus to neocortex \citep{buzsaki2015hippocampal})
	\item Homeostatic regulation under varying conditions (hypothalamic control \citep{sterling2012allostasis})
\end{itemize}

\subsection{Neuroscience-to-Trading Mapping}

This mapping is grounded in empirical neuroscience and cognitive modeling \citep{frank2006working, collins2014opponent, foster2013hierarchical}. JANUS implements ten brain regions, each serving a distinct functional role:

\begin{table}[htbp]
	\centering
	\begin{tabularx}{\textwidth}{|l|L|L|}
		\hline
		\textbf{Brain Region} & \textbf{Biological Function}                          & \textbf{Trading Function}                                        \\
		\hline
		Visual Cortex         & Pattern recognition                                   & GAF/ViViT chart analysis \citep{wang2015imaging, arnab2021vivit} \\
		\hline
		Hippocampus           & Episodic memory \citep{buzsaki2015hippocampal}        & Experience replay buffer \citep{schaul2015prioritized}           \\
		\hline
		Prefrontal Cortex     & Logic and planning \citep{frank2006working}           & LTN constraint checking \citep{badreddine2022logic}              \\
		\hline
		Basal Ganglia         & Action selection \citep{collins2014opponent}          & Buy/sell/hold decisions                                          \\
		\hline
		Cerebellum            & Motor prediction \citep{wolpert1998cerebellum}        & Market impact forecasting \citep{almgren2001optimal}             \\
		\hline
		Amygdala              & Threat detection \citep{ledoux2000emotion}            & Risk circuit breakers                                            \\
		\hline
		Thalamus              & Attentional gating \citep{halassa2017thalamic}        & Modality fusion and signal routing                               \\
		\hline
		Hypothalamus          & Homeostatic regulation \citep{sterling2012allostasis} & Position sizing and risk appetite                                \\
		\hline
		Cortex                & Strategic planning                                    & Regime schemas and hierarchical RL                               \\
		\hline
		Integration           & Inter-region coordination                             & Service bridges and data pipeline                                \\
		\hline
	\end{tabularx}
\end{table}

\section{Brain Region Architectures}

The following sections detail how each brain region's computational principles are implemented in JANUS\@.

\subsection{Visual Cortex: Pattern Recognition}

The visual cortex processes market data as images through the DiffGAF pipeline (Section~\ref{sec:visual_forward}), with submodules for GAF encoding (GASF/GADF), vision model inference (DiffGAF-LSTM and ViViT), data ingestion and buffering, parametric UMAP \citep{mcinnes2018umap} with neural encoder projection, drift detection (Low/Moderate/High/Critical severity classification), and Qdrant-backed regime cluster persistence for real-time representation monitoring.

\subsection{Cortex: Strategic Planning \& Long-term Memory}

The neocortex implements slow, statistical learning of market schemas \citep{mcclelland1995complementary}.

\subsubsection{Trading Implementation}

\textbf{Component:} Neocortical Schema Network with Hierarchical RL Manager

\begin{itemize}
	\item Schema prototypes stored with feature-range matching and confidence scoring
	\item Seven market regime templates (Bull, Bear, Ranging, Crisis, Recovery, Bubble, Deflation) with Markov transition matrices
	\item Declarative memory and long-term knowledge base for persistent market insights
	\item Hierarchical RL manager for multi-level strategic planning
	\item Slow consolidation during sleep cycles
\end{itemize}

\subsection{Hippocampus: Episodic Memory \& Experience Replay}

The hippocampus provides fast learning and episodic memory storage, with consolidation via Sharp-Wave Ripples \citep{buzsaki2015hippocampal, kar2023selection}.

\subsubsection{Trading Implementation}

\textbf{Component:} Episodic Buffer + SWR Replay

\begin{itemize}
	\item Fixed-size circular buffer storing recent trades
	\item Sparse encoding via random projections
	\item Emotional tagging for consolidation priority
	\item Trade episode and market event recording
	\item Spatial mapping of experience relationships
	\item Prioritized replay with sum-tree sampling during training
	\item Multi-phase sleep consolidation (Awake → Light → Deep → Integration → Transition)
	\item \textbf{Diffusion-Based Synthetic Data:} Regime-conditional DDPM \citep{coletta2024conditional} generates synthetic market sequences for training data augmentation, with configurable noise schedules (linear, cosine, quadratic), Min-SNR loss weighting, EMA model averaging, and automated quality assessment comparing synthetic vs.\ real feature distributions and autocorrelation structure
\end{itemize}

\subsection{Thalamus: Attentional Gating \& Modality Fusion}
\label{sec:thalamus}

The Thalamus functions as the ``gatekeeper'' of perception in JANUS, regulating the flow of visual and numerical data into the decision engine \citep{halassa2017thalamic, george2025thalamic}.

\subsubsection{Trading Implementation}

\textbf{Component:} Multi-Head Cross-Attention with Modality-Specific Fusion

\begin{itemize}
	\item Cross-attention with causal masking and residual connections
	\item Saliency computation and attentional focus control
	\item Gating mechanism for signal suppression and amplification
	\item Specialized fusion engines: order book, price, volume, and sentiment
	\item External data source integration (news, weather, celestial/space weather)
	\item Signal routing to downstream brain regions
	\item \textbf{Chronos Time Series Forecasting:} ONNX-based inference pipeline \citep{ansari2024chronos, ansari2024chronos2} with quantile-based tokenization, configurable presets (T5-Tiny/\allowbreak{}Small/\allowbreak{}Base), and confidence-interval forecast~output
	\item \textbf{BERT Sentiment Analysis:} FinBERT/\allowbreak{}DistilBERT sentiment embeddings \citep{candle2024} running natively in Rust via Candle, producing \texttt{[CLS]} embeddings stored in Qdrant for regime-aware sentiment~aggregation
\end{itemize}

The Thalamic Reticular Nucleus provides attentional gating, enabling JANUS to focus computational resources on the most informative market data streams.
Wilson-Cowan mean-field models \citep{wilson1972excitatory, wilson2024bidirectional} implement oscillatory attention dynamics, modeling coupled excitatory-inhibitory neural populations via ODEs with Hilbert-transform-based amplitude/phase estimation, bifurcation analysis, neuromodulator-driven regime switching, and fixed-point stability~classification.

\subsection{Hypothalamus: Homeostatic Regulation}
\label{sec:hypothalamus}

The Hypothalamus implements allostatic regulation \citep{sterling2012allostasis}, maintaining the system's internal balance across varying market conditions.

\subsubsection{Trading Implementation}

\textbf{Component:} Adaptive Position Sizing and Risk Appetite Control

\begin{itemize}
	\item \textbf{Position Sizing:} Dynamically adjusts position sizes based on regime, portfolio heat, and recent performance using Kelly Criterion-inspired scaling
	\item \textbf{Homeostasis:} Monitors portfolio-level vital signs (exposure, drawdown, correlation) and applies corrective adjustments to maintain target ranges
	\item \textbf{Energy Management:} Tracks ``metabolic'' state of the trading system—capital utilization, margin usage, and recovery capacity—and modulates aggression accordingly
	\item \textbf{Risk Appetite:} Integrates regime signals from the ensemble detector with internal portfolio state to produce a single risk appetite scalar that modulates all downstream position sizing
\end{itemize}

The Hypothalamus sits at the second stage of the brain wiring pipeline, translating raw regime detection into calibrated risk parameters before signals reach the Amygdala and downstream modules.

\subsection{Basal Ganglia: Action Selection \& Reinforcement Learning}

The basal ganglia implements Opponent Actor Learning (OpAL) \citep{collins2014opponent, jaskir2023opal}, balancing Go (Direct) and No-Go (Indirect) pathways modulated by dopamine \citep{schultz1997reward}.
\citet{jaskir2023opal} extended OpAL with dynamic dopamine modulation, normalized prediction errors, and uncertainty-adjusted learning rates (OpAL*), demonstrating robust advantages in sparse reward environments and large action spaces---conditions directly analogous to financial markets.
High dopamine (bull market/high confidence) amplifies the Direct pathway; low dopamine (bear market/uncertainty) amplifies the Indirect pathway.
This creates dynamic risk tolerance that adapts to market volatility, implementing allostatic regulation \citep{sterling2012allostasis} rather than simple homeostatic feedback.
The Hypothalamus module (Section~\ref{sec:hypothalamus}) translates regime detection into calibrated risk parameters that modulate dopaminergic signaling throughout the decision pipeline.
\textbf{No prior work applies OpAL or any basal ganglia computational model to financial decision-making.}
See Section~\ref{sec:decision_engine} for the complete mathematical formulation.

\subsection{Prefrontal Cortex: Logic, Planning \& Compliance}

The prefrontal cortex provides working memory gating and logical reasoning capabilities \citep{frank2006working}.

\subsubsection{Trading Implementation}

\textbf{Component:} Logic Tensor Network + Conscience Module

\begin{itemize}
	\item Łukasiewicz fuzzy logic with linguistic hedges
	\item Predicate grounding and constraint satisfaction
	\item Wash sale rule enforcement with full 30-day window tracking
	\item Position limits and risk limit constraints
	\item Proprietary firm rule compliance (daily loss limits, trailing drawdown, consistency)
	\item Strategic planning with goal decomposition, subgoal generation, plan synthesis, and contingency planning
	\item \textbf{Quantum-Inspired Portfolio Optimization} \citep{mugel2022dynamic}: QAOA (Quantum Approximate Optimization Algorithm) simulator for combinatorial asset selection via QUBO formulation, VQE (Variational Quantum Eigensolver) for continuous weight optimization using parameterized circuits, and simulated quantum annealing for escaping local minima in non-convex portfolio landscapes --- complementing classical Mean-Variance, Risk Parity, and Black-Litterman optimizers
\end{itemize}

\subsection{Amygdala: Fear, Threat Detection \& Circuit Breakers}

The amygdala provides rapid threat detection and fear learning \citep{ledoux2000emotion}, with connections to substantia nigra enabling fear extinction \citep{monfils2009extinction}. The amygdala's role in encoding emotional significance and modulating memory consolidation \citep{caligiore2019superlearning} maps directly to JANUS's circuit-breaker and fear-network architecture.

\subsubsection{Trading Implementation}

\textbf{Component:} Multi-Layer Threat Detection and Safety System

\textbf{Anomaly Detection} uses multiple complementary methods: Z-score deviation, isolation forest scoring, moving average deviation, percentile outliers, and multivariate scoring. Threats are classified into severity levels (None → Low → Medium → High → Critical).

\textbf{Mahalanobis Distance:}
\begin{equation}
	D_M(\mathbf{s}_t) = \sqrt{{(\mathbf{s}_t - \boldsymbol{\mu})}^T \boldsymbol{\Sigma}^{-1} (\mathbf{s}_t - \boldsymbol{\mu})}
\end{equation}
where $\boldsymbol{\mu}$ is the historical mean state and $\boldsymbol{\Sigma}$ is the covariance matrix.

\textbf{Circuit Breaker Condition:}
\begin{equation}
	\text{Trigger} = \begin{cases}
		1 & \text{if } D_M(\mathbf{s}_t) > \tau_{\text{danger}} \\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
where $\tau_{\text{danger}}$ is calibrated to a false-positive rate (e.g., $\tau = 5$ for $p < 0.001$).

\textbf{Additional Threat Signals:}
\begin{itemize}
	\item Sudden volatility spike: $\sigma_t > 3 \cdot \sigma_{\text{baseline}}$
	\item Drawdown threshold: cumulative loss $> L_{\max}$
	\item Liquidity crisis: bid-ask spread $> 10 \times$ normal
	\item Regime shift detection
	\item Correlation breakdown across monitored pairs
	\item Black swan event detection
	\item Flash crash detection via VPIN
\end{itemize}

\textbf{VPIN Flow Toxicity:} The VPIN calculator \citep{easley2011vpin, easley2012flow} uses volume bucket aggregation, bulk volume classification, and rolling window computation to produce a toxicity score. High and critical thresholds trigger graduated responses from position reduction to full kill switch activation.

\textbf{Production Circuit Breakers:}
\begin{itemize}
	\item \textbf{Kill Switch:} Dual-layer design with in-process \texttt{AtomicBool} for zero-latency local halt and Redis-backed distributed coordination across services
	\item \textbf{Position Freeze:} Prevents new position entry while allowing exits
	\item \textbf{Safe Mode:} Reduces system to minimal-risk operation
	\item \textbf{Cancel All:} Emergency cancellation of all pending orders
\end{itemize}

\textbf{Fear Extinction:} The fear learning system implements extinction mechanisms \citep{monfils2009extinction, caligiore2019superlearning}, allowing the system to ``unlearn'' fear when threats have passed. This prevents the agent from becoming permanently paralyzed by a single traumatic market event (e.g., flash crash) while maintaining protective circuit breakers for genuine systemic risks.

\subsection{Cerebellum: Motor Control \& Execution}

The cerebellum provides forward models for motor prediction, adapted here for market impact forecasting \citep{almgren2001optimal}.

\subsubsection{Trading Implementation}

\textbf{Component:} Forward Model for Market Impact with Error Correction

\begin{itemize}
	\item \textbf{Almgren-Chriss Model:} Full optimal execution with permanent/temporary impact coefficients, risk aversion parameter, and optimal trajectory calculation
	\item \textbf{VPIN Integration:} Volume-Synchronized Probability of Informed Trading for flow toxicity detection
	\item \textbf{Forward Models:} Adverse selection detection, Smith predictor for latency compensation, order latency estimation, and fill probability prediction
	\item \textbf{Error Correction:} PID controller, feedback loops, and adaptive correction for continuous execution quality improvement
	\item \textbf{LOB Simulator} \citep{fu2024jaxlob}: Full limit order book simulator with price-time priority matching engine, support for Limit, Market, IOC, FOK, Post-Only, Iceberg, Stop, and Stop-Limit order types, self-trade prevention, maker/taker fee computation, L2/L3 snapshots, VWAP and weighted mid-price calculations, queue position estimation, configurable tick/lot sizing, and Almgren-Chriss market impact modeling — enabling parallel synthetic training data generation for reinforcement learning
\end{itemize}

Price movement from order execution:
\begin{equation}
	\Delta p = f_{\text{cerebellum}}(\text{order\_size}, \text{liquidity}, \text{volatility})
\end{equation}

\subsection{Integration: Inter-Region Coordination}

The Integration module provides the ``white matter'' connecting brain regions, handling service bridges, cross-region coordination, and the data pipeline that routes information between the Forward, Backward, Execution, Data, and CNS services.

% ============================================
% PART 5: RUST IMPLEMENTATION
% ============================================
\newpage
\part{Rust Implementation}
\label{part:rust}
\setcounter{section}{0}

\section*{Abstract}

This document provides production-ready Rust implementation specifications for Project JANUS\@. The choice of Rust is strategic, prioritizing memory safety and concurrency \citep{matsakis2014rust, jung2018rustbelt}, with zero-cost abstractions essential for nanosecond-critical high-frequency trading environments. This section includes:

\begin{itemize}
	\item \textbf{ML Framework Strategy} leveraging Candle \citep{candle2024} for end-to-end Rust-native ML
	\item \textbf{High-Performance Services} with async Tokio runtime \citep{tokio2024}
	\item \textbf{Rust-Native Training \& Inference Pipeline} — full ML lifecycle in Rust
	\item \textbf{Neuromorphic Module} — complete brain-region implementations in pure Rust
	\item \textbf{Trading Strategies} — nine regime-aware strategies with gating and affinity
	\item \textbf{Deployment Architecture} (Docker Compose + Kubernetes)
\end{itemize}

\section{Architectural Overview}

\subsection{The Rust-Only Philosophy}

The exclusive use of Rust across the entire stack---training, inference, and production services---is justified by the language's formal safety guarantees \citep{matsakis2014rust, jung2018rustbelt} and the performance requirements of high-frequency trading systems:

\begin{enumerate}
	\item \textbf{Performance:} Zero-cost abstractions, no GC pauses—critical for sub-microsecond latency
	\item \textbf{Safety:} Memory safety without runtime overhead, preventing undefined behavior
	\item \textbf{Concurrency:} Fearless async/await with Tokio \citep{tokio2024} for handling high-frequency websocket feeds
	\item \textbf{Ecosystem:} End-to-end ML training and inference via Candle \citep{candle2024}
	\item \textbf{Unified Stack:} Single language for the entire pipeline eliminates cross-language serialization overhead, deployment complexity, and FFI boundary risks
\end{enumerate}

\subsection{Component Diagram}

\begin{tcolorbox}[colback=codegray, colframe=rustcolor, title=System Architecture (Pure Rust)]
	\begin{verbatim}
+-------------------------------------------------+
|           Rust Training Service                 |
|       (Candle + DiffGAF + LTN + DQN)            |
+------------+------------------------------------+
             | Native Model Artifacts
             v
+-------------------------------------------------+
|           Rust Forward Service                  |
|    (Tokio + Neuromorphic Brain + Strategies)     |
|  gRPC: 50051 | HTTP: 7000                       |
+------------+------------------------------------+
             |
    +--------+----------------+
    v        v                v
+----------+ +--------------+ +------------------+
|Execution | |   Backward   | |   CNS Service    |
| Service  | |   Service    | | (Health Monitor) |
|HTTP:8081 | |  (internal)  | |   (internal)     |
|gRPC:50052| |              | |                  |
+----------+ +------+-------+ +------------------+
                    |
    +---------------+--------------+
    v               v              v
+----------+ +----------+ +--------------+
|PostgreSQL| |  Redis   | |  Data Service|
|  :5432   | |  :6379   | |  (internal)  |
+----------+ +----------+ +--------------+
\end{verbatim}
\end{tcolorbox}

\section{Machine Learning Framework Strategy}

\subsection{Framework Stack}

The following Rust-native frameworks comprise the JANUS ML stack \citep{candle2024}:

\begin{table}[htbp]
	\centering
	\begin{tabularx}{\textwidth}{|l|L|L|L|}
		\hline
		\textbf{Framework}        & \textbf{Pros}                                              & \textbf{Cons}     & \textbf{Use Case}                                          \\
		\hline
		Candle \citep{candle2024} & Pure Rust, HuggingFace integration, minimal deps, autograd & Younger ecosystem & Primary training \& inference (DiffGAF, ViViT, LTN, DQN)   \\
		\hline
		ndarray                   & Zero-dependency numerical arrays, mature ecosystem         & No autograd       & Traditional fuzzy logic, GAF encoding, feature engineering \\
		\hline
		Polars \citep{polars2024} & High-speed DataFrames                                      & N/A               & Data manipulation                                          \\
		\hline
	\end{tabularx}
\end{table}

\subsection{Rust-Native ML Architecture}

The system operates as a \textbf{fully Rust-native} ML pipeline with no external language dependencies:

\begin{itemize}
	\item End-to-end training and inference in Rust via Candle with autograd support
	\item Custom differentiable kernels for DiffGAF and LTN operations
	\item Double DQN with online/target networks for reinforcement learning
	\item Native model serialization using \texttt{safetensors} format
	\item GPU acceleration via wgpu (Candle) and CUDA (optional)
	\item Zero cross-language overhead—no FFI bridges, no serialization boundaries
	\item Training infrastructure: AdamW/SGD optimizers, warmup+cosine LR scheduling, prioritized replay buffers
\end{itemize}

\section{Forward Service: Rust Implementation}

\subsection{Performance Requirements}

FPGA acceleration using AMD Alveo U55C cards \citep{amd2023alveo, vemeko2023fpga, marino2023mevit} is planned as future work. Current targets:

\begin{itemize}
	\item Latency: p99 < 10ms (target: < 1μs with FPGA)
	\item Throughput: 10,000 req/s
	\item Memory: < 2GB RSS
\end{itemize}

\subsection{Core Data Structures}

The system maintains several key data structures for real-time processing:

\textbf{Market State Representation:}
\begin{equation}
	\mathcal{S}_t = (\tau_t, \mathbf{f}_t, \mathcal{O}_t, \mathbf{c}_t)
\end{equation}
where:
\begin{itemize}
	\item $\tau_t \in \mathbb{Z}^+$ is the timestamp
	\item $\mathbf{f}_t \in \mathbb{R}^d$ is the feature vector
	\item $\mathcal{O}_t = (\mathcal{B}_t, \mathcal{A}_t)$ is the order book with bids $\mathcal{B}_t$ and asks $\mathcal{A}_t$
	\item $\mathbf{c}_t$ contains contextual metadata (volatility, spreads, volume)
\end{itemize}

\textbf{Order Book Structure:}
\begin{align}
	\mathcal{B}_t & = \{(p_i, q_i) : p_i \in \mathbb{R}^+, q_i \in \mathbb{R}^+ \}_{i=1}^{N_{\text{bid}}} \\ % chktex 3
	\mathcal{A}_t & = \{(p_j, q_j) : p_j \in \mathbb{R}^+, q_j \in \mathbb{R}^+ \}_{j=1}^{N_{\text{ask}}} % chktex 3
\end{align}

\subsection{GAF Transformation Algorithm}

The GAF transformation converts time series to 2D images via the following algorithm \citep{wang2015imaging}. For training data generation, JANUS employs a Rust-native GPU-accelerated limit order book simulator, inspired by JAX-LOB \citep{fu2024jaxlob}, that enables parallel simulation of thousands of order books, solving the data scarcity problem inherent in traditional trading systems.

\begin{algorithm}[htbp]
	\caption{GAF Computation}
	\label{alg:gaf}
	\begin{algorithmic}[1]
		\State \textbf{Input:} Time series $X = \{x_1, \ldots, x_W\}$, window size $W$
		\State \textbf{Output:} Gramian matrix $\mathbf{G} \in \mathbb{R}^{W \times W}$
		\State
		\State $\tilde{X} \leftarrow \text{Normalize}(X)$ to $[-1, 1]$
		\State $\phi_i \leftarrow \arccos(\tilde{x}_i)$ for $i = 1, \ldots, W$
		\For{$i = 1$ to $W$}
		\For{$j = 1$ to $W$}
		\State $\mathbf{G}_{ij} \leftarrow \cos(\phi_i + \phi_j)$
		\EndFor
		\EndFor
		\State \Return $\mathbf{G}$ reshaped to $[1, W, W]$ tensor
	\end{algorithmic}
\end{algorithm}

\textbf{Computational Complexity:} $\mathcal{O}(W^2)$ for matrix construction, where $W$ is the window size.

\subsection{LTN Constraint Evaluation}

Each constraint is represented as a weighted predicate function:

\textbf{Constraint Structure:}
\begin{equation}
	\mathcal{C}_k = (P_k, w_k)
\end{equation}
where $P_k: \mathcal{S} \to [0,1]$ is a predicate and $w_k \in \mathbb{R}^+$ is the weight.

\textbf{Evaluation Function:}
\begin{equation}
	\text{Eval}(\mathcal{C}_k, \mathcal{S}_t) = w_k \cdot P_k(\mathcal{S}_t)
\end{equation}

\textbf{T-norm Operations (already defined in Part 2):}
\begin{align}
	a \land_{\mathcal{L}} b       & = \max(0, a + b - 1) \quad \text{(Conjunction)} \\
	a \Rightarrow_{\mathcal{L}} b & = \min(1, 1 - a + b) \quad \text{(Implication)}
\end{align}

\textbf{Total Constraint Satisfaction:}
\begin{equation}
	\mathcal{L}_{\text{constraint}} = 1 - \frac{1}{K} \sum_{k=1}^{K} \text{Eval}(\mathcal{C}_k, \mathcal{S}_t)
\end{equation}

\subsection{Async Service Architecture}

The service follows an event-driven architecture with the following characteristics:

\textbf{Request Processing Pipeline:}
\begin{enumerate}
	\item \textbf{Initialization:} Load native Rust model $\mathcal{M}_{\text{ViViT}}$ and LTN engine $\mathcal{E}_{\text{LTN}}$
	\item \textbf{Connection Handling:} Bind gRPC listener on port 50051 and HTTP on port 7000
	\item \textbf{Concurrent Processing:} For each incoming request:
	      \begin{itemize}
		      \item Spawn asynchronous task with model clone
		      \item Process request independently (non-blocking)
		      \item Return prediction and constraint satisfaction scores
	      \end{itemize}
\end{enumerate}

\textbf{Concurrency Model:}
\begin{equation}
	\text{Throughput} = \frac{N_{\text{workers}} \times 1000}{T_{\text{avg}}}
\end{equation}
where $N_{\text{workers}}$ is the thread pool size and $T_{\text{avg}}$ is average processing time in ms.

\textbf{Performance Characteristics:}
Non-blocking I/O via async/await, zero-copy model sharing across tasks, and bounded memory through connection limiting.

\section{Backward Service: Batch Processing}

\subsection{Prioritized Experience Replay}

The replay buffer maintains experiences with importance-based sampling.

\textbf{Buffer State:}
\begin{equation}
	\mathcal{B} = \{(e_i, p_i)\}_{i=1}^{N} % chktex 3
\end{equation}
where $e_i$ is an experience and $p_i \in \mathbb{R}^+$ is its priority.

\textbf{Hyperparameters:}
\begin{itemize}
	\item $\alpha \in [0,1]$: Priority exponent (0 = uniform, 1 = full prioritization)
	\item $\beta \in [0,1]$: Importance sampling correction
	\item $C$: Buffer capacity
\end{itemize}

\begin{algorithm}[htbp]
	\caption{Prioritized Experience Sampling}
	\label{alg:sampling}
	\begin{algorithmic}[1]
		\State \textbf{Input:} Buffer $\mathcal{B}$, batch size $B$
		\State \textbf{Output:} Sampled batch $\{e_{i_1}, \ldots, e_{i_B}\}$
		\State
		\State Compute probabilities: $P(i) = \frac{p_i^\alpha}{\sum_j p_j^\alpha}$
		\For{$k = 1$ to $B$}
		\State Sample index $i_k \sim \text{Categorical}(P)$
		\State Add $e_{i_k}$ to batch
		\EndFor
		\State \Return batch
	\end{algorithmic}
\end{algorithm}

\textbf{Importance Weights:}
\begin{equation}
	w_i = \left(\frac{1}{N \cdot P{(i)}}\right)^\beta % chktex 3
\end{equation}
These weights correct for the non-uniform sampling distribution.

\subsection{Schema Consolidation Algorithm}

Schemas are formed by clustering experience embeddings and storing centroids.

\begin{algorithm}[htbp]
	\caption{Schema Update}
	\label{alg:schema_update}
	\begin{algorithmic}[1]
		\State \textbf{Input:} Experiences $\mathcal{E} = \{e_1, \ldots, e_N\}$, number of clusters $K$
		\State \textbf{Output:} Updated schema database
		\State
		\State Extract embeddings: $\mathbf{h}_i = \text{Embed}(e_i)$ for $i = 1, \ldots, N$
		\State Cluster: $\mathcal{C} \leftarrow \text{K-means}(\{\mathbf{h}_i\}, K)$
		\For{$k = 1$ to $K$}
		\State Compute centroid: $\mathbf{z}_k = \frac{1}{|C_k|} \sum_{i \in C_k} \mathbf{h}_i$
		\State Compute statistics:
		\State \quad $n_k = |C_k|$
		\State \quad $\bar{r}_k = \frac{1}{|C_k|} \sum_{i \in C_k} r_i$ (average reward)
		\State \textbf{Upsert} schema $k$ with vector $\mathbf{z}_k$ and metadata $(n_k, \bar{r}_k)$
		\EndFor
	\end{algorithmic}
\end{algorithm}

\textbf{Note:} In production, the primary schema classification uses deterministic feature-range matching for interpretability. The K-means clustering algorithm above is used for schema \emph{formation} from accumulated experiences during deep consolidation phases.

\textbf{Schema Metadata:}
Each schema $k$ stores:
\begin{itemize}
	\item Centroid vector $\mathbf{z}_k \in \mathbb{R}^d$
	\item Member count $n_k$
	\item Average reward $\bar{r}_k$
	\item Volatility $\sigma_k$ (standard deviation of returns)
\end{itemize}

\textbf{K-means Objective:}
\begin{equation}
	\min_{\mathcal{C}} \sum_{k=1}^{K} \sum_{i \in C_k} \|\mathbf{h}_i - \mathbf{z}_k\|^2
\end{equation}

\section{Execution Service}
\label{sec:execution}

The Execution Service is a dedicated microservice responsible for multi-exchange order routing, providing a clean separation between decision-making (Forward~Service) and order~management.

\subsection{Multi-Exchange Support}

The service supports multiple cryptocurrency exchanges through a unified adapter interface (\texttt{exchanges} crate), with exchange-specific adapters handling authentication, rate limiting, and order format translation:

\begin{itemize}
	\item \textbf{Kraken:} Full REST and WebSocket adapter with order normalization (primary exchange)
	\item \textbf{Bybit:} Dedicated client crate (\texttt{bybit-client}) with unified order types
	\item \textbf{Coinbase:} REST adapter with authentication and rate limiting
	\item \textbf{OKX:} REST adapter with order format translation
	\item \textbf{Binance:} Legacy connector maintained for backward compatibility
	\item \textbf{Kucoin:} Legacy connector maintained for backward compatibility
\end{itemize}

Each adapter implements a common \texttt{ExchangeAdapter} trait, enabling transparent exchange selection at runtime.
The \texttt{normalizer} module translates exchange-specific order responses into canonical JANUS~types.

\subsection{Service Interface}

\begin{itemize}
	\item HTTP API on port 8081 for order management
	\item gRPC on port 50052 for low-latency inter-service communication
	\item Standardized order lifecycle (place, modify, cancel, query)
	\item Circuit breaker per exchange with configurable failure thresholds and recovery timeouts
	\item Rate limiter with token bucket and sliding window algorithms
\end{itemize}

\subsection{Execution Algorithms}

The system provides production execution algorithms:

\begin{itemize}
	\item \textbf{TWAP:} Time-Weighted Average Price execution with configurable slice intervals
	\item \textbf{VWAP:} Volume-Weighted Average Price execution with volume profile estimation
	\item \textbf{Iceberg:} Hidden large orders by exposing small visible tip orders to minimize market impact
	\item \textbf{Execution Analytics:} Per-venue latency tracking, fill rate monitoring, and slippage analysis
\end{itemize}

\section{CNS Service: System Health Monitoring}
\label{sec:cns}

The Central Nervous System (CNS) Service provides system-wide health monitoring and operational safety, analogous to the autonomic nervous system's regulation of vital~functions.

\subsection{Preflight Validation}

Before entering live or paper trading, the CNS Service executes a five-phase preflight sequence, each with configurable criticality levels (Critical, Required, Optional):

\begin{enumerate}
	\item \textbf{Infrastructure Phase:} Database connectivity, Redis availability, Qdrant health, shared memory paths
	\item \textbf{Sensory Phase:} Exchange API authentication, WebSocket feed health, market data freshness
	\item \textbf{Regulatory Phase:} Kill switch state verification, compliance rule loading, wash sale detector initialization
	\item \textbf{Strategy Phase:} Model file availability, version consistency, strategy affinity configuration
	\item \textbf{Executive Phase:} Memory and CPU resource adequacy, neuromorphic brain region initialization
\end{enumerate}

Critical failures abort the boot sequence; required failures block trading but allow monitoring; optional failures are logged but permit full operation. The \texttt{PreFlightRunner} supports both sequential and parallel-within-phase execution modes, producing a comprehensive \texttt{BootReport} with Discord-formatted notifications.

\subsection{Runtime Monitoring}

During operation, the CNS Service provides:
\begin{itemize}
	\item \textbf{Watchdog Monitoring:} Heartbeat-based component liveness detection with configurable degraded/dead thresholds, per-component criticality levels (Critical, Important, NonEssential), and automatic kill switch triggering when critical components die
	\item \textbf{Boot Reports:} Comprehensive system state summary with pass/fail/skip counts per phase
	\item \textbf{Prometheus Metrics:} 15+ custom metrics exposed for Grafana dashboards (6+ pre-built dashboards)
	\item \textbf{Alert Integration:} Slack, PagerDuty, and generic webhook notifications with severity-based routing
	\item \textbf{Distributed Tracing:} Jaeger integration for cross-service latency analysis
	\item \textbf{Circuit Breakers:} Per-component circuit breakers with Closed → Open → HalfOpen state machine, configurable failure windows, and recovery timeouts
	\item \textbf{Reflex Actions:} Automated responses including component restart, throttling, graceful shutdown, and safe command execution with allowlist validation
	\item \textbf{Neuromorphic Brain Coordinator:} Topological initialization ordering of all 10 brain regions based on dependency graphs, per-region health scoring, and global brain activation/deactivation
\end{itemize}

\section{Trading Strategies}
\label{sec:strategies}

The \texttt{strategies} crate implements nine regime-aware trading strategies, each designed for specific market conditions as identified by the ensemble regime detector:

\subsection{Trend-Following Strategies}

\begin{itemize}
	\item \textbf{EMA Flip:} 8/21 EMA crossover with ATR-based stops, trading pullbacks to the fast EMA in the trend direction
	\item \textbf{EMA Ribbon Scalper:} 8/13/21 EMA ribbon with volume confirmation for higher-quality pullback entries
	\item \textbf{Trend Pullback:} Fibonacci retracement entries within established trends, confirmed by RSI divergence and candlestick patterns (pin bars, engulfing)
	\item \textbf{Momentum Surge:} Detects sudden price surges with volume spikes, entering on the first pullback within the surge
	\item \textbf{Multi-Timeframe Trend:} EMA 50/200 crossover with ADX strength and higher-timeframe alignment
\end{itemize}

\subsection{Mean-Reversion Strategies}

\begin{itemize}
	\item \textbf{Mean Reversion:} Bollinger Bands with RSI confirmation and ATR-based stops
	\item \textbf{Bollinger Squeeze Breakout:} Detects low-volatility squeeze periods and generates breakout signals when price escapes the bands
	\item \textbf{VWAP Scalper:} Mean reversion scalping around the Volume-Weighted Average Price with standard deviation bands
	\item \textbf{Opening Range Breakout:} Trades breakouts above or below the first N candles of a session with volume confirmation
\end{itemize}

\subsection{Strategy Gating and Affinity}

The \texttt{StrategyGate} module controls which strategies execute based on:

\begin{itemize}
	\item \textbf{Regime Compatibility:} Each asset can define preferred strategies per regime (e.g., Trending → EMA Flip, MeanReverting → Bollinger Squeeze)
	\item \textbf{Affinity Scoring:} Strategy-asset affinity tracker with performance-weighted scoring
	\item \textbf{Allowlists/Denylists:} Per-asset strategy filtering via TOML configuration
	\item \textbf{Untested Strategy Policy:} Configurable flag to allow or block strategies without historical performance data
\end{itemize}

\section{Neuromorphic Module: Brain-Region Implementations}
\label{sec:neuromorphic_impl}

The \texttt{neuromorphic} crate is the largest module in the JANUS codebase, implementing all ten brain regions as production Rust code. Each region is a self-contained submodule with its own configuration, state management, and comprehensive test suite.

\begin{tcolorbox}[colback=codegray, colframe=neurogreen, title=Neuromorphic Crate Structure]
	\begin{verbatim}
neuromorphic/
+-- visual_cortex/    # GAF, ViViT, preprocessing, UMAP viz
+-- cortex/           # Schemas, knowledge base, planning
|   +-- memory/       # 7 regime schemas + Markov transitions
+-- hippocampus/      # Episodic buffer, SWR, consolidation
|   +-- swr/          # Ripple detection, compressed replay
+-- thalamus/         # Attention, fusion, gating, routing
|   +-- fusion/       # Orderbook, price, volume, sentiment
+-- hypothalamus/     # Position sizing, homeostasis, energy
|   +-- position_sizing/  # Drawdown scaling, Kelly criterion
+-- basal_ganglia/    # Direct/Indirect pathways, OpAL
|   +-- praxeological/  # Go/No-Go signals, confidence
+-- amygdala/         # Threat detection, VPIN, kill switch
|   +-- vpin/         # Calculator, flash crash, toxicity
+-- cerebellum/       # Forward models, error correction
|   +-- error_correction/  # PID controller, feedback loops
|   +-- forward_models/    # Smith predictor, fill prob
+-- prefrontal/       # LTN, fuzzy logic, conscience, goals
|   +-- ltn/          # Hedges: very, somewhat, extremely
+-- integration/      # Coordinator, message bus, bridges
|   +-- engine/       # Cognitive core, orchestrator
+-- distributed/      # Multi-node coordination
\end{verbatim}
\end{tcolorbox}

\subsection{Key Implementation Details}

\begin{sloppypar}

	\textbf{Basal Ganglia --- Opponent Actor Learning (OpAL):}
	The \texttt{praxeological} module implements the full Go/No-Go architecture with dopamine-modulated action selection.
	The \texttt{GoSignal} evaluates action value against adaptive thresholds with dopamine sensitivity, urgency boosting, and facilitation bias.
	The \texttt{NoGoSignal} evaluates 12 inhibition reasons (risk threshold, position limit, loss limit, high volatility, low liquidity, cooling off, correlation risk, drawdown protection, time restriction, external halt, learned pattern, and custom) with learned inhibition patterns and adaptive thresholds.
	The actor-critic framework includes Generalized Advantage Estimation (GAE) and TD($\lambda$) learning for stable policy updates, with winner-take-all selection and habit caching for frequently-encountered states.

	\textbf{Cerebellum --- Forward Models and Error Correction:}
	The \texttt{almgren\_chriss} module implements optimal execution trajectories with permanent/temporary impact coefficients.
	The \texttt{pid\_controller} provides a full PID implementation with anti-windup, dead band, derivative filtering, cascaded PID, and Ziegler-Nichols auto-tuning.
	The \texttt{forward\_models} submodule includes adverse selection detection, Smith predictor for latency compensation, order latency estimation, and fill probability prediction.

	\textbf{Amygdala --- VPIN and Kill Switch:}
	The \texttt{vpin} module implements volume-synchronized probability of informed trading with bulk volume classification, rolling bucket computation, and configurable high/critical thresholds.
	The \texttt{kill\_switch} provides a four-scope design (per-strategy, per-instrument, per-service, global) with emergency actions (cancel all orders, close all positions, disable trading, send alerts).
	The fear network integrates reinforcement learning (FNI-RL) to adapt threat responses based on historical outcomes.

	\textbf{Cortex --- Schema Formation:}
	The \texttt{schemas} module implements all seven regime schemas (Bull, Bear, Ranging, Crisis, Recovery, Bubble, Deflation) with 10 market features (trailing return, realised volatility, average correlation, max drawdown, vol rate of change, momentum signal, mean reversion signal, credit spread, yield curve slope, relative volume), weighted feature-range matching, a full Markov transition matrix with stationary distribution computation, and EMA-smoothed confidence tracking.
	The cortex also implements a hierarchical RL manager with feudal goal-setting and subgoal generation for multi-level strategic planning.

	\textbf{Prefrontal --- Fuzzy Logic with Linguistic Hedges:}
	The \texttt{ltn} submodule implements the complete Łukasiewicz fuzzy logic system with five linguistic hedges: \texttt{very}~($x^2$, concentration), \texttt{somewhat}~($\sqrt{x}$, dilation), \texttt{slightly}~($\sqrt{x} - x$), \texttt{extremely}~($x^3$), and \texttt{more\_or\_less}~($\sqrt{x}$, synonym for dilation).
	A full expression evaluator supports nested hedge application.

	\textbf{Thalamus --- Multimodal Fusion:}
	Four specialized fusion engines process distinct data streams:
	\texttt{orderbook\_fusion} (multi-venue book consolidation with weighted mid-price, imbalance detection, and EMA smoothing),
	\texttt{price\_fusion} (multi-timeframe price action),
	\texttt{volume\_fusion} (volume profile analysis), and
	\texttt{sentiment\_fusion} (news and social sentiment signals).
	The \texttt{gating} submodule implements Wilson-Cowan oscillatory dynamics \citep{wilson1972excitatory, wilson2024bidirectional} with Hilbert-transform-based amplitude/phase estimation, bifurcation analysis, and neuromodulator-driven regime switching, alongside sensory gates with relevance scoring and threshold-based filtering.
	The \texttt{sources} submodule integrates Chronos time series forecasting \citep{ansari2024chronos, ansari2024chronos2} via ONNX inference with quantile tokenization and confidence intervals, BERT/FinBERT sentiment embeddings \citep{candle2024} running natively via Candle with Qdrant-backed storage, plus news feeds, weather data, and celestial/space weather for commodity correlation analysis.

	\textbf{Hippocampus --- Sharp-Wave Ripple Simulation:}
	The \texttt{swr} submodule implements 11 ripple types (large profit, large loss, novel pattern, market anomaly, strategy breakthrough, risk event, regime change, correlation breakdown, volatility spike, liquidity event, periodic) with priority-weighted detection, inspired by biological SWR replay \citep{wilson1994replay, buzsaki2015hippocampal}.
	The \texttt{compressed\_replay} module provides compressed experience sequences, and \texttt{consolidation\_sync} coordinates the transfer from hippocampal to neocortical storage, implementing CLS theory \citep{kumaran2016cls}.
	The hippocampus also houses a feudal RL worker agent with a skill library and tactical policy for executing subgoals issued by the cortex's strategic planner.
	A regime-conditional DDPM diffusion model \citep{coletta2024conditional} generates synthetic market data for training data augmentation, with configurable noise schedules (linear, cosine, quadratic), Min-SNR loss weighting, and automated quality assessment comparing synthetic vs.\ real feature distributions.

\end{sloppypar}

\textbf{Distributed Training Infrastructure:}
The \texttt{distributed} submodule provides multi-GPU and multi-node training coordination with AllReduce, Parameter Server, and Ring AllReduce gradient synchronization strategies. Distributed data loading supports multiple sharding strategies (Contiguous, RoundRobin, Random, Stratified). The infrastructure includes NCCL GPU-to-GPU communication, gRPC-based inter-node coordination, and distributed checkpointing with cloud storage backends.

\textbf{GPU Compute Infrastructure:}
\begin{sloppypar}

	\textbf{GPU Compute Infrastructure:}
	The \texttt{gpu} submodule provides custom wgpu compute kernels for hardware-accelerated numerical operations: MatMul, Softmax, LayerNorm, Attention, GELU, Reduce, Embedding Lookup, and Pairwise Distance.
	The module includes a device manager, buffer pool, kernel registry, and a Candle tensor bridge for CPU\allowbreak{}↔\allowbreak{}GPU data transfer.
	All GPU functionality is gated behind a \texttt{gpu} feature flag, with graceful CPU fallback when unavailable.

	\textbf{Prefrontal --- Quantum-Inspired Portfolio Optimization:}
	The \texttt{planning} submodule includes quantum-inspired portfolio optimizers \citep{mugel2022dynamic}:
	a QAOA (Quantum Approximate Optimization Algorithm) simulator for combinatorial asset selection via QUBO formulation,
	a VQE (Variational Quantum Eigensolver) portfolio optimizer for continuous weight optimization using parameterized circuits,
	and simulated quantum annealing for escaping local minima in non-convex portfolio landscapes.
	These complement classical Mean-Variance, Risk Parity, and Black-Litterman optimizers.

	\textbf{Cerebellum --- LOB Simulator:}
	The \texttt{lob\_simulator} module provides a full limit order book simulator \citep{fu2024jaxlob} with price-time priority matching, support for Limit, Market, IOC, FOK, Post-Only, Iceberg, Stop, and Stop-Limit order types, self-trade prevention, maker/taker fee computation, L2/L3 snapshots, VWAP and weighted mid-price calculations, Almgren-Chriss market impact estimation, order event history, and configurable tick/lot sizing.
	The simulator enables parallel training data generation, solving the data scarcity problem for reinforcement~learning.

\end{sloppypar}

\subsection{Supporting Crates}

The modular Rust workspace includes dedicated crates:

\begin{itemize}[leftmargin=*]
	\begin{sloppypar}
		\item \textbf{training:} End-to-end training loop with AdamW/SGD optimizers, learning rate schedulers (warmup+cosine, step, exponential), prioritized experience replay with SWR sampling, gradient clipping, checkpointing with metadata, early stopping, and pluggable callbacks
		\item \textbf{ml:} LSTM and MLP models via Candle, Double DQN with online/target networks and soft updates, feature engineering (price, volume, technical indicators, normalizers), and dataset management
		\item \textbf{logic:} Dual-mode LTN system---non-differentiable (ndarray) for inference-time constraint checking and differentiable (Candle) for gradient-based training---with three t-norm families (Łukasiewicz, Product, G\"odel), learnable/\allowbreak{}threshold/\allowbreak{}similarity predicates, and comprehensive rule composition (ForAll, Exists, Implies, Iff, AndN, OrN)
		\item \textbf{ltn:} Domain-specific neuro-symbolic engine with 10 market axioms encoded as logical rules (e.g., Trending~+ Positive Divergence~$\rightarrow$~Long, Low Confidence~$\rightarrow$~Neutral), integrating DSP features through fuzzy predicates with a hybrid supervised-semantic loss function
		\item \textbf{regime:} Ensemble regime detection combining HMM (Gaussian emissions, Baum-Welch, online parameter updates), indicator-based (ADX, Bollinger Bands, ATR, EMA, RSI), and ensemble fusion with agreement tracking and strategy routing
		\item \textbf{dsp:} Digital signal processing with FRAMA (Fractal Adaptive Moving Average) and Sevcik fractal dimension estimation producing an 8D feature vector (divergence, alpha, fractal dimension, Hurst exponent, regime, sign, deviation, confidence) at sub-microsecond latency per tick
		\item \textbf{lob:} Standalone limit order book crate with matching engine, fill probability models, L2 replay, latency simulation, and Almgren-Chriss market impact estimation
		\item \textbf{compliance:} Dedicated \texttt{WashSaleDetector} with full 30-day lookback/\allowbreak{}lookforward window, partial wash sale handling, cost basis tracking, and position-aware detection; plus \texttt{ComplianceSheriff} enforcing proprietary firm trading rules (daily loss limits, maximum loss thresholds, mandatory stop-losses)
		\item \textbf{optimizer:} Hyperparameter optimization with configurable samplers, constraints, backtesting integration, and result publishing
		\item \textbf{data-quality:} Market data validation and anomaly detection
		\item \textbf{gap-detection:} Multi-layer time series gap detection (sequence ID, heartbeat, statistical, volume-aware) with PostgreSQL persistence
		\item \textbf{indicators:} Technical indicator library (ADX, ATR, Bollinger Bands, EMA, RSI, MACD) with incremental O(1) per-tick computation
		\item \textbf{rate-limiter:} Token bucket and sliding window rate limiters with async circuit breaker, exchange-specific algorithms
		\item \textbf{questdb-writer:} High-performance time series ingestion to QuestDB via ILP, batched writes targeting {>}100K inserts/sec
		\item \textbf{memory:} Three-tier memory hierarchy with production Qdrant vector database client (circuit breaker, exponential backoff retries, TLS, mock fallback), predefined collections for market regimes, episodic memory, sentiment embeddings, and schema prototypes, plus prioritized experience replay buffer
	\end{sloppypar}
\end{itemize}

\section{Deployment Architecture}

\subsection{Service Orchestration}

The system deploys as eight services plus supporting infrastructure:

\textbf{Service Topology:}
\begin{enumerate}
	\item \textbf{Forward Service:}
	      \begin{itemize}
		      \item Ports: gRPC 50051, HTTP 7000
		      \item Dependencies: Native model artifacts, Redis, PostgreSQL
		      \item Resource limits: 2GB memory, 2 CPU cores
	      \end{itemize}

	\item \textbf{Backward Service:}
	      \begin{itemize}
		      \item Internal service (no external ports)
		      \item Dependencies: PostgreSQL, Redis
		      \item Scheduling: Triggered during market close
	      \end{itemize}

	\item \textbf{Execution Service:}
	      \begin{itemize}
		      \item Ports: HTTP 8081, gRPC 50052
		      \item Dependencies: Exchange API credentials, Redis (kill switch)
		      \item Multi-exchange order routing
	      \end{itemize}

	\item \textbf{Data Service:}
	      \begin{itemize}
		      \item Internal service for centralized market data management
		      \item Dependencies: Exchange WebSocket feeds, QuestDB (time series)
	      \end{itemize}

	\item \textbf{CNS Service:}
	      \begin{itemize}
		      \item Internal service for health monitoring and preflight validation
		      \item Dependencies: All other services (monitoring target)
	      \end{itemize}

	\item \textbf{API Service:}
	      \begin{itemize}
		      \item HTTP API gateway for external access
		      \item Dependencies: Forward Service, Backward Service
	      \end{itemize}

	\item \textbf{Registry Service:}
	      \begin{itemize}
		      \item Internal service discovery and registration
		      \item Dependencies: Redis
	      \end{itemize}

	\item \textbf{Optimizer Service:}
	      \begin{itemize}
		      \item Hyperparameter optimization with backtesting integration
		      \item Metrics port: 9092
	      \end{itemize}
\end{enumerate}

\textbf{Infrastructure:}
\begin{itemize}
	\item \textbf{PostgreSQL} (port 5432): Primary relational storage for signals, portfolios, and performance
	\item \textbf{Redis} (port 6379): Operational state, kill switch coordination, hot-reloadable config
	\item \textbf{QuestDB} (ports 9000/9009): High-performance time series storage for market data
	\item \textbf{Qdrant}: Vector similarity search engine for schema pattern matching
	\item \textbf{Prometheus} (port 9090): Metrics collection with 1700+ lines of alert rules
	\item \textbf{Grafana} (port 3000): Visualization dashboards (6+ dashboards including strategy, regime, CNS, brain region monitors)
	\item \textbf{Alertmanager} (port 9093): Alert routing with Discord integration
	\item \textbf{Jaeger} (port 16686): Distributed tracing
	\item \textbf{Loki + Promtail}: Centralized log aggregation with label-based organization
	\item \textbf{Authelia}: OpenID Connect / SAML authentication gateway
	\item \textbf{Nginx}: Reverse proxy with SSL termination
\end{itemize}

\textbf{Service Communication:}
\begin{equation}
	\text{Forward} \xrightarrow[\text{gRPC}]{\text{signals}} \text{Execution} \xrightarrow[\text{HTTP}]{\text{orders}} \text{Exchanges}
\end{equation}
\begin{equation}
	\text{Forward} \xrightarrow[\text{SQL}]{\text{experiences}} \text{PostgreSQL} \xrightarrow[\text{nightly}]{\text{batch}} \text{Backward} \xrightarrow[\text{SQL}]{\text{schemas}} \text{PostgreSQL}
\end{equation}

\textbf{Volume Management:}
Model artifacts are stored on shared read-only volumes; PostgreSQL and QuestDB use persistent volumes with automated backups; Redis data is ephemeral with configurable persistence; experience buffers rotate daily.

% ============================================
% CONCLUSION
% ============================================
\newpage
% ============================================
% PART 6: LIMITATIONS AND OPEN PROBLEMS
% ============================================
\newpage
\part{Limitations and Open Problems}
\label{part:limitations}
\setcounter{section}{0}

\section*{Abstract}

No system of JANUS's complexity can be presented without candid engagement with its limitations. This section addresses the challenges that peer reviewers will---and should---raise, and describes mitigation strategies where they exist.

\section{The Brain Metaphor Critique}

Every generation projects its latest technology onto the brain: clocks, telephone switchboards, computers, and now neural networks. Critics may argue that JANUS's brain-region mapping is the latest instance of this pattern. We acknowledge the validity of this concern and offer two defenses:

\begin{enumerate}
	\item \textbf{Functional, not literal:} Following \citet{hassabis2017neuroscience}, JANUS uses neuroscience as \textit{functional inspiration}, not biological simulation. The mappings are justified by computational principles (e.g., reinforcement learning for action selection, supervised error correction for timing) rather than cellular fidelity.
	\item \textbf{Scientifically productive:} The Doya framework \citep{doya1999computations} has generated falsifiable predictions confirmed by neuroimaging. The super-learning hypothesis \citep{caligiore2019superlearning} demonstrates that multi-region integration produces capabilities exceeding single-region models. JANUS's architecture is grounded in this validated framework.
\end{enumerate}

\section{Integration Complexity}

\citet{glasmachers2017limits} demonstrated experimentally that end-to-end training can fail even for small multi-module systems due to non-trivial couplings between components. JANUS integrates ten brain-region modules, creating substantial coupling risk.

\textbf{Mitigations:} (i)~Modular training: each brain region can be trained independently before integration. (ii)~Service boundaries: the Forward and Backward services communicate via gRPC, creating natural decoupling points. (iii)~The brain wiring pipeline (Section~\ref{sec:decision_engine}) defines explicit information flow, preventing uncontrolled coupling.

\textbf{Honest assessment:} Integration testing at scale remains an open challenge. Ablation studies (Part~\ref{part:validation}) are needed to quantify inter-region dependencies.

\section{Non-Stationarity and Regime Shifts}

Financial environments are fundamentally non-stationary. RL algorithms optimized for one regime may fail catastrophically in the next \citep{padakandla2020nonstationary}. JANUS's ensemble regime detector is an important mitigation but is itself a heuristic---novel regimes not represented in training data may not be correctly classified.

\textbf{Mitigations:} (i)~Three-timescale memory allows rapid adaptation at short timescales while preserving long-term knowledge \citep{masset2025multitimescale}. (ii)~Brain-inspired replay \citep{vandeven2020braininspired} helps mitigate catastrophic forgetting. (iii)~The hypothalamic homeostatic regulator provides regime-independent risk controls.

\textbf{Honest assessment:} No existing approach fully solves the stability--plasticity dilemma in non-stationary environments. JANUS's multi-timescale architecture is theoretically motivated but requires empirical validation across diverse market conditions.

\section{Backtest Overfitting Risk}

\citet{bailey2015backtest} established the Combinatorially Symmetric Cross-Validation (CSCV) framework showing that conventional backtesting dramatically overstates expected performance. \citet{lopezdeprado2018reasons} warned that ``ML algorithms will always find a pattern, even if there is none.'' With ten brain regions and numerous hyperparameters, JANUS is particularly susceptible to overfitting.

\textbf{Mitigations:} (i)~Walk-forward validation protocol described in Part~\ref{part:validation}. (ii)~LTN constraints provide domain knowledge that regularizes against spurious patterns. (iii)~The CNS service monitors for performance degradation relative to out-of-sample benchmarks.

\section{Crowding Resistance Is Not Crowding Immunity}

As discussed in Section~\ref{sec:crowding}, the literature supports heterogeneity as a mechanism for crowding \textit{resistance}, not crowding \textit{immunity}. Key limitations:

\begin{itemize}
	\item \citet{khandani2011quants} showed that independently developed strategies converged on similar factors in 2007 despite nominal diversity
	\item Evolutionary dynamics can cause heterogeneous populations to converge over time if selection pressure favors similar strategies
	\item Tail events may produce correlated liquidation regardless of strategy diversity \citep{caccioli2014stability}
\end{itemize}

\textbf{Mitigations:} (i)~Active monitoring of cross-instance correlation. (ii)~Periodic re-randomization of heterogeneity parameters. (iii)~Multi-agent LOB simulation for ongoing validation (Part~\ref{part:validation}).

\section{LTN Scalability}

\citet{wan2024ltn_efficiency} found that LTN workloads exhibit a fundamental hardware utilization challenge: symbolic operations are memory-bounded while neural operations are compute-bounded, creating inefficiencies on standard hardware. As the axiom count grows, LTN inference latency may conflict with hot-path requirements.

\textbf{Mitigations:} JANUS implements a dual-mode LTN system: strict mode (full axiom evaluation) for the cold path and approximate mode (reduced axiom set with cached evaluations) for the hot path.

\section{Adversarial Robustness}

\citet{goldblum2021adversarial} demonstrated that adversarial traders can fool automated ML systems by placing adversarial orders on public exchanges. JANUS's neural components---particularly the visual pattern recognition and sentiment analysis modules---are vulnerable to such attacks.

\textbf{Mitigations:} (i)~The amygdala's multi-method anomaly detection provides some defense against adversarial inputs. (ii)~The FNI-RL fear network can learn to recognize and avoid adversarial patterns. (iii)~LTN constraints provide a logic-based floor that adversarial perturbations cannot violate.

\textbf{Honest assessment:} Adversarial robustness in financial ML is an open research problem. JANUS's defenses are heuristic, not provably robust.

% ============================================
% PART 7: VALIDATION FRAMEWORK
% ============================================
\newpage
\part{Validation Framework and Experimental Design}
\label{part:validation}
\setcounter{section}{0}

\section*{Abstract}

For academic peer review, empirical claims require a rigorous experimental protocol. This section describes the planned validation framework, including multi-agent simulation, walk-forward backtesting, ablation studies, and benchmark comparisons.

\section{Multi-Agent LOB Simulation Protocol}

The primary validation of JANUS's crowding-resistance claim requires multi-agent simulation using the Rust-native GPU-accelerated limit order book simulator \citep{fu2024jaxlob}.

\textbf{Experimental conditions:}
\begin{itemize}
	\item \textbf{Condition A (Homogeneous):} 50--200 identical JANUS instances with shared parameters trading simultaneously
	\item \textbf{Condition B (Heterogeneous):} 50--200 JANUS instances with engineered heterogeneity (Table~\ref{tab:heterogeneity})
	\item \textbf{Control:} Background liquidity provided by zero-intelligence agents \citep{farmer2005predictive}
\end{itemize}

\textbf{Metrics:}
\begin{itemize}
	\item Return kurtosis (tail risk under crowding)
	\item Net signed volume vs.\ price move (co-impact measurement)
	\item Preservation of the square-root impact law \citep{toth2011anomalous}
	\item Cross-instance return correlation
	\item Effective market capacity (Sharpe ratio degradation as $N$ increases)
\end{itemize}

\section{Walk-Forward Backtesting}

Following \citet{bailey2015backtest} and \citet{lopezdeprado2018advances}, the backtesting protocol uses:

\begin{enumerate}
	\item \textbf{Combinatorially Symmetric Cross-Validation (CSCV):} Partition historical data into $S$ subsets, form all $\binom{S}{S/2}$ train/test splits, and compute the probability of backtest overfitting
	\item \textbf{Out-of-sample Sharpe ratio degradation:} Compare in-sample to out-of-sample Sharpe ratios across multiple walk-forward windows
	\item \textbf{Regime-conditional performance:} Disaggregate performance by detected regime to identify regime-specific overfitting
\end{enumerate}

\section{Ablation Studies}

Ablation studies isolate the contribution of each component:

\begin{itemize}
	\item \textbf{Brain region ablation:} Remove each brain region individually and measure performance degradation, directly demonstrating the value of multi-region integration
	\item \textbf{Memory tier ablation:} Compare the three-tier CLS memory against single-tier experience replay
	\item \textbf{LTN ablation:} Remove symbolic constraints and measure constraint violation rates and risk-adjusted returns
	\item \textbf{Heterogeneity ablation:} Compare homogeneous vs.\ heterogeneous multi-agent performance (overlaps with LOB simulation)
	\item \textbf{Regime detection ablation:} Remove the ensemble regime detector and measure adaptation speed to market transitions
\end{itemize}

\section{Benchmark Comparisons}

\begin{itemize}
	\item \textbf{Standard RL baselines:} PPO, SAC, and TD3 on the same market data and reward function
	\item \textbf{Rule-based systems:} Classic momentum, mean-reversion, and factor-based strategies (Quant 1.0/2.0 comparisons)
	\item \textbf{Single-region systems:} Pure basal ganglia RL without cerebellar timing, pure replay without CLS consolidation
	\item \textbf{Existing neuromorphic approaches:} SNN-based portfolio optimization \citep{mohan2025snn_portfolio} where feasible
\end{itemize}

% ============================================
% CONCLUSION
% ============================================
\newpage
\section*{Conclusion}

Project JANUS represents a paradigm shift in quantitative trading: from opaque black boxes to transparent, brain-inspired systems that combine the best of deep learning and symbolic reasoning. Grounded in the Doya--Hassabis framework \citep{doya1999computations, hassabis2017neuroscience} and motivated by the growing threat of strategy crowding \citep{bucci2020coimpact, khandani2011quants}, JANUS is---to our knowledge---the first trading system to map multiple brain regions to distinct functional subsystems within a unified architecture.

\subsection*{Key Innovations}

\begin{enumerate}
	\item \textbf{Neuromorphic Architecture:} Ten functionally motivated brain regions with modular design, distributed training infrastructure, and Wilson-Cowan oscillatory dynamics \citep{doya1999computations, caligiore2019superlearning}
	\item \textbf{Crowding Resistance:} Engineered heterogeneity across deployments reducing co-impact costs and cross-instance correlation \citep{wagner2011diversity, demiguel2021crowding}
	\item \textbf{Neuro-Symbolic Fusion:} Dual-mode LTN system with domain-specific axioms and linguistic hedges bridging neural networks and logical constraints \citep{badreddine2022logic}
	\item \textbf{Multi-Timescale Memory:} Three-tier hierarchy with sleep-phase consolidation mirrors hippocampal--neocortical transfer, implementing CLS theory \citep{kumaran2016cls, masset2025multitimescale}
	\item \textbf{OpAL Decision Engine:} First application of basal ganglia computational models to financial action selection \citep{collins2014opponent, jaskir2023opal}
	\item \textbf{Ensemble Regime Detection:} HMM, statistical, and technical methods fused for robust market state identification with strategy routing
	\item \textbf{Production-Ready Safety:} Four-scope kill switch, multi-method anomaly detection, FNI-RL fear network, and CNS health monitoring with five-phase preflight
	\item \textbf{Pure Rust Stack:} End-to-end training and inference in Rust \citep{matsakis2014rust} with high-performance, safe, and maintainable eight-service architecture
	\item \textbf{Fractal Signal Processing:} Sub-microsecond DSP pipeline producing 8D feature vectors via FRAMA and Sevcik fractal dimension for regime-aware signal generation
	\item \textbf{GPU Compute Infrastructure:} Custom wgpu kernels (MatMul, Softmax, LayerNorm, Attention, GELU) with Candle tensor bridge for hardware-accelerated inference
\end{enumerate}

\subsection*{Future Work}

The following items remain as planned enhancements beyond the current implementation:

\begin{itemize}
	\item Hardware acceleration using FPGAs \citep{marino2023mevit, vemeko2023fpga, amd2023alveo} and neuromorphic chips for nanosecond-level latency in high-frequency trading applications
	\item Execution of the full validation framework described in Part~\ref{part:validation}
	\item Investigation of adversarial robustness guarantees beyond heuristic defenses
\end{itemize}

\subsection*{Implementation Status}

The following components have been implemented since initial specification:

\begin{table}[htbp]
	\centering
	\small
	\begin{tabularx}{\textwidth}{|l|L|}
		\hline
		\textbf{Component} & \textbf{Description}                                                                                                    \\
		\hline
		Flow Toxicity      & VPIN calculator with flash crash detection                                                                              \\
		\hline
		Risk Control       & Full PID controller with Ziegler-Nichols auto-tuning                                                                    \\
		\hline
		Compliance         & Wash sale detection, proprietary firm rule enforcement                                                                  \\
		\hline
		Strategies         & Nine regime-aware strategies with gating and affinity scoring                                                           \\
		\hline
		Safety             & Four-scope kill switch, FNI-RL fear network                                                                             \\
		\hline
		Training           & Multi-GPU/multi-node with AllReduce, Parameter Server, Ring AllReduce                                                   \\
		\hline
		Feudal RL          & Cortex manager and hippocampus worker agents                                                                            \\
		\hline
		LTN                & 10 market axioms with hybrid supervised-semantic loss                                                                   \\
		\hline
		Regime             & Markov transition matrix with stationary distribution                                                                   \\
		\hline
		DSP                & 8D feature vector pipeline with sub-microsecond latency                                                                 \\
		\hline
		Vision             & End-to-end DiffGAF $\rightarrow$ ViViT pipeline with dual-GAF                                                           \\
		\hline
		CLS Memory         & Three-tier hierarchy with sleep-phase consolidation                                                                     \\
		\hline
		Thalamus           & Wilson-Cowan oscillation models \citep{wilson1972excitatory, wilson2024bidirectional} with Hilbert-transform estimation \\
		\hline
		Forecasting        & Chronos \citep{ansari2024chronos, ansari2024chronos2} via ONNX inference                                                \\
		\hline
		NLP                & BERT/FinBERT sentiment via Candle \citep{candle2024} with Qdrant storage                                                \\
		\hline
		Visualization      & Parametric UMAP \citep{mcinnes2018umap} with drift detection                                                            \\
		\hline
		Quantum            & QAOA, VQE, simulated annealing \citep{mugel2022dynamic}                                                                 \\
		\hline
		Synthetic Data     & Regime-conditional DDPM diffusion models \citep{coletta2024conditional}                                                 \\
		\hline
		LOB Simulator      & GPU-accelerated matching engine \citep{fu2024jaxlob} with Almgren-Chriss impact                                         \\
		\hline
		GPU Kernels        & Custom wgpu (MatMul, Softmax, LayerNorm, Attention, GELU, Reduce)                                                       \\
		\hline
		Qdrant Client      & Circuit breaker, exponential backoff, TLS, mock fallback                                                                \\
		\hline
	\end{tabularx}
\end{table}

\vspace{1cm}

\begin{tcolorbox}[colback=janusblue!5, colframe=janusblue, title=\textbf{Repository \& Contact}]
	\centering
	\textbf{GitHub:} \url{https://github.com/nuniesmith/fks}

	\vspace{0.5cm}

	For implementation code, updates, and discussions, visit the repository.

	\vspace{0.5cm}

	\textit{``The god of beginnings and transitions, looking simultaneously to the future and the past.''} % chktex 38
\end{tcolorbox}

\newpage

% ============================================
% BIBLIOGRAPHY
% ============================================

\begin{sloppypar}
	\printbibliography[title={References}]
\end{sloppypar}

\end{document}
