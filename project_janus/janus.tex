\documentclass[12pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{pmboxdraw}
\usepackage{newunicodechar}
\usepackage[english]{babel}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{setspace}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

% --- Define Left-Aligned X Column for Tables ---
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

% --- URL BREAKING ---
\usepackage{xurl}
\usepackage{hyperref}

% --- CONFIGURATION ---
\onehalfspacing
\setlength{\headheight}{15pt}

\definecolor{janusblue}{RGB}{0, 51, 102}
\definecolor{accentgold}{RGB}{204, 153, 51}
\definecolor{warnred}{RGB}{153, 0, 0}
\definecolor{forwardblue}{RGB}{41, 98, 255}
\definecolor{backwardpurple}{RGB}{138, 43, 226}
\definecolor{neurogreen}{RGB}{34, 139, 34}
\definecolor{rustorange}{RGB}{255, 140, 0}
\definecolor{codegray}{RGB}{245, 245, 245}
\definecolor{rustcolor}{RGB}{70, 130, 180}
\definecolor{pythoncolor}{RGB}{55, 118, 171}

% --- UNICODE CHARACTER DECLARATIONS ---
\newunicodechar{▼}{\ensuremath{\blacktriangledown}}
\newunicodechar{→}{\ensuremath{\rightarrow}}
\newunicodechar{←}{\ensuremath{\leftarrow}}
\newunicodechar{↔}{\ensuremath{\leftrightarrow}}
\newunicodechar{⇒}{\ensuremath{\Rightarrow}}
\newunicodechar{…}{\ldots}
\newunicodechar{≥}{\ensuremath{\geq}}
\newunicodechar{≤}{\ensuremath{\leq}}
\newunicodechar{≠}{\ensuremath{\neq}}
\newunicodechar{≈}{\ensuremath{\approx}}
\newunicodechar{∈}{\ensuremath{\in}}
\newunicodechar{∉}{\ensuremath{\notin}}
\newunicodechar{∧}{\ensuremath{\wedge}}
\newunicodechar{∨}{\ensuremath{\vee}}
\newunicodechar{¬}{\ensuremath{\neg}}
\newunicodechar{×}{\ensuremath{\times}}
\newunicodechar{÷}{\ensuremath{\div}}
\newunicodechar{∞}{\ensuremath{\infty}}
\newunicodechar{∑}{\ensuremath{\sum}}
\newunicodechar{∏}{\ensuremath{\prod}}
\newunicodechar{∫}{\ensuremath{\int}}
\newunicodechar{√}{\ensuremath{\sqrt}}
\newunicodechar{∂}{\ensuremath{\partial}}
\newunicodechar{∇}{\ensuremath{\nabla}}
\newunicodechar{α}{\ensuremath{\alpha}}
\newunicodechar{β}{\ensuremath{\beta}}
\newunicodechar{γ}{\ensuremath{\gamma}}
\newunicodechar{δ}{\ensuremath{\delta}}
\newunicodechar{ε}{\ensuremath{\epsilon}}
\newunicodechar{θ}{\ensuremath{\theta}}
\newunicodechar{λ}{\ensuremath{\lambda}}
\newunicodechar{μ}{\ensuremath{\mu}}
\newunicodechar{π}{\ensuremath{\pi}}
\newunicodechar{σ}{\ensuremath{\sigma}}
\newunicodechar{τ}{\ensuremath{\tau}}
\newunicodechar{φ}{\ensuremath{\phi}}
\newunicodechar{ω}{\ensuremath{\omega}}
\newunicodechar{Δ}{\ensuremath{\Delta}}
\newunicodechar{Σ}{\ensuremath{\Sigma}}
\newunicodechar{Π}{\ensuremath{\Pi}}
\newunicodechar{Ω}{\ensuremath{\Omega}}

% --- MATH OPERATORS ---
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\sigmoid}{sigmoid}

% --- HYPERREF CONFIGURATION ---
\hypersetup{
    colorlinks=true,
    linkcolor=janusblue,
    filecolor=accentgold,
    urlcolor=janusblue,
    citecolor=janusblue,
    pdftitle={Project JANUS: Complete Technical Specification},
    pdfauthor={Jordan Smith},
    pdfsubject={Neuromorphic Trading Intelligence},
    pdfkeywords={neuromorphic computing, algorithmic trading, machine learning, Rust, neuroscience},
    bookmarks=true,
    bookmarksopen=true,
    bookmarksnumbered=true
}

% --- HEADER/FOOTER ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textcolor{janusblue}{\textbf{Project JANUS}}}
\fancyhead[R]{\textcolor{accentgold}{Complete Specification}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% --- SECTION FORMATTING ---
\titleformat{\section}
  {\Large\bfseries\color{janusblue}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{janusblue}}
  {\thesubsection}{1em}{}

% --- CODE LISTINGS STYLE ---
\lstdefinestyle{rust}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{rustcolor},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    morekeywords={fn, let, mut, pub, struct, impl, use, mod, trait, enum, match, if, else, return, self, Self, true, false, async, await}
}

\lstdefinestyle{python}{
    language=Python,
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{pythoncolor},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=4,
    morecomment=[l]{\#}
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{codegray},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\begin{document}

% --- TITLE PAGE ---
\begin{titlepage}
    \centering
    \vspace*{3cm}

    {\Huge\bfseries\color{janusblue} Project JANUS\par}
    \vspace{0.5cm}
    {\Large\color{accentgold} Neuromorphic Trading Intelligence\par}
    \vspace{2cm}

    {\LARGE\bfseries Complete Technical Specification\par}
    \vspace{1cm}

    {\large\itshape A Brain-Inspired Architecture for Autonomous Financial Systems\par}
    \vspace{3cm}

    \begin{tcolorbox}[colback=janusblue!5, colframe=janusblue, title=\textbf{Unified Documentation}, width=0.85\textwidth]
    This document consolidates all technical specifications of Project JANUS:
    \begin{enumerate}[leftmargin=2cm]
        \item \textbf{\textcolor{janusblue}{Main Architecture}} — System design and philosophical foundation
        \vspace{0.2cm}
        \item \textbf{\textcolor{forwardblue}{Forward Service}} — Real-time decision-making and execution
        \vspace{0.2cm}
        \item \textbf{\textcolor{backwardpurple}{Backward Service}} — Memory consolidation and learning
        \vspace{0.2cm}
        \item \textbf{\textcolor{neurogreen}{Neuromorphic Architecture}} — Brain-region mapping
        \vspace{0.2cm}
        \item \textbf{\textcolor{rustorange}{Rust Implementation}} — Production deployment guide
    \end{enumerate}
    \end{tcolorbox}

    \vfill

    {\large\bfseries Author\par}
    {\large Jordan Smith\par}
    \vspace{0.5cm}

    {\large\bfseries Date\par}
    {\large \today\par}
    \vspace{1cm}

    {\small\itshape ``The god of beginnings and transitions, looking simultaneously to the future and the past.''\par}

\end{titlepage}

% --- TABLE OF CONTENTS ---
\newpage
\tableofcontents
\newpage

% ============================================
% PART 1: MAIN ARCHITECTURE
% ============================================
\part{Main Architecture}
\label{part:main}

\section*{Overview}

Part 1 provides the architectural philosophy and system design overview for Project JANUS. This section would typically include:

\begin{itemize}
    \item \textbf{Introduction:} The crisis of complexity in quantitative trading
    \item \textbf{Architectural Philosophy:} The dual-service design (Forward/Backward)
    \item \textbf{Core Components:} Vision, Logic, Fusion, and Decision systems
    \item \textbf{Memory Hierarchy:} Three-timescale learning architecture
    \item \textbf{Implementation Strategy:} Rust-first development approach
\end{itemize}

\textbf{Note:} The detailed mathematical specifications for each component are presented in Parts 2-5 below. This consolidated document focuses on the theoretical foundations and algorithmic specifications rather than high-level architectural discussion.

% ============================================
% PART 2: FORWARD SERVICE
% ============================================
\newpage
\part{Forward Service (Janus Bifrons)}
\label{part:forward}

% Forward Service Content
\section*{Abstract}

JANUS Forward represents the "wake state" of the JANUS trading system, responsible for all real-time decision-making during market hours. This service combines:

\begin{itemize}
    \item \textbf{Visual Pattern Recognition} using Gramian Angular Fields (GAF) and Video Vision Transformers (ViViT)
    \item \textbf{Symbolic Reasoning} via Logic Tensor Networks (LTN) for constraint satisfaction
    \item \textbf{Multimodal Fusion} integrating time series, visual, and textual market data
    \item \textbf{Dual-Pathway Decision Making} inspired by basal ganglia architecture
\end{itemize}

The Forward service operates on a hot path with strict latency requirements, implementing end-to-end gradient flow through differentiable market simulation while maintaining regulatory compliance through symbolic constraints.

\section{Visual Pattern Recognition: DiffGAF and ViViT}
\label{sec:visual_forward}

The visual subsystem transforms time series data into spatiotemporal images, enabling the system to "see" market patterns that traditional numerical methods miss.

\subsection{Mathematical Foundation: Gramian Angular Fields}

Time series are encoded into polar coordinates and projected onto Gramian matrices, creating 2D representations that preserve temporal correlations.

\subsubsection{Input Preprocessing}

Given raw market data $X = \{x_1, x_2, \ldots, x_T\}$ where $x_t \in \mathbb{R}^D$ (multi-feature time series), we first apply feature selection to extract $F$ relevant features.

\subsubsection{Step 1: Learnable Normalization}

Instead of fixed min-max scaling, we use learnable affine transformations with domain constraints:
\begin{equation}
\tilde{x}_t = \tanh\left(\gamma \odot \frac{x_t - \mu}{\sigma} + \beta\right)
\end{equation}
where $\gamma, \beta \in \mathbb{R}^F$ are learned parameters, and $\mu, \sigma$ are running statistics. The $\tanh$ function guarantees $\tilde{x}_t \in (-1, 1)$, ensuring the subsequent $\arccos$ operation is well-defined.

\subsubsection{Step 2: Polar Coordinate Transformation}

Map normalized values to angular space:
\begin{align}
\phi_t &= \arccos(\tilde{x}_t) \in [0, \pi] \\
r_t &= \frac{t}{T} \quad \text{(normalized timestamp)}
\end{align}

\subsubsection{Step 3: Gramian Field Generation}

Construct the Gramian Angular Summation Field (GASF):
\begin{equation}
\mathbf{G}_{ij} = \cos(\phi_i + \phi_j) = \tilde{x}_i \tilde{x}_j - \sqrt{1-\tilde{x}_i^2}\sqrt{1-\tilde{x}_j^2}
\end{equation}

Or the Gramian Angular Difference Field (GADF):
\begin{equation}
\mathbf{G}_{ij} = \sin(\phi_i - \phi_j) = \sqrt{1-\tilde{x}_i^2}\tilde{x}_j - \tilde{x}_i\sqrt{1-\tilde{x}_j^2}
\end{equation}

\subsection{3D Spatiotemporal Manifolds: GAF Video}

To capture temporal dynamics, we generate a sequence of GAF frames using sliding windows.

\subsubsection{Sliding Window GAF Video Generation}

Given a time series of length $T$, window size $W$, and stride $S$:
\begin{enumerate}
    \item Extract windows: $X_k = \{x_{(k-1)S+1}, \ldots, x_{(k-1)S+W}\}$ for $k = 1, \ldots, N$
    \item Generate GAF for each window: $\mathbf{G}_k = \text{GAF}(X_k) \in \mathbb{R}^{W \times W}$
    \item Stack into video: $\mathbf{V} = [\mathbf{G}_1, \mathbf{G}_2, \ldots, \mathbf{G}_N] \in \mathbb{R}^{N \times W \times W}$
\end{enumerate}

\subsection{Video Vision Transformer (ViViT)}

The GAF video is processed by a factorized spatiotemporal transformer.

\subsubsection{Patch Embedding}

Divide each frame $\mathbf{G}_k$ into non-overlapping patches:
\begin{equation}
\mathbf{P}_k = \text{Reshape}(\mathbf{G}_k) \in \mathbb{R}^{P \times (p^2)}
\end{equation}
where $P = (W/p)^2$ is the number of patches per frame.

\subsubsection{Spatial Attention}

Apply self-attention within each frame:
\begin{equation}
\mathbf{Z}^{(l)}_k = \text{MSA}(\text{LN}(\mathbf{Z}^{(l-1)}_k)) + \mathbf{Z}^{(l-1)}_k
\end{equation}

\subsubsection{Temporal Attention}

Apply attention across frames:
\begin{equation}
\mathbf{H}^{(l)} = \text{MSA}(\text{LN}([\mathbf{Z}^{(l)}_1, \ldots, \mathbf{Z}^{(l)}_N]))
\end{equation}

\section{Logic Tensor Networks: Symbolic Reasoning Engine}

LTNs bridge neural networks and first-order logic, enabling differentiable constraint satisfaction.

\subsection{Mathematical Foundation}

\subsubsection{Grounding Function}

Map logical constants to real vectors:
\begin{equation}
\mathcal{G}: \mathcal{C} \to \mathbb{R}^d
\end{equation}

\subsubsection{Predicate Grounding}

A predicate $P(x)$ is grounded as a neural network $f_\theta: \mathbb{R}^d \to [0,1]$.

\subsection{Lukasiewicz T-Norm Operations}

\subsubsection{Conjunction (AND)}

For training, we use Product Logic to ensure smooth gradients:
\begin{equation}
u \land v = u \cdot v
\end{equation}

For inference/evaluation, standard Łukasiewicz logic may be used:
\begin{equation}
u \land v = \max(0, u + v - 1)
\end{equation}

\subsubsection{Disjunction (OR)}
\begin{equation}
u \lor v = \min(1, u + v)
\end{equation}

\subsubsection{Negation (NOT)}
\begin{equation}
\neg u = 1 - u
\end{equation}

\subsubsection{Implication (IF-THEN)}

For training (Product Logic):
\begin{equation}
u \Rightarrow v = 1 - u + u \cdot v
\end{equation}

For inference (Łukasiewicz Logic):
\begin{equation}
u \Rightarrow v = \min(1, 1 - u + v)
\end{equation}

\subsection{Knowledge Base Formulation}

\subsubsection{Wash Sale Constraint}
\begin{equation}
\forall t: \text{Sell}(t) \land \text{Buy}(t') \land |t-t'| < 30 \Rightarrow \neg \text{TaxLoss}(t)
\end{equation}

\subsubsection{Almgren-Chriss Risk Constraint}
\begin{equation}
\forall \text{order}: \text{Execute}(\text{order}) \Rightarrow \text{Slippage}(\text{order}) < \lambda \cdot \text{Volatility}
\end{equation}

\subsection{Logical Loss Function}

\subsubsection{Satisfiability Aggregation}
\begin{equation}
\text{SAT}(\mathcal{KB}) = \text{p-mean}_{i=1}^{|\mathcal{KB}|}(\phi_i)
\end{equation}

\subsubsection{Logical Loss}
\begin{equation}
\mathcal{L}_{\text{logic}} = 1 - \text{SAT}(\mathcal{KB})
\end{equation}

\section{Multimodal Fusion: Gated Cross-Attention}

\subsection{Input Modalities}

\begin{itemize}
    \item Visual: $\mathbf{v} \in \mathbb{R}^{d_v}$ (from ViViT)
    \item Temporal: $\mathbf{t} \in \mathbb{R}^{d_t}$ (from LSTM/Transformer)
    \item Textual: $\mathbf{s} \in \mathbb{R}^{d_s}$ (from BERT embeddings)
\end{itemize}

\subsection{Gated Cross-Attention Mechanism}

\subsubsection{Attention Computation}
\begin{equation}
\text{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \softmax\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\end{equation}

\subsubsection{Gating Mechanism}
\begin{equation}
g = \sigmoid(\mathbf{W}_g[\mathbf{v}; \mathbf{t}; \mathbf{s}] + \mathbf{b}_g)
\end{equation}

\section{Decision Engine: Basal Ganglia Pathways}

\subsection{Praxeological Motor: Dual Pathways}

\subsubsection{Direct Pathway (Go Signal)}
\begin{equation}
\mathbf{d}_{\text{direct}} = \text{ReLU}(\mathbf{W}_d \mathbf{h}_{\text{fused}} + \mathbf{b}_d)
\end{equation}

\subsubsection{Indirect Pathway (No-Go Signal)}
\begin{equation}
\mathbf{d}_{\text{indirect}} = \text{ReLU}(\mathbf{W}_i \mathbf{h}_{\text{fused}} + \mathbf{b}_i)
\end{equation}

\subsection{Cerebellar Forward Model}

\subsubsection{Market Impact Prediction}
\begin{equation}
\hat{p}_{t+1} = f_{\text{cerebellum}}(\mathbf{s}_t, \mathbf{a}_t)
\end{equation}

% ============================================
% PART 3: BACKWARD SERVICE
% ============================================
\newpage
\part{Backward Service (Janus Consivius)}
\label{part:backward}

\section*{Abstract}

JANUS Backward represents the "sleep state" of the system, responsible for memory consolidation, schema formation, and learning from accumulated experience. This service implements:

\begin{itemize}
    \item \textbf{Three-Timescale Memory Hierarchy} (Hippocampus → SWR → Neocortex)
    \item \textbf{Sharp-Wave Ripple Simulation} for prioritized experience replay
    \item \textbf{Schema Formation} via UMAP-based clustering
    \item \textbf{Recall-Gated Consolidation} ensuring only successful patterns are promoted
\end{itemize}

The Backward service runs on a cold path during off-market hours, performing computationally intensive operations to distill daily experiences into long-term knowledge.

\section{Memory Hierarchy: Three-Timescale Architecture}

\subsection{Short-Term Memory (Hippocampus)}

\subsubsection{Episodic Buffer}

Stores raw experiences during trading:
\begin{equation}
\mathcal{D}_{\text{hippo}} = \{(s_t, a_t, r_t, s_{t+1}, \mathbf{c}_t)\}_{t=1}^{T}
\end{equation}
where $\mathbf{c}_t$ contains contextual metadata (volatility, spreads, volume).

\subsubsection{Pattern Separation}

Uses random projections to ensure diverse encoding:
\begin{equation}
\mathbf{h}_t = \tanh(\mathbf{W}_{\text{rand}} \cdot [s_t; a_t; \mathbf{c}_t])
\end{equation}

\subsection{Medium-Term Consolidation (SWR Simulator)}

\subsubsection{Replay Prioritization}

Compute TD-error based priority:
\begin{equation}
p_i = |\delta_i| + \epsilon
\end{equation}
where $\delta_i = r_i + \gamma \max_{a'} Q(s_{i+1}, a') - Q(s_i, a_i)$ and $\epsilon = 10^{-6}$ ensures numerical stability.

\subsubsection{Sampling Probability}
\begin{equation}
P(i) = \frac{p_i^\alpha}{\sum_j p_j^\alpha}
\end{equation}
where $\alpha \in [0, 1]$ controls prioritization strength (typically $\alpha = 0.6$).

\subsubsection{Importance Sampling Correction}
\begin{equation}
w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta
\end{equation}
where $\beta \in [0, 1]$ is annealed from $0.4 \to 1.0$ during training to fully correct bias at convergence.

\subsection{Long-Term Memory (Neocortex)}

\subsubsection{Schema Representation}

Each schema is a prototype:
\begin{equation}
\mathbf{z}_k = \frac{1}{|\mathcal{C}_k|} \sum_{i \in \mathcal{C}_k} \mathbf{h}_i
\end{equation}

\subsubsection{Recall-Gated Consolidation}

Only update schemas from successfully recalled experiences:
\begin{equation}
\mathbf{z}_k \leftarrow \mathbf{z}_k + \eta \cdot \mathbb{1}[\text{recall\_success}] \cdot (\mathbf{h}_{\text{new}} - \mathbf{z}_k)
\end{equation}

\section{UMAP Visualization: Cognitive Dashboard}

\subsection{AlignedUMAP for Schema Formation}

Maintains consistent embeddings across sleep cycles.

\subsubsection{Objective Function}

The full UMAP loss includes both attraction and repulsion terms:
\begin{equation}
\mathcal{L}_{\text{UMAP}} = \sum_{i \neq j} \left[
    w_{ij} \log(q_{ij}) + (1 - w_{ij}) \log(1 - q_{ij})
\right]
\end{equation}
where $q_{ij} = \left(1 + \|\mathbf{y}_i - \mathbf{y}_j\|^2\right)^{-1}$.

\textbf{Note:} In practice, the repulsion term $(1 - w_{ij})$ is approximated via \emph{negative sampling} to achieve $\mathcal{O}(N)$ complexity. For each positive edge, we sample $k=5$ random negative pairs.

\subsection{Parametric UMAP for Real-Time Monitoring}

Train a neural network to project new experiences:
\begin{equation}
\mathbf{y}_{\text{new}} = f_\theta(\mathbf{h}_{\text{new}})
\end{equation}

\section{Integration with Vector Database (Qdrant)}

\subsection{Schema Storage}

Each schema is stored in the vector database with the following structure:

\textbf{Schema Representation:}
\begin{equation}
\mathcal{S}_k = (\text{id}_k, \mathbf{z}_k, \mathcal{M}_k)
\end{equation}

where:
\begin{itemize}
    \item $\text{id}_k \in \mathbb{N}$: Unique schema identifier
    \item $\mathbf{z}_k \in \mathbb{R}^d$: Centroid embedding vector
    \item $\mathcal{M}_k$: Metadata payload containing:
    \begin{itemize}
        \item $n_k = |C_k|$: Number of experiences in cluster
        \item $\bar{r}_k = \frac{1}{n_k}\sum_{i \in C_k} r_i$: Average reward
        \item $\sigma_k = \sqrt{\frac{1}{n_k}\sum_{i \in C_k}(r_i - \bar{r}_k)^2}$: Volatility (std. dev. of returns)
    \end{itemize}
\end{itemize}

\textbf{Storage Invariant:}
All schema vectors are L2-normalized for cosine similarity search:
\begin{equation}
\mathbf{z}_k \leftarrow \frac{\mathbf{z}_k}{\|\mathbf{z}_k\|_2}
\end{equation}

\subsection{Similarity Search}

Retrieve nearest schemas:
\begin{equation}
\mathcal{N}_k = \argmax_{k} \text{cosine}(\mathbf{h}_t, \mathbf{z}_k)
\end{equation}

% ============================================
% PART 4: NEUROMORPHIC ARCHITECTURE
% ============================================
\newpage
\part{Neuromorphic Architecture}
\label{part:neuro}

\section*{Abstract}

This document maps the computational components of Project JANUS to specific brain regions, ensuring biological plausibility and leveraging neuroscience insights for system design. The neuromorphic approach provides:

\begin{itemize}
    \item \textbf{Modular Design} with clear functional boundaries
    \item \textbf{Biological Validation} of architectural decisions
    \item \textbf{Emergent Intelligence} through brain-inspired interactions
\end{itemize}

\section{Neuromorphic Design Philosophy}

\subsection{Why Brain-Inspired Architecture?}

The brain efficiently solves problems similar to trading:
\begin{itemize}
    \item Pattern recognition under uncertainty
    \item Fast decision-making with delayed rewards
    \item Continual learning without catastrophic forgetting
    \item Multi-timescale memory consolidation
\end{itemize}

\subsection{Neuroscience-to-Trading Mapping}

\begin{table}[h]
\centering
\begin{tabularx}{\textwidth}{|l|L|L|}
\hline
\textbf{Brain Region} & \textbf{Biological Function} & \textbf{Trading Function} \\
\hline
Visual Cortex & Pattern recognition & GAF/ViViT chart analysis \\
\hline
Hippocampus & Episodic memory & Experience replay buffer \\
\hline
Prefrontal Cortex & Logic and planning & LTN constraint checking \\
\hline
Basal Ganglia & Action selection & Buy/sell/hold decisions \\
\hline
Cerebellum & Motor prediction & Market impact forecasting \\
\hline
Amygdala & Threat detection & Risk circuit breakers \\
\hline
\end{tabularx}
\end{table}

\section{Brain Region Architectures}

\subsection{Cortex: Strategic Planning \& Long-term Memory}

\subsubsection{Trading Implementation}

\textbf{Component:} Neocortical Schema Network

\textbf{Implementation:}
\begin{itemize}
    \item Schema prototypes stored in Qdrant vector database
    \item Each schema represents a market regime (trending, mean-reverting, volatile)
    \item Slow consolidation during sleep cycles
\end{itemize}

\subsection{Hippocampus: Episodic Memory \& Experience Replay}

\subsubsection{Trading Implementation}

\textbf{Component:} Episodic Buffer + SWR Replay

\textbf{Implementation:}
\begin{itemize}
    \item Fixed-size circular buffer storing recent trades
    \item Sparse encoding via random projections
    \item Prioritized replay during training
\end{itemize}

\subsection{Basal Ganglia: Action Selection \& Reinforcement Learning}

\subsubsection{Trading Implementation}

\textbf{Component:} Dual-Pathway Decision Module

The basal ganglia implements competing pathways for action selection:

\textbf{Direct Pathway (Go Signal):}
\begin{equation}
\mathbf{d}_{\text{direct}} = \text{ReLU}(\mathbf{W}_{\text{direct}} \mathbf{h} + \mathbf{b}_{\text{direct}})
\end{equation}
where $\mathbf{h}$ is the fused state representation and $\mathbf{W}_{\text{direct}} \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$.

\textbf{Indirect Pathway (No-Go Signal):}
\begin{equation}
\mathbf{d}_{\text{indirect}} = \text{ReLU}(\mathbf{W}_{\text{indirect}} \mathbf{h} + \mathbf{b}_{\text{indirect}})
\end{equation}

\textbf{Action Selection:}
\begin{equation}
\mathbf{a}_t = \text{softmax}(\mathbf{d}_{\text{direct}} - \lambda \cdot \mathbf{d}_{\text{indirect}})
\end{equation}
where $\lambda > 0$ is the inhibition weight parameter.

\subsection{Prefrontal Cortex: Logic, Planning \& Compliance}

\subsubsection{Trading Implementation}

\textbf{Component:} Logic Tensor Network

Ensures regulatory compliance:
\begin{itemize}
    \item Wash sale rules
    \item Position limits
    \item Capital allocation constraints
\end{itemize}

\subsection{Amygdala: Fear, Threat Detection \& Circuit Breakers}

\subsubsection{Trading Implementation}

\textbf{Component:} Anomaly Detection Module

Triggers emergency stops based on statistical distance from normal operation:

\textbf{Mahalanobis Distance:}
\begin{equation}
D_M(\mathbf{s}_t) = \sqrt{(\mathbf{s}_t - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{s}_t - \boldsymbol{\mu})}
\end{equation}
where $\boldsymbol{\mu}$ is the historical mean state and $\boldsymbol{\Sigma}$ is the covariance matrix.

\textbf{Circuit Breaker Condition:}
\begin{equation}
\text{Trigger} = \begin{cases}
1 & \text{if } D_M(\mathbf{s}_t) > \tau_{\text{danger}} \\
0 & \text{otherwise}
\end{cases}
\end{equation}
where $\tau_{\text{danger}}$ is calibrated to a false-positive rate (e.g., $\tau = 5$ for $p < 0.001$).

\textbf{Additional Threat Signals:}
\begin{itemize}
    \item Sudden volatility spike: $\sigma_t > 3 \cdot \sigma_{\text{baseline}}$
    \item Drawdown threshold: cumulative loss $> L_{\max}$
    \item Liquidity crisis: bid-ask spread $> 10 \times$ normal
\end{itemize}

\subsection{Cerebellum: Motor Control \& Execution}

\subsubsection{Trading Implementation}

\textbf{Component:} Forward Model for Market Impact

Predicts price movement from order execution:
\begin{equation}
\Delta p = f_{\text{cerebellum}}(\text{order\_size}, \text{liquidity}, \text{volatility})
\end{equation}

% ============================================
% PART 5: RUST IMPLEMENTATION
% ============================================
\newpage
\part{Rust Implementation}
\label{part:rust}

\section*{Abstract}

This document provides production-ready Rust implementation specifications for Project JANUS, including:

\begin{itemize}
    \item \textbf{ML Framework Strategy} (PyTorch → ONNX → Rust inference)
    \item \textbf{High-Performance Services} with async Tokio runtime
    \item \textbf{Hybrid Training Pipeline} (Python training, Rust deployment)
    \item \textbf{Deployment Architecture} (Docker Compose + Kubernetes)
\end{itemize}

\section{Architectural Overview}

\subsection{The Rust-First Philosophy}

\begin{enumerate}
    \item \textbf{Performance:} Zero-cost abstractions, no GC pauses
    \item \textbf{Safety:} Memory safety without runtime overhead
    \item \textbf{Concurrency:} Fearless async/await with Tokio
    \item \textbf{Ecosystem:} Production-ready ML inference via ONNX
\end{enumerate}

\subsection{Component Diagram}

\begin{tcolorbox}[colback=codegray, colframe=rustcolor, title=System Architecture]
\begin{verbatim}
┌─────────────────────────────────────────────────┐
│            Python Training Gateway              │
│    (FastAPI + PyTorch + HuggingFace)           │
└────────────┬────────────────────────────────────┘
             │ ONNX Export
             ▼
┌─────────────────────────────────────────────────┐
│           Rust Forward Service                  │
│  (Tokio + ONNX Runtime + DiffGAF + LTN)        │
└─────────────────────────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────────────┐
│          Rust Backward Service                  │
│   (Rayon + Qdrant + UMAP + PER Buffer)         │
└─────────────────────────────────────────────────┘
\end{verbatim}
\end{tcolorbox}

\section{Machine Learning Framework Strategy}

\subsection{Framework Comparison Matrix}

\begin{table}[h]
\centering
\begin{tabularx}{\textwidth}{|l|L|L|L|}
\hline
\textbf{Framework} & \textbf{Pros} & \textbf{Cons} & \textbf{Use Case} \\
\hline
tch-rs & Native PyTorch, GPU support & C++ deps, larger binary & Training \& inference \\
\hline
ONNX Runtime & Universal, production-ready & No training & Inference only \\
\hline
Candle & Pure Rust, HF integration & Young ecosystem & Future migration \\
\hline
\end{tabularx}
\end{table}

\subsection{Recommended Migration Path}

\subsubsection{Phase 1: Hybrid (Months 1-3)}
\begin{itemize}
    \item Train models in PyTorch (Python)
    \item Export to ONNX format
    \item Rust inference via \texttt{ort} crate
\end{itemize}

\subsubsection{Phase 2: Rust-Native Inference (Months 4-6)}
\begin{itemize}
    \item Optimize ONNX models for Rust
    \item Custom kernels for DiffGAF/LTN
    \item Benchmark against Python baseline
\end{itemize}

\subsubsection{Phase 3: Full Rust ML (Months 7-12)}
\begin{itemize}
    \item Migrate training to Candle
    \item End-to-end Rust pipeline
    \item Custom GPU kernels via wgpu
\end{itemize}

\section{Forward Service: Rust Implementation}

\subsection{Performance Requirements}

\begin{itemize}
    \item Latency: p99 < 10ms
    \item Throughput: 10,000 req/s
    \item Memory: < 2GB RSS
\end{itemize}

\subsection{Core Data Structures}

The system maintains several key data structures for real-time processing:

\textbf{Market State Representation:}
\begin{equation}
\mathcal{S}_t = (\tau_t, \mathbf{f}_t, \mathcal{O}_t, \mathbf{c}_t)
\end{equation}
where:
\begin{itemize}
    \item $\tau_t \in \mathbb{Z}^+$ is the timestamp
    \item $\mathbf{f}_t \in \mathbb{R}^d$ is the feature vector
    \item $\mathcal{O}_t = (\mathcal{B}_t, \mathcal{A}_t)$ is the order book with bids $\mathcal{B}_t$ and asks $\mathcal{A}_t$
    \item $\mathbf{c}_t$ contains contextual metadata (volatility, spreads, volume)
\end{itemize}

\textbf{Order Book Structure:}
\begin{align}
\mathcal{B}_t &= \{(p_i, q_i) : p_i \in \mathbb{R}^+, q_i \in \mathbb{R}^+ \}_{i=1}^{N_{\text{bid}}} \\
\mathcal{A}_t &= \{(p_j, q_j) : p_j \in \mathbb{R}^+, q_j \in \mathbb{R}^+ \}_{j=1}^{N_{\text{ask}}}
\end{align}

\subsection{GAF Transformation Algorithm}

The GAF transformation converts time series to 2D images via the following algorithm:

\textbf{Algorithm: GAF Computation}
\begin{algorithmic}[1]
\State \textbf{Input:} Time series $X = \{x_1, \ldots, x_W\}$, window size $W$
\State \textbf{Output:} Gramian matrix $\mathbf{G} \in \mathbb{R}^{W \times W}$
\State
\State $\tilde{X} \leftarrow \text{Normalize}(X)$ to $[-1, 1]$
\State $\phi_i \leftarrow \arccos(\tilde{x}_i)$ for $i = 1, \ldots, W$
\For{$i = 1$ to $W$}
    \For{$j = 1$ to $W$}
        \State $\mathbf{G}_{ij} \leftarrow \cos(\phi_i + \phi_j)$
    \EndFor
\EndFor
\State \Return $\mathbf{G}$ reshaped to $[1, W, W]$ tensor
\end{algorithmic}

\textbf{Computational Complexity:} $\mathcal{O}(W^2)$ for matrix construction, where $W$ is the window size.

\subsection{LTN Constraint Evaluation}

Each constraint is represented as a weighted predicate function:

\textbf{Constraint Structure:}
\begin{equation}
\mathcal{C}_k = (P_k, w_k)
\end{equation}
where $P_k: \mathcal{S} \to [0,1]$ is a predicate and $w_k \in \mathbb{R}^+$ is the weight.

\textbf{Evaluation Function:}
\begin{equation}
\text{Eval}(\mathcal{C}_k, \mathcal{S}_t) = w_k \cdot P_k(\mathcal{S}_t)
\end{equation}

\textbf{T-norm Operations (already defined in Part 2):}
\begin{align}
a \land_{\mathcal{L}} b &= \max(0, a + b - 1) \quad \text{(Conjunction)} \\
a \Rightarrow_{\mathcal{L}} b &= \min(1, 1 - a + b) \quad \text{(Implication)}
\end{align}

\textbf{Total Constraint Satisfaction:}
\begin{equation}
\mathcal{L}_{\text{constraint}} = 1 - \frac{1}{K} \sum_{k=1}^{K} \text{Eval}(\mathcal{C}_k, \mathcal{S}_t)
\end{equation}

\subsection{Async Service Architecture}

The service follows an event-driven architecture with the following characteristics:

\textbf{Request Processing Pipeline:}
\begin{enumerate}
    \item \textbf{Initialization:} Load ONNX model $\mathcal{M}_{\text{ViViT}}$ and LTN engine $\mathcal{E}_{\text{LTN}}$
    \item \textbf{Connection Handling:} Bind TCP listener on port 8080
    \item \textbf{Concurrent Processing:} For each incoming request:
    \begin{itemize}
        \item Spawn asynchronous task with model clone
        \item Process request independently (non-blocking)
        \item Return prediction and constraint satisfaction scores
    \end{itemize}
\end{enumerate}

\textbf{Concurrency Model:}
\begin{equation}
\text{Throughput} = \frac{N_{\text{workers}} \times 1000}{T_{\text{avg}}}
\end{equation}
where $N_{\text{workers}}$ is the thread pool size and $T_{\text{avg}}$ is average processing time in ms.

\textbf{Performance Characteristics:}
- Non-blocking I/O via async/await
- Zero-copy model sharing across tasks
- Bounded memory through connection limiting

\section{Backward Service: Batch Processing}

\subsection{Prioritized Experience Replay}

The replay buffer maintains experiences with importance-based sampling.

\textbf{Buffer State:}
\begin{equation}
\mathcal{B} = \{(e_i, p_i)\}_{i=1}^{N}
\end{equation}
where $e_i$ is an experience and $p_i \in \mathbb{R}^+$ is its priority.

\textbf{Hyperparameters:}
\begin{itemize}
    \item $\alpha \in [0,1]$: Priority exponent (0 = uniform, 1 = full prioritization)
    \item $\beta \in [0,1]$: Importance sampling correction
    \item $C$: Buffer capacity
\end{itemize}

\textbf{Sampling Algorithm:}
\begin{algorithmic}[1]
\State \textbf{Input:} Buffer $\mathcal{B}$, batch size $B$
\State \textbf{Output:} Sampled batch $\{e_{i_1}, \ldots, e_{i_B}\}$
\State
\State Compute probabilities: $P(i) = \frac{p_i^\alpha}{\sum_j p_j^\alpha}$
\For{$k = 1$ to $B$}
    \State Sample index $i_k \sim \text{Categorical}(P)$
    \State Add $e_{i_k}$ to batch
\EndFor
\State \Return batch
\end{algorithmic}

\textbf{Importance Weights:}
\begin{equation}
w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta
\end{equation}
These weights correct for the non-uniform sampling distribution.

\subsection{Schema Consolidation Algorithm}

Schemas are formed by clustering experience embeddings and storing centroids.

\textbf{Algorithm: Schema Update}
\begin{algorithmic}[1]
\State \textbf{Input:} Experiences $\mathcal{E} = \{e_1, \ldots, e_N\}$, number of clusters $K$
\State \textbf{Output:} Updated schema database
\State
\State Extract embeddings: $\mathbf{h}_i = \text{Embed}(e_i)$ for $i = 1, \ldots, N$
\State Cluster: $\mathcal{C} = \{C_1, \ldots, C_K\} \leftarrow \text{K-means}(\{\mathbf{h}_i\}, K)$
\For{$k = 1$ to $K$}
    \State Compute centroid: $\mathbf{z}_k = \frac{1}{|C_k|} \sum_{i \in C_k} \mathbf{h}_i$
    \State Compute statistics:
    \State \quad $n_k = |C_k|$
    \State \quad $\bar{r}_k = \frac{1}{|C_k|} \sum_{i \in C_k} r_i$ (average reward)
    \State \textbf{Upsert} schema $k$ with vector $\mathbf{z}_k$ and metadata $(n_k, \bar{r}_k)$
\EndFor
\end{algorithmic}

\textbf{Schema Metadata:}
Each schema $k$ stores:
\begin{itemize}
    \item Centroid vector $\mathbf{z}_k \in \mathbb{R}^d$
    \item Member count $n_k$
    \item Average reward $\bar{r}_k$
    \item Volatility $\sigma_k$ (standard deviation of returns)
\end{itemize}

\textbf{K-means Objective:}
\begin{equation}
\min_{\mathcal{C}} \sum_{k=1}^{K} \sum_{i \in C_k} \|\mathbf{h}_i - \mathbf{z}_k\|^2
\end{equation}

\section{Deployment Architecture}

\subsection{Service Orchestration}

The system deploys as three independent services:

\textbf{Service Topology:}
\begin{enumerate}
    \item \textbf{Forward Service:}
    \begin{itemize}
        \item Port: 8080 (HTTP API)
        \item Dependencies: ONNX model files
        \item Environment: Logging level, model paths
        \item Resource limits: 2GB memory, 2 CPU cores
    \end{itemize}

    \item \textbf{Backward Service:}
    \begin{itemize}
        \item Internal service (no external ports)
        \item Dependencies: Qdrant vector database
        \item Environment: Qdrant connection URL
        \item Scheduling: Triggered during market close
    \end{itemize}

    \item \textbf{Qdrant Vector Database:}
    \begin{itemize}
        \item Ports: 6333 (HTTP), 6334 (gRPC)
        \item Persistent storage for schemas
        \item Vector similarity search engine
    \end{itemize}
\end{enumerate}

\textbf{Service Communication:}
\begin{equation}
\text{Forward} \xrightarrow{\text{experiences}} \text{Buffer} \xrightarrow{\text{nightly}} \text{Backward} \xrightarrow{\text{schemas}} \text{Qdrant}
\end{equation}

\textbf{Volume Management:}
- Model artifacts: Shared read-only volume
- Schema database: Persistent volume with backups
- Experience buffer: Ephemeral storage (daily rotation)

% ============================================
% CONCLUSION
% ============================================
\newpage
\section*{Conclusion}

Project JANUS represents a paradigm shift in quantitative trading: from opaque black boxes to transparent, brain-inspired systems that combine the best of deep learning and symbolic reasoning.

\subsection*{Key Innovations}

\begin{enumerate}
    \item \textbf{Neuromorphic Architecture:} Biologically plausible design with modular brain regions
    \item \textbf{Neuro-Symbolic Fusion:} LTNs bridge neural networks and logical constraints
    \item \textbf{Multi-Timescale Memory:} Three-tier hierarchy mirrors hippocampal-neocortical consolidation
    \item \textbf{Production-Ready Rust:} High-performance, safe, and maintainable implementation
\end{enumerate}

\subsection*{Future Work}

\begin{itemize}
    \item Quantum computing integration for portfolio optimization
    \item Continual learning without catastrophic forgetting
    \item Multi-agent systems for distributed trading
    \item Regulatory AI for automated compliance
\end{itemize}

\vspace{1cm}

\begin{tcolorbox}[colback=janusblue!5, colframe=janusblue, title=\textbf{Repository \& Contact}]
\centering
\textbf{GitHub:} \url{https://github.com/nuniesmith/technical_papers}

\vspace{0.5cm}

For implementation code, updates, and discussions, visit the repository.

\vspace{0.5cm}

\textit{``The god of beginnings and transitions, looking simultaneously to the future and the past.''}
\end{tcolorbox}

\end{document}
